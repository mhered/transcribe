Volvemos entonces para la siguiente charla. Va a comenzar en 9 minutos, así que aprovechan de ir a buscar un vasito de agua, un café, un bocata, no sé, pero venimos con todo para la siguiente charla luego. Nos vemos en 1 minuto. Vamos a buscar el vasito de agua, un bocata, un bocata, no sé, pero venimos con todo la siguiente charla. Nos vemos en 1 minuto. Vamos a buscar el vasito de agua, un bocata, no sé, pero venimos con todo el agua, un bocata, no sé, pero venimos con todo el agua, un bocata, no sé, Y estamos de vuelta con la siguiente charla que comienza ahora en un minutito. Un tema súper interesante, yo creo que todas las personas que hemos estado desarrollando código ese de mala manera intentamos hacer el quité a todo lo que es testing, pero algo realmente necesario, cualquier persona que trabaje profesionalmente en esto es, hay algo que hay que hacer sigos y así que el tema es súper interesante, la charla se titula Python Testing Best Practices y quería invitar ahora la transmisión a Ismael Mendoza. ¿Qué tal? Ismael, hola, se te escucha muy bien, se te ve bien, perfecto, te creo que estamos preparados, vamos a mirar, ah, ya te compartí tu pantalla también para hacer la charla. La tenemos ahí, ¿quizás si quieres aumentar un pelín, le font? Mejor. Sí, yo al menos lo pudo leer, déjame comprobarlo en YouTube, sí, se ve bastante claro, de todas formas si es que alguien encuentra que hay algún problema o algo nos puede avisar por YouTube, sí que tenemos que subir un par de puntitos en la fuente. Bueno, Ismael, bienvenido a LaPyConEspany, no sé si le hubieras ya participado en LaPyConEspany alguna vez. No como ponente, pero sí que vine desde 2019 que fue alicante. Perfecto, pues que genial entonces que ahora tenga la oportunidad de ponerte. No te quiero quitar más tiempo, ya estamos en el horario de tu charla, como sabes tienes 20 minutitos, después luego veremos un par de preguntas, así que mucha suerte. Vale, muchas gracias. Bueno, buenos días. Muchas gracias por haberme invitado. Mi nombre es Ismael Mendonze, yo soy ingeniero de software, actualmente estoy trabajando en Zapier y les voy a hablar un poco sobre buenas prácticas para escribir pruebas en Python. Quiero empezar la charla hablando sobre principios de prueba, que estos sí que son un poco generales a cualquier lenguaje que utilicemos, así que vamos allá. Una de las primeras cosas a tomar en cuenta para escribir pruebas vendría siendo test isolation, que quiere decir que los casos de prueba que escribas no deberían causar efectos secundarios sobre otras pruebas y hay varias razones por la que esto puede ocurrir. Una de ellas es que en Python en particular tengamos un patch que permanezca activo entre varias pruebas, verdad. Entonces, vamos a verlo con ejemplos. Aquí tengo un cliente de GitHub al cual le estoy aplicando un patch, un método, simplemente el método get y lo estoy aplicando mediante una API un poco riesgosa de patch que es start. Luego en la siguiente prueba lo que voy a probar es si el mock se está manteniendo. Entonces, si ejecutamos esta prueba, vemos que pasa, lo cual es un poco extraño, porque quiere decir que tenemos un efecto secundario en caso de prueba en el otro. ¿Cómo lo solucionamos? Una de las formas que tenemos en Python para solucionar este side effects es utilizando context manager, sobre todo en la API de patch. Todo lo que ocurre dentro de este context manager va a permanecer ahí y el patch se va a eliminar una vez que salgas del context manager. De manera que en la siguiente prueba esto ya no va a ser un mock. Si lo ejecutamos, vamos a ver que ahora si tenemos el comportamiento indicado o lo que esperábamos que es que el mock ya no funciona. Otra manera es un decorador. Si decoramos el método de prueba con el patch, la única diferencia con lo anterior es que el scope en el que se aplica el patch va a ser a través de todo el método de prueba y no solamente en la parte de código que queríamos. Entonces, si lo ejecutamos, vamos a tener el mismo comportamiento que antes, que es justamente lo que esperamos. Otra de las causas y esto es una de las más comunes que pueden causar efectos secundarios en diferentes casos de prueba es cuando utilizamos una base de datos. Por ejemplo, aquí tengo un user store en donde creamos un usuario en este método de prueba y en el siguiente método estoy esperando que ese store esté vacío. Entonces, si lo ejecutamos, nos vamos a dar cuenta que va a fallar porque sí que tenemos algo en esa base de datos. Entonces, cuando tenemos una base de datos de prueba, tenemos que procurar que entre las pruebas estemos utilizando algún método de cleanup. Entonces, por ejemplo, si estamos utilizando pytest, utilizaríamos algo como un fixture y los fixtures tienen la, digamos, tienen este feature que podemos hacer el teardown después de hacer yield del fixture y antes de hacer el yield, vendría haciéndolo el setup. Entonces, si ejecutamos esta prueba, en este caso particular, estoy usando el fixture automáticamente. Si ejecutamos la prueba, ahora sí que nos va a funcionar porque estamos garantizando que entre cada caso de prueba estamos limpiando la base de datos. Otro caso muy particular de side effects es cuando estamos compartiendo estado entre, entre los casos de prueba. Muy común cuando estamos usando un setup class. Entonces, si setiamos un usuario username o lo que sea, lo compartimos entre los casos de prueba y en este caso de prueba estamos cambiando el nombre, bien sea por un error, bien sea por cualquier cosa, aquí ya esto no va, no va a funcionar. Entonces, si lo ejecutamos, vamos a ver que nos va a fallar y es porque el string que estamos esperando en el primer método en que habíamos setiado a través de clases ya es diferente. Entonces, cuando vayas a setiar estado o compartir estado en una clase de prueba, lo más importante es que lo use solo para lectura pero que no escribas sobre ello, porque esto es un claro indicio de que van a causar side effects en sus pruebas. Quiero mostrarles este quote que me parece de Michael Ford, que es el que creó la librería MOC, que ahora está en el Standard Library of Python y viene de este artículo de 30 best practices for software development testing. Dice, mientras más tengas que hacer MOCs en tu código, peores tu código. Mientras más tengas que instanciar estado o para probar un pedacito de código, peores el código. Y la idea vendría siendo que tengas unidades pequeñitas que puedas probar y en cambio uses unidades grandes de integración y test funcionales, de manera que tú puedas ver que estas unidades se comporten bien correctamente entre ellas. Ahora, siguiendo esta idea sobre las unidades, hablemos de pruebas unidades. Las pruebas unidades tienen ciertas reglas que deberíamos tomar en cuenta para que funcionen bien. Entre ellas tenemos una que las pruebas unidades debemos tratarlas como cajas negras. Es decir, si vamos a probar una clase, si vamos a probar un método privado, traten de no modificar ni métodos privados ni comportamientos internos de los métodos que vayan a probar durante la prueba. Mientras más se haga esto, las pruebas se van haciendo cada vez menos fiable. Deben ser pequeñas y bien enfocadas cuando estás viendo que tu prueba unitaria empieza a crecer muchísimo, es un indicador super claro que el código está mal estructurado. En esto, por ejemplo, cosas como TDD, que es Driven Development, te enseñan a modularizar un poco más el código. No estoy diciendo que hagan TDD o que lo hagan aunque no lo hagan, sino que tomen en cuenta lo que enseña TDD que también es importante para estructurar sus pruebas. Deben ser rápidas, es decir, las pruebas de endurec, o sea, pruebas que duren menos de 0.11 segundos y no son consideradas pruebas unidades. Esto tomenlo con un granito de sal porque no es 100% así, pero es un buen mindset a tener cuando escriban pruebas unidades. Imagínense que tengan un codebase donde tienen más de mil pruebas y cada una dure un segundo. Ya el continuous integration se devuelve a infinito y no se puede trabajar así. Algunas recomendaciones generales, no sólo para un test. Siempre deben incluir pruebas. Cuando en la industria, cuando tienes código que está en producción, debe ir siempre con pruebas. Toda base de código sin pruebas se sume como defectuoso. Y es muy propenso a que tenga Regression, Box y bueno, quién sabe, o sea, en un codebase ya empiezan a trabajar mucho en un equipo grande de ingeniería y necesita pruebas. Las pruebas debemos tratarlas como código a producto a veces por ser pruebas como que las dejamos a un lado y no nos importa tanto y las hacemos como queramos. Pero todo esto de join gonna need it, keep it simple, stupid, then repeat yourself y cualquier tipo de patrón de diseño aplican igualmente a las pruebas. Porque a medida que el codebase crece, las pruebas crecen y se vuelven inmantenibles. Lo más importante es que se estructuren de una buena manera de forma que puedan crecer con tu código. Y otro es favorecer dependency injection sobre moquear y hacer patch. Esto va muy ligado a la idea anterior que el cuota anterior que me mostré a Michael Ford y es que en vez de colocar objetos espía o moquear muchas cosas, lo ideal sería crear el objeto como tal e inyectarlo sobre el método que quieren probar. En Python tenemos body frameworks. En estos quiero hacer énfasis principalmente en estos dos en Python y unit tests. Nose y Nose2 son básicamente unit tests con plugins. Nose ya no está mantenido. Nose2 sería lo que ahora está en mantenimiento por la comunidad. Mi recomendación es que se mantengan con los dos primeros. Y mi recomendación entre estos dos es Python. Porque te ofrece toda la flexibilidad que necesitas para escribir pruebas funcionales y pruebas de clases te permite usar fixtures que favorece dependency injection y rehusabilidad de código. Unit test fue una portabilidad que vino de JUnit que por eso se parece tanto cosas como ya utilizada en camel case y todo y es solo basado en clases. Entonces usando estos dos frameworks o conociéndolos muchas veces nos preguntamos ¿Pruedas funcionales o pruebas basadas en clases? ¿Qué tengo que utilizar? Como les mencioné, empites por lo general se pueden utilizar funciones o clases y en unit tests por lo general son solo clases que es lo único que te ofrece. Hay cierto creencia de que empites favorece más funcional que en clases pero eso realmente no es cierto. Puedes utilizar tantas funciones como clases. Así se ve una prueba funcional en empites, es muy sencilla, es bastante to the point. Así se ve una clase en empites. Básicamente la única diferencia con unit tests es que no estamos heredando de test case y favorecemos a usar fixtures antes de usar los métodos de set up teardown que te ofrece unit test. Y en unit tests si que heredamos de test case utilizamos cosas como set up. Es básicamente por ahí va la idea, la diferencia entre ellas. De hecho si las ejecutamos estas tres cosas que acabamos de mostrar son completamente equivalentes y las tres funcionan. Mi recomendación es que bien sea pites o bien sea unit tests se mantengan con uno de ellos, es decir no los mezclen en el sentido de que si estás usando test case no utilices pites, no utilices fixtures porque el comportamiento empieza a fallar y los test no van a funcionar como esperas. Lecomendaciones generales para decidir entre función o clases. Si la idea sería usar clases como una forma de agrupación semántica de prueba, esto quiere decir que si estás haciendo pruebas del mismo feature, que de hecho que compartan estado entre ellos o que engloben como la misma idea lo ideal sería utilizar una clase. Si necesitas ejecutar un conjunto de pruebas dentro de un mismo core, es decir cuando estemos en continuous integration y tengamos la prueba ejecuta en de forma paralela, las todo lo que viene en una clase se ejecuta en el mismo core. Y si estamos en pites utiliza fixtures para reducir código entre funciones y métodos, es decir no utilices el estilo de y unit para este tipo de cosas. Generalmente cuando estamos escribiendo pruebas, una cosa importante es setup y teardown que ya lo mencionaba mucho durante la charla. Estos métodos generalmente lo usamos para preparar y limpiar datos en las pruebas. Esa es como nuestra idea principal de los setups y teardowns. En pites la recomendación de fixtures y hay varias maneras de usarla. Si lo vamos a usar más como setups y teardowns, el auto-use es importante porque esto es lo que va a permitir que se usen automáticamente en cada una de tus pruebas. Es decir esto se va a ejecutar dependiendo del scope en el que lo definen. Si lo defines a función se va a ejecutar una vez antes de cada función de prueba. Y el scope puede cambiar mucho. Puedes hacerlo a package, session, class, module. Bueno hay muchas. O sea de manera que te da la flexibilidad que tú quieras. Si la fixture no lo vas a colocar como auto-use, la puedes utilizar en cualquier prueba si la pasas como parámetro de la prueba y esto es lo que favorece dependency injection si estamos utilizando fixtures. Esto es a lo que yo me refería cuando hablábamos de xunit style. Es como en pites, también podrías hacer lo que haces en unit tests de usar setup class, setup module, setup function, etc. Se escriben diferente, así que tengan cuidado porque las de unit tests son camel case y estas son snack case. No son tan recomendadas porque no puedes reutilizarlas mucho. El scope es lo que sea. Bien sea el módulo, bien sea la clase, la función, y ahí no van a salir. En unit tests es lo que me había mencionado. Tenemos setup, setup class, teardown y teardown class. Aquí hay algo importante a tomar en cuenta y es que setup class puede causar cosas como efectos como el que vimos al principio de la charla en donde definíamos un usuario a nivel de clase y si lo modificamos en alguna de los métodos, esto nos podría causar un side effect. Moviéndonos un poco del tema de setup y teardown patch en mock, son otros los conceptos que vemos muchísimo en python. Quiero enfocarme en dos librerías principalmente para mocking y a pesar de que hay recomendado python, voy a recomendar unittest.mock para hacer los mocks. La razón es que unit test mock es una librería que se introdujo al standard library porque fue creada por este Michael Ford y al final pasó a ser parte de unit test en el standard library y funciona muy bien con pytest. El problema de otras librerías como por ejemplo pytest mock es que te da un fixture que te se llama mocker, por ejemplo, y él es conveniente porque automáticamente deshace los patch al final de las pruebas, es decir no tenemos que usar ni context manager ni decorators como se los mostré anteriormente, pero puede causar confusión porque en el momento de que tienes un code bay suficientemente grande y una cantidad suficientemente importante en ingenieros trabajando en ella, muy posible ves unit test mock y mocker funcionando ahí en conjunto y no funcionan también y se los quiero mostrar con un ejemplo. Aquí estoy haciendo lo que no debería y usar mocker como un context manager, hecho si lo ejecuto lo que va a pasar es un poco lo esperado, el test pasa pero me dan un warning, me dice mock return by pytest mock, do not need to be used as context manager, no dice vale me dio un warning, pero qué pasa si yo quiero cambiar el comportamiento de mi mock ahora ya no quiero que sea un mock nada más sino que devuelva una función nueva con un valor nuevo por alguna razón en mi test. Aquí sé que ya empiezan a ocurrir cosas oscuras y te dan un error crítico que dice attribute error enter vale básicamente te están diciendo que mocker la estás utilizando con context manager cuando no, este tipo de cosas puede suceder y por eso yo recomiendo mantenerse solo con unit test mock que está bastante bien. Otro de las cosas interesantes de la librería de mock es que te permite hacer comprobaciones sobre si tu método ha sido llamado, cómo ha sido llamado y con qué parámetros. Con este pequeño ejemplo se los quiero mostrar y es que tenemos un cliente de github que obtiene los gist de github y nada más estamos llamando aquí a un método get, eso es todo lo que hace este método. Ahora en mi prueba voy a hacer un mock sobre un método completamente diferente y voy a llamar a get gist que como vimos solo llama get, esta prueba debe estaría fallar porque aquí no estoy llamando a get gist URLs en ningún sitio, sin embargo esta prueba pasa y se preguntarán pero por qué pasa simplemente es porque la API y los mocs es un poco diferente a lo que uno esperaría. Aquí estamos haciendo una cert sobre un método llamado cold ones cuando realmente debería hacerse de la siguiente forma. Sin embargo lo que yo quiero que se llevan de aquí y se los muestro ejecutando esto es que aquí sí que vemos el error, no? Expect get gist URLs to happen cold ones. Lo que quiero que se llevan de este ejemplo es que la API de los mocs es infinito, o sea tú puedes hacer punto tal, punto método, punto método y eso va a funcionar porque justamente es la idea de los mocs pero hay que tener cuidado. En los patch que justamente digamos es la acción que estamos ejecutando para poder crear mocs en nuestro código pasa algo particular en Python al menos y es que el objeto que queremos aplicarle el patch le excluir con ciertas reglas. Uno, ser importado desde tu archivo de pruebas, es decir no vas a poder hacer un patch que no puedas importar desde ahí y que el patch que estemos definiendo en el patch tiene que ser de donde el objeto se use, no donde el objeto se define y esto es motivo de confusión. En este ejemplo imagínense el archivo service.py y aquí tenemos nuestro cliente de GitHub al que le hacemos get gist, vale bien y tenemos el GitHub Client que se define GitHub Client que es otro módulo completamente diferente. En mis pruebas yo debo hacer el patch en service y no en GitHub Client, de hecho debe escribirse así service, GitHub Client, Get Gist. Aquí nos está definiendo GitHub Client, aquí se está usando GitHub Client y aquí sí que nuestra prueba va a funcionar porque los patch tienen esta particularidad. Otra cosa que nos encontramos y a lo mejor alguno ya ha visto esto es cuando estamos viendo un caso de prueba que necesita tanto aplicar tantos mocs que se vuelve una escalera, lo que llamamos un patch help. Tienes tanto mocs que hasta el código se vuelve elegible y complicado. Esto tiene un par de red flags, el más importante es el que estás haciendo demasiados mocs, probablemente deberías modularizar el código para no tener que aplicar tantos patch. Sin embargo, si existe el caso en el que lo necesitas hacer patch multiple puede ayudar en este caso, es decir fíjense cómo se abstrae toda la lógica del patching afuera del método y de manera que queda como un one liner y aquí sí que se entiende lo que está pasando en el caso de prueba. Lo importante de llevarse aquí es revisar la IPI de patch porque tiene múltiples ideas, que la IPI es bastante rica, es decir que puedes valerte eso para no solo utilizar un context manager, el mismo context manager en todos los casos. Si ejecutamos estos dos ejemplos, los dos van a pasar porque los dos funcionan, esto es más de cómo estructurar el código. Por lo general, cuando estamos haciendo pruebas también queremos probar que nuestro código es capaz de elevar excepciones. En este caso particular tenemos un método que simplemente eleva una excepción cuya string va a ser error y en este caso de prueba estoy haciendo una cert de que el string que estoy devolviendo aquí que debe ser error es some other string, o sea es completamente distinto, esta prueba no puede pasar. En embargo la ejecuto y también va a pasar y eso son, dice pero por qué, por qué está pasando. Lo que sucede en este caso particular es que la cert debe ejecutarse fuera del context manager. Si estás haciendo la cert dentro del context manager, lo que va a suceder es que la excepción se va a elevar y en ese contexto el código que sigue dentro del context manager no se ejecuta. Pero error como es un objeto en Python, mantendrá su valor fuera del context manager y ahí sí que puedes hacer la cert y va a funcionar correctamente. Ese es el comportamiento que esperaríamos, es que falla. De fechas, tiempo y todo lo relacionado es complicado en las pruebas, son un indicador de que puedan ocurrir test flaking, digamos, como lo que llamamos pruebas indeterminísticas. En este caso, muy sencillo, tenemos una función que calcula que día es mañana y en nuestra prueba estamos usando datetime.today. Entonces si ejecutamos esta prueba, la prueba va a pasar porque hoy estamos a 2 y mañana es 3, pero esta prueba les va a durar un solo día, ya mañana va a fallar. Entonces qué vamos a hacer? Uno favorece dependence injection si queremos irnos por ese lado y es instanciar nuestra fecha y pasarla a media dependence injection en la función que queremos probar. Y la opción 2 que tenía Python es freezegun que es una librería que es bastante útil. Esta es lo que va a hacer es un patch sobre cualquier datetime, time o cualquier de estas funciones que te devuelva fechas y la va a devolver la fecha que especifica acá. Si ejecutamos esas pruebas que acabo de mencionarles, van a pasar las dos porque hemos instanciado la fecha que queremos. En cuanto a pruebas hasincanas, tenemos PyTest a SyncIO o PyTest Tornado, depende del framework que estás utilizando. Lo más importante es que revises la documentación y por ejemplo si has SyncIO utilices PyTest marca SyncIO, si es Tornado, marque y test. Si los mezclas vas a empezar a tener problemas. Ellos, como las sintaxis es más o menos la misma, pueden funcionar pero tienen comportamientos extraños cuando empiezas a mezclar las dos API. También tenemos fixtures que pueden hacer asignos, simplemente se pueden utilizar con la sintaxis a SyncAwait y MOX que me parece una de las cosas más brutales de Python 3.9 en la librería de Unites es que te trae un async MOX. Entonces, esto libra muchísimos dolores de cabeza cuando queremos hacer MOX que son asíncronos y necesitamos llamarlos con await. Ahora me gustaría terminar la charla con un par de consejos para continuous integration. El primero de ellos es Coverage. 100% de Coverage no implica 100% de calidad, sin embargo es muy común y es necesario tener al menos un mínimo nivel que nos garantice la calidad del código. En Backend, 80-90% suele ser lo más recomendado, apunten a 90% a pesar de que 100% no implique calidad, mientras más Coverage tenga pues menos probabilidades tiene de que el código tenga problemas. En Python se hace muy fácil con CopFailUnder90, este flag básicamente va a fallar el test de Continuous Integration si el Coverage baja a menos del 90%. Paralelización, esto ayuda muchísimo a agilizar las pruebas en Continuous Integration. En PyTest tenemos PyTest Exdist, que es una librería que es la que te permite hacer la paralelización. Y aquí hay una de las cosas que quería mostrarles, que voy a ejecutar la primera prueba que hice en la charla que fue el patch que lo hicimos a Start y funcionaba en los dos casos de prueba, pero esta vez la voy a ejecutar en dos cores. De hecho, si ejecutamos la prueba, ya aquí tenemos, va a fallar, a diferencia que al principio sí que pasaba y falla porque se está ejecutando en dos hilos diferentes, es decir el side effect que teníamos del patch ya no se va a ejecutar, o sea, ya no va a ocurrir en el segundo caso de prueba. Por eso es que le he querido mostrar este tipo de cosas, porque Continuous Integration empieza a ser indeterminístico cuando tenemos estas problemas en los casos de prueba. En Yankwa, es bastante sencillo, con el flag paralel lo podemos hacer. Y por último, reportes. Cuando tenemos Continuous Integration, por lo general queremos saber qué pasa. Uno de los reportes más importantes vendría siendo el reporte de Corel, que al ejecutarlo vemos por dónde ha pasado las pruebas. En este caso, como solo estoy ejecutando un solo archivo, que es el patch, vamos a ver que GitHub Client es el único por el que se pasó y tenemos un 67% de Coverage Man y tan bueno. Y por último, el reporte de Flaky o Heightzen Test, que vendría siendo si tu prueba pasa en tu Continuous Integration, luego no pasa y luego vuelve a pasar, es lo que llamamos Flaky Test, en cierta también se llama Heightzen Test o pruebas indeterminísticas. Entonces, este es el tipo de cosas que a tu mano en cuenta. No hay un Custom Solution para esto, como decir, Python Exist, pero sí que lo puedes implementar y es un reporte interesante a tener en tu Continuous Integration. Y bueno, con esto yo creo que daré final a mi charla. Muchísimas gracias por escucharme y bueno, puedo abrir la puerta para preguntas que tengan. Muchas gracias, Imel. Super interesante tu charla. Teníamos una preguntita que apareció por YouTube a mediados de tu charla más o menos relacionado a medir el tiempo máximo por text, por test. De todas formas, igual quería recordar a las personas que igual escriban sus preguntas por Discord, que por allá vamos a continuar la discusión, pero en caso si quieras decir algo relacionado al tiempo por test. Sí, ahí justamente, si no me equivoco, una librería de Python Test que te permite medir el tiempo entre las pruebas y como un step de Continuous Integration que es bastante interesante es acotarlo. Es decir, si decimos que nuestras pruebas máximo un segundo, digamos, por ejemplo, y esa prueba, ese step de Continuous Integration ya no va a pasar si tienes pruebas que duren más de ese tiempo. Esto a lo mejor hay que ver cómo se hace porque si tienes end-to-ends o pruebas que son, que implican no solamente Python, sino la UI, un segundo a lo mejor es poco, pero eso ya tienes que es un poco de fine tuning, dependiendo del Continuous Integration que tengas. Y es muy común escuchar a veces en charla relacionada a testing también este módulo que se llama Hypothesis. No sé si lo ha utilizado o si tiene alguna opinión con respecto a eso respecto a lo que presentaste hoy. Sí, Hypothesis es muy bueno, lo he visto para, sobre todo cuando estamos probando APIs. Cuando tenemos APIs definidas con OpenAPI justamente, una combinación que es como muy natural es Hypothesis y Eschematicis, que es básicamente probar tu esquema de OpenAPI y utilizar Hypothesis para generar esta prueba. Es el caso de uso que yo más he visto con Hypothesis. Bueno, hay un par de comentarios positivos en el canal que después lo puede leer, pero a la gente le gustó mucho tu charla, todos quedaron impresionados con FriskGun y muchos otros detalles. Así que, si, bueno, ya estamos casi hace en el tiempo. No me había dado cuenta buen nombre de ordenador. Lo estaba viendo yo. Muchas gracias, Himael, por participar de la API con 2021 y bueno, esperemos que podamos obtenerte quizás un durante el día en el canal de Discord para que continuamos la discusión, porque hay mucha gente con muchas opiniones y con ganas de ser mejor en testing. Vale, perfecto. Muchas gracias. Estoy en el canal y podemos hablar por ahí. Gracias. Perfecto. Muchas gracias, Himael, que tenga buen sado. Gracias.

