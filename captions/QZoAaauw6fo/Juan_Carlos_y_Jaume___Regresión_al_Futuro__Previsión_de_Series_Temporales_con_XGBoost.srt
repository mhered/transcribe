1
00:00:00,000 --> 00:00:29,000
Y trabajo en la PCL y soy el encargado del director del departamento de datos, y yaumes, uno de los que trabaja con nosotros es matemático y ya se presentará el...

2
00:00:29,000 --> 00:00:39,000
...lo que yo quiero mostrarle ahora es un proyecto que nosotros teníamos dentro de la empresa, el proyecto lo llamamos es el proyecto Temis,

3
00:00:39,000 --> 00:00:50,000
y es un proyecto que intenta lo que estábamos comentando, es un proyecto que lo que hace es hacer análisis temporales para poder hacer previsiones hacia futuro,

4
00:00:50,000 --> 00:00:57,000
dado teniendo una serie estudiando su comportamiento histórico o otras variables que nosotros le vamos a añadir, ¿vale?

5
00:00:57,000 --> 00:01:05,000
Entonces, perdón. Lo que quería decir eso es, esto es más o menos como un esquema general de lo que es el proyecto Temis,

6
00:01:05,000 --> 00:01:14,000
es un proyecto que nosotros damos desde servicio o se quiere dar que esto es un módulo desde un proyecto que va desde el tratamiento de datos,

7
00:01:14,000 --> 00:01:23,000
que es la recogida, limpia de datos, estructuración de datos y la explotación de los mismos, yo solo me voy a quedar con la última parte,

8
00:01:23,000 --> 00:01:32,000
que es simplemente como después que tú tienes todo un ciclo de datos bien montado, nosotros vamos a poder explotar los datos de manera pasiva o activa,

9
00:01:32,000 --> 00:01:43,000
pasiva es porque yo solo quiero hacer monitorización de los datos como me han llegado o porque yo proceso los datos y hago cualquier modelo de machine learning o deep learning o lo que ustedes quieran hacer.

10
00:01:43,000 --> 00:01:46,000
Entonces vamos a meternos en lo que yo quiero hacer.

11
00:01:46,000 --> 00:01:53,000
Todo mundo sabe que es una serie temporal, voy a pasar rápidamente, una serie temporal es un conjunto de datos ordenado de manera cronológica,

12
00:01:53,000 --> 00:01:58,000
¿qué queremos analizar? El comportamiento de una variable en un tiempo determinado, ¿ok?

13
00:01:58,000 --> 00:02:10,000
Eso es lo que tenemos allí. La serie temporal es lo regular, pueden tener dos partes cuando queremos analizar series temporales con componentes estocásticas.

14
00:02:10,000 --> 00:02:17,000
¿Por qué? Porque no siempre vamos a poder predecir con exactitud el futuro de una serie temporal y por eso es que son estocásticas.

15
00:02:17,000 --> 00:02:20,000
Entonces, ¿cómo se compone de eso? Se compone de dos partes.

16
00:02:20,000 --> 00:02:33,000
Una parte regular que son, la puedes componer, esta parte que se ve aquí arriba, se puede componer en cómo se comporta la tendencia, la estacionalidad y luego se separa de ahí la parte estocástica o de ruido.

17
00:02:33,000 --> 00:02:35,000
Y así es como yo compongo la serie.

18
00:02:35,000 --> 00:02:44,000
Si yo sumara estos tres componentes, recupero la serie de arriba o hay métodos de yo tener la serie de arriba y descomponerla en esas tres.

19
00:02:44,000 --> 00:02:47,000
Me toca una vez, ¿vale? Ok, muy bien.

20
00:02:47,000 --> 00:02:49,000
Y entonces es lo que vamos a hacer.

21
00:02:49,000 --> 00:03:00,000
Como les decía, los modelos tradicionales de tipo, modelos autoregresivos, que son un montón, y este es un libro bellísimo para estudiar, es bastante duro, ojo.

22
00:03:00,000 --> 00:03:04,000
Pero ahí está la Biblia del análisis de series temporales, ahí está todo.

23
00:03:04,000 --> 00:03:06,000
Para este tipo de modelos.

24
00:03:06,000 --> 00:03:09,000
Ahí están explicados todos esos que están aquí.

25
00:03:09,000 --> 00:03:14,000
Yo me voy a ir rápidamente, ¿cuál es el problema que tiene estos modelos cuando queremos poner estos sistemas en producción?

26
00:03:14,000 --> 00:03:16,000
Es un modelo que yo tengo que estudiar.

27
00:03:16,000 --> 00:03:19,000
¿Cuál es el comportamiento de las correlaciones?

28
00:03:19,000 --> 00:03:30,000
Que hay con cada uno, si veo la serie, voy mirando cada periodo hacia atrás o cada ventanita hacia atrás y tengo que ver qué partes están correlacionadas.

29
00:03:30,000 --> 00:03:37,000
Eso se hace estudiando los correlogramas que están aquí y hay toda una técnica para estudiar que no voy a caer en ese detalle.

30
00:03:37,000 --> 00:03:41,000
Pero el problema de eso, es que cuando yo quiero poner esos proyectos en producción,

31
00:03:41,000 --> 00:03:47,000
yo tengo que para poder encontrar cuál es la correlación que existen, tengo que cumplir una serie de condiciones.

32
00:03:47,000 --> 00:03:52,000
Si no es una serie estacionaria, la tengo que convertir estacionaria.

33
00:03:52,000 --> 00:04:06,000
Además tengo que saber cuáles son las correlaciones temporales que hay en tanto estacionalidad como de comportamientos que son picos arriba o picos abajo.

34
00:04:06,000 --> 00:04:09,000
Y eso se hace utilizando un montón estas técnicas.

35
00:04:09,000 --> 00:04:17,000
Son algoritmos muy complejos para encontrar las correlaciones, saber si cumplen algunas propiedades estadísticas.

36
00:04:17,000 --> 00:04:22,000
Las tengo que mirar una a una y a partir de ahí construir el modelo matemático.

37
00:04:22,000 --> 00:04:26,000
¿Cuál es ese modelo que yo tengo que construir? Este que está aquí abajo.

38
00:04:26,000 --> 00:04:37,000
Este modelo son un modelo de harima cualquiera en donde la primera parte es si la serie es estacional.

39
00:04:37,000 --> 00:04:56,000
Yo tengo que encontrar cuál es la variable décima que está aquí, el cual yo tengo que diferenciar la serie y restar un periodo menos el siguiente o el siguiente para que la serie que no sea estacional se vuelva estacional.

40
00:04:56,000 --> 00:05:03,000
Luego tengo los parámetros regresivos de decir cómo está correlacionado cada punto de la serie con respecto a su pasado.

41
00:05:03,000 --> 00:05:07,000
A un paso, a dos pasos, a tres pasos, a cuatro pasos es la ventana.

42
00:05:07,000 --> 00:05:11,000
Y un término de las medias móviles.

43
00:05:11,000 --> 00:05:15,000
Y luego el último pedazo es la parte estocástica.

44
00:05:15,000 --> 00:05:22,000
Entonces construir ese modelo implica hacer todo lo que yo comenté antes.

45
00:05:22,000 --> 00:05:34,000
Tenemos que necesitar los correlogramas, aplicar todos los test estadísticos, aplicar esos test estadísticos implica ir probando los errores uno a uno, aunque lo ejecuten en una máquina.

46
00:05:34,000 --> 00:05:38,000
Yo tengo que analizar una tabla a ver si ese es el mejor modelo sino probar otro modelo.

47
00:05:38,000 --> 00:05:40,000
Y eso es lo que yo voy a encontrar, cuáles son.

48
00:05:40,000 --> 00:05:49,000
El parámetro P, el parámetro Q que se ve aquí y el parámetro T que me describan mejor esa serie temporal para después hacer un modelo de predicción.

49
00:05:49,000 --> 00:05:53,000
Y luego tengo que probar el modelo de predicción a ver si ese modelo fue correcto.

50
00:05:53,000 --> 00:05:57,000
Entonces eso implica mucha tarea que no se puede automatizar para poner un modelo de predicción.

51
00:05:57,000 --> 00:06:02,000
Pues yo lo que quiero es leer la serie temporal y luego hago la predicción.

52
00:06:02,000 --> 00:06:07,000
Para eso aplicamos los modelos boosting.

53
00:06:07,000 --> 00:06:18,000
Yo no voy a dar en detalle los modelos boosting es buscar un algoritmo matemático que resuelva modelos sencillos, modelos de machine learning muy sencillo,

54
00:06:18,000 --> 00:06:23,000
pero que resuelven un pedacito del problema, después ejecutó el resultado de eso.

55
00:06:23,000 --> 00:06:30,000
Otro modelo es el mismo modelo sencillo con el resultado de optimización del anterior y lo fui sumando todos.

56
00:06:30,000 --> 00:06:39,000
Una vez que los tenga todos sumados, me voy a quedar con el mejor cómputo de resultado de todos ellos y construyo el modelo final.

57
00:06:39,000 --> 00:06:43,000
Aquí tengo una representación gráfica esa que está aquí.

58
00:06:43,000 --> 00:06:48,000
Yo quiero hacer un modelo de clasificación.

59
00:06:48,000 --> 00:06:52,000
Entonces yo voy a ir haciendo pedacitos de modelo y voy discriminando.

60
00:06:52,000 --> 00:07:03,000
Por ejemplo, si yo quiero separar la parte azul de la roja, yo voy discriminando cuando acierto con los azules y cuando acierto con los rojos.

61
00:07:03,000 --> 00:07:05,000
En un solo paso muy sencillito.

62
00:07:05,000 --> 00:07:12,000
Entonces es un modelo que no consume mucho tiempo ni consume mucha memoria y es muy fácil de ejecutar, independientemente de la cantidad de datos.

63
00:07:12,000 --> 00:07:23,000
Simplemente cuando hablamos del modelo XGBoost, lo que hace es que es una librería que ya está optimizada para que haga todos esos pasos, pero calculando árboles de decisión.

64
00:07:23,000 --> 00:07:40,000
Y lo que nosotros vamos a minimizar para poder encontrar el mejor score de este modelo es lo que se utiliza para calcular la discriminación en los árboles de decisión,

65
00:07:40,000 --> 00:07:46,000
que puede ser o el índice de Guinness o la distribución de Guinness o la pérdida de entropía.

66
00:07:46,000 --> 00:07:56,000
Esto es un detalle que las dejo, pero simplemente el modelo XGBoost lo que hace es aplicar este algoritmo, este algoritmo de boosting o el gradient boosting,

67
00:07:56,000 --> 00:07:59,000
de tal manera que están bien optimizados para el XGBoost.

68
00:07:59,000 --> 00:08:02,000
Aquí está la forma matemática, es lo mismo.

69
00:08:02,000 --> 00:08:11,000
Lo que pasa es que arriba yo quiero optimizar esa ecuación que está ahí, FM son los M-pasos que yo necesito, los M-arbolitos que yo voy construyendo uno a uno,

70
00:08:11,000 --> 00:08:18,000
y la suma es la suma del score que lele, la ecuación de pérdida que yo voy a optimizar,

71
00:08:18,000 --> 00:08:25,000
y en este caso la optimización de pérdida que yo estoy usando para este modelo es la ganancia de entropía o la pérdida de entropía.

72
00:08:25,000 --> 00:08:35,000
Aquí viene el core más importante, para que esto funcione yo tengo que poner una estructura de dato de Machine Learning que él pueda entender

73
00:08:35,000 --> 00:08:41,000
cómo es la matriz de datos, yo quiero leer a partir de una sola serie temporal que yo quiero mandarles.

74
00:08:41,000 --> 00:08:51,000
Yo elijo mi serie temporal, y ahora yo le voy a pasar un conjunto de parámetros que él me va a concatenar una matriz que tiene básicamente estas partes.

75
00:08:51,000 --> 00:09:00,000
Yo lo que voy a hacer es una primera concatenación que va a ser una primera matriz en donde yo voy a poner ahí todos los LAC que yo voy a considerar.

76
00:09:00,000 --> 00:09:06,000
¿Qué quiere decir los LAC? Que yo voy a poner mi serie temporal mentra, una serie temporal, y yo digo pues ahora yo quiero que tú me escribas una matriz

77
00:09:06,000 --> 00:09:13,000
donde cada posición es el retraso 1, retraso 2, retraso 3, retraso 4, en el retraso, los retrasos que yo quiera.

78
00:09:13,000 --> 00:09:26,000
Luego puedo agregar otra matriz que lo que no tiene los modelos anteriores de los modelos autoregresivos es que la mayoría son univariables.

79
00:09:26,000 --> 00:09:32,000
Es decir, puedo estudiar la serie temporal. Si yo quiero ver cuál es el comportamiento cruzado de otra variabilidad externa,

80
00:09:32,000 --> 00:09:39,000
no lo puedo hacer porque cuesta mucho meterlo y tendría que estudiar modelos multivariantes autoregresivos.

81
00:09:39,000 --> 00:09:49,000
Es decir, es la muerte para poner en producción. Y luego, y luego tenemos otras variables que nosotros podemos considerar,

82
00:09:49,000 --> 00:10:00,000
que por ejemplo en el caso que estamos analizando series de ventas, pues puede ser calendarios de eventos que yo sé que me pueden impactar

83
00:10:00,000 --> 00:10:07,000
dentro de la serie que estoy estudiando. Por ejemplo, si yo estoy vendiendo, quiero ver cuál es el comportamiento de mi serie temporal

84
00:10:07,000 --> 00:10:15,000
de una floristería, pues yo puedo decir aquí qué pasa cuando es aniversario, día de los enamorados, lo que sea, cualquier, de forever.

85
00:10:15,000 --> 00:10:21,000
Y eso es lo que queremos hacer. Entonces hacemos esto. Aquí hay una cosa curiosa que nosotros hacemos en el modelo, por ejemplo,

86
00:10:21,000 --> 00:10:29,000
si la segunda variable, yo estoy prediciendo ventas y la segunda variable o la segunda matriz que yo quiero poner es hablar

87
00:10:29,000 --> 00:10:37,000
de cuál es el tráfico de las personas que van a la tienda, es una variable que claro yo no sé en el futuro cuánta gente va a ir.

88
00:10:37,000 --> 00:10:44,000
Entonces, nosotros tenemos preparado el modelo que la hace como una segunda predicción interna, aunque sea menos exacta,

89
00:10:44,000 --> 00:10:52,000
pero la incluye dentro del modelo para que lo considere cuando voy a hacerla desde el futuro, cómo se estaría comportando esa segunda variable.

90
00:10:52,000 --> 00:11:00,000
Entonces, eso es importante porque hace unas buenas predicciones, pero tengo el error acumulado ahora de las dos series.

91
00:11:00,000 --> 00:11:06,000
Perdemos un poco de precisión, pero ganamos en poder incluir todas estas variables en nuestro modelo.

92
00:11:06,000 --> 00:11:13,000
Básicamente, esto luciría de esta manera. Yo concateno aquí la serie temporal, es la primera columna.

93
00:11:13,000 --> 00:11:22,000
Yo le digo, pues yo quiero que me haga los lag de los primeros cinco puntos, después a las 35 semanas, al año, a los seis meses,

94
00:11:22,000 --> 00:11:26,000
yo construyo esa que la vamos a llamar los parámetros autoregresivos.

95
00:11:26,000 --> 00:11:32,000
Luego, la segunda variable que ustedes veían ahí que parecía que era la variable de tráfico,

96
00:11:32,000 --> 00:11:40,000
pasa por un segundo ciclo interno de nuestro modelo que hace la predicción y le añade a esta concatenación de estos parámetros.

97
00:11:40,000 --> 00:11:47,000
Y luego las otras variables, por ejemplo, esto que está aquí, que no sé, bueno, la última, había una última cajita,

98
00:11:47,000 --> 00:11:55,000
y esa última cajita que sería el calendario de actividades de la promoción de un producto,

99
00:11:55,000 --> 00:12:01,000
cuando lo voy a sacar a quienes van dirigidos, la estacionalidad, el producto, lo que sea.

100
00:12:01,000 --> 00:12:09,000
Y bueno, con esto ya terminaba mi parte, quería mostrar lo último, que era cómo funcionaba,

101
00:12:09,000 --> 00:12:14,000
y solamente aquí está entonces, aquí lo que se tienen que quedar es con lo siguiente.

102
00:12:14,000 --> 00:12:19,000
Yo ahora estoy promoniendo un modelo que es el XGBoost, ustedes lo pueden poner, es muy sencillo,

103
00:12:19,000 --> 00:12:27,000
después tenemos que hacer todo el autotune, indicado en los parámetros, pero eso es ya controlar el modelo del XGBoost,

104
00:12:27,000 --> 00:12:38,000
pero nosotros podemos poner todos, podemos construir esa matriz que es la parte artística de cómo introducir este modelo,

105
00:12:38,000 --> 00:12:43,000
es cómo se han ido concatenando todas esas variables, vale, entonces eso es lo interesante.

106
00:12:43,000 --> 00:12:49,000
Eso tiene un problema, perdón, esto tiene una ventaja, y luego les digo que el problema primero lo bueno,

107
00:12:49,000 --> 00:12:56,000
eso tiene una ventaja, la ventaja es que fíjense que anteriormente yo tenía que buscar con un querrograma

108
00:12:56,000 --> 00:13:04,000
y hacer analizar las variables estadísticas una a una, mirando cómo se comportaban los errores aquí.

109
00:13:04,000 --> 00:13:12,000
Estos árboles de decisión, estos árboles de decisión,

110
00:13:12,000 --> 00:13:19,000
vengo acá, estos árboles de decisión me van a permitir a mí encontrar todas las correlaciones

111
00:13:19,000 --> 00:13:25,000
y yo voy a poder construir todas estas dos partes que tengo aquí,

112
00:13:25,000 --> 00:13:32,000
porque voy a conseguir todas las correlaciones que yo voy a encontrar a nivel de las medias móviles y a nivel de los retardos,

113
00:13:32,000 --> 00:13:37,000
y no tengo que hacer ninguno de esos estudios que yo había dicho,

114
00:13:37,000 --> 00:13:42,000
y bueno, yo sé que esto ha estado un poco desordenado, no sé si se entendió lo que queríamos hacer,

115
00:13:42,000 --> 00:13:49,000
y voy a intentar explicar la implementación de este sistema y los casos de estudio que hemos hecho,

116
00:13:49,000 --> 00:13:56,000
pero es importante decir aquí, la tarea de diseño que nosotros hemos hecho, que esto ya va a estar automatizado,

117
00:13:56,000 --> 00:14:03,000
fue simplemente escribir una serie temporal y convertirla en una matriz concatenada de un conjunto de datos,

118
00:14:03,000 --> 00:14:10,000
transformarlo en un conjunto de datos para modelos de aprendizaje supervisados,

119
00:14:10,000 --> 00:14:14,000
esto es lo que es el punto final.

120
00:14:14,000 --> 00:14:21,000
Bueno, soy yauma, ya os han dicho que tengo una parte de Superheroes,

121
00:14:21,000 --> 00:14:25,000
pero es una vía secreta que no os voy a contar ahora, porque si no os tendría que matar a todos,

122
00:14:25,000 --> 00:14:33,000
no, no, no, bueno, entonces, claro, una vez tenemos toda la parte teórica que hemos entendido perfectamente,

123
00:14:33,000 --> 00:14:38,000
tenemos que hacer un app, y bueno, no es muy difícil saber lo que tenemos que hacer,

124
00:14:38,000 --> 00:14:43,000
necesitamos un núcleo que nos orqueste todo lo que ha comentado Juan Carlos,

125
00:14:43,000 --> 00:14:48,000
necesitamos una interfaz para que el usuario se pueda comunicar con este núcleo

126
00:14:48,000 --> 00:14:55,000
y pueda poner, bueno, entrar toda la serie temporal o variables y parámetros que necesite,

127
00:14:55,000 --> 00:15:00,000
y necesitamos pues una herramienta de análisis y visualización de esos resultados,

128
00:15:00,000 --> 00:15:09,000
¿no? Y esto, hui, esto bueno, lo vemos aquí un poco más en grande,

129
00:15:09,000 --> 00:15:14,000
lo que vamos a necesitar evidentemente es la serie temporal, los LAX que hablaba,

130
00:15:14,000 --> 00:15:20,000
cuántos vamos a necesitar, los hiperparámetros de entrenamiento que necesita e-boots,

131
00:15:20,000 --> 00:15:27,000
que podemos hacerlo pues manual, si creemos saber cuál es,

132
00:15:27,000 --> 00:15:32,000
vamos a necesitar para esa serie temporal, pero también podemos poner una serie de parámetros

133
00:15:32,000 --> 00:15:38,000
que el mismo modelo va a buscar cuáles son los mejores dentro de los que le hemos dado,

134
00:15:38,000 --> 00:15:44,000
también necesitamos pues los pronósticos independientes de la segunda variable,

135
00:15:44,000 --> 00:15:49,000
de la que estaba hablando Juan Carlos, algunos parámetros adicionales,

136
00:15:49,000 --> 00:15:53,000
campos adicionales como en el caso de las ventas, cuando va a haber rebajas,

137
00:15:53,000 --> 00:16:00,000
porque va a ser interesante porque eso sí que va a ayudar a saber lo a predecir,

138
00:16:00,000 --> 00:16:07,000
por ejemplo, ventas, y esto lo vamos a servir todo a lo que hemos en medio,

139
00:16:07,000 --> 00:16:13,000
que básicamente vamos a hacer todos estos cálculos, aquí vamos a calcular los LAX,

140
00:16:13,000 --> 00:16:18,000
vamos a hacer el análisis y vamos a buscar los parámetros mejores,

141
00:16:18,000 --> 00:16:22,000
de estos hiperparámetros de entrenamiento, y habrá una salida que bien guardemos

142
00:16:22,000 --> 00:16:29,000
en base de datos o en fichero y que serviremos para una interfaz gráfica

143
00:16:29,000 --> 00:16:32,000
para que el análisis sea más fácil, ¿vale?

144
00:16:32,000 --> 00:16:39,000
Entonces, para hacer el orchestador lo que hemos decidido, o lo que hemos usado es,

145
00:16:39,000 --> 00:16:45,000
quiero, lo que es una librería de Python que mantenía por Quantum Black,

146
00:16:45,000 --> 00:16:51,000
que ayuda bastante, que está indicada para PyLands de MLOs, ¿vale?

147
00:16:51,000 --> 00:16:57,000
Entre otras cosas, ayuda a tener un código fácil de mantener,

148
00:16:57,000 --> 00:17:05,000
bien estructurado para pasar a producción, y nos ayuda, como podéis ver,

149
00:17:05,000 --> 00:17:10,000
a tener una pipeline clara para entender lo que está pasando, ¿vale?

150
00:17:10,000 --> 00:17:14,000
En este caso no se puede leer muy bien, pero básicamente lo que tenemos arriba

151
00:17:14,000 --> 00:17:20,000
es el tratamiento de parámetros, todo seguido sería el tratamiento de la variable adicional,

152
00:17:20,000 --> 00:17:26,000
y su predicción si se hace falta, que alimenta nuestro modelo a la izquierda,

153
00:17:26,000 --> 00:17:34,000
y devuelve tanto métricas como la predicción para que,

154
00:17:34,000 --> 00:17:39,000
poderla mostrar después, en este caso este ejemplo lo guarda en un JSON,

155
00:17:39,000 --> 00:17:45,000
donde tenemos todos estos datos que vamos a poder visualizar después.

156
00:17:45,000 --> 00:17:48,000
Quedro nos permite hacerlo de manera bastante granular,

157
00:17:48,000 --> 00:17:52,000
o sea, aquí podemos ver también los inputs y outputs de cada proceso,

158
00:17:52,000 --> 00:17:57,000
lo que pasa, bueno, si se viera bien, en verde podéis verlo todo,

159
00:17:57,000 --> 00:18:02,000
que acaba todo centralizándose la salida de los datos que vamos...

160
00:18:02,000 --> 00:18:05,000
En verde porque está todo válido, ¿eh?

161
00:18:05,000 --> 00:18:11,000
Bueno, entonces para conectar todo esto necesitamos un API,

162
00:18:11,000 --> 00:18:17,000
la herramienta que hemos decidido usar es FastAppy, hace muy fácil la creación de esta API,

163
00:18:17,000 --> 00:18:22,000
nos da esta interfaz tan bonita para que el usuario pueda hacer pruebas,

164
00:18:22,000 --> 00:18:27,000
entonces tener una query que nos va a servir para automatizar estas predicciones.

165
00:18:27,000 --> 00:18:31,000
Por ejemplo, en el mismo caso de la web de predecir ventas,

166
00:18:31,000 --> 00:18:35,000
podemos tener una query que cada semana,

167
00:18:35,000 --> 00:18:40,000
calcule la predicción para la semana que viene, fácilmente,

168
00:18:40,000 --> 00:18:44,000
y tener esta predicción con los nuevos datos que llegan,

169
00:18:44,000 --> 00:18:49,000
al entrar en nuestro Quedro, en este caso, en nuestro Odyssey.

170
00:18:49,000 --> 00:18:55,000
Para mostrar los datos, pues elegimos Plotty, Dash,

171
00:18:55,000 --> 00:19:00,000
que deja hacer estos Dash bobas de manera muy fácil,

172
00:19:00,000 --> 00:19:04,000
además puedes hacer Zoom y ver los datos más claramente.

173
00:19:04,000 --> 00:19:07,000
En este caso, seguimos con ejemplo de las ventas,

174
00:19:07,000 --> 00:19:12,000
en verde Post-Feed estaría la predicción, ¿vale?

175
00:19:12,000 --> 00:19:17,000
En verde, en lo que da Zoom y ahora es verde más elfo,

176
00:19:17,000 --> 00:19:22,000
creo tenemos lo que sería en los datos reales, ¿vale?

177
00:19:22,000 --> 00:19:25,000
Y aquí, en estos gráficos que podéis ver,

178
00:19:25,000 --> 00:19:29,000
son el peso que tiene cada una de las entradas en esta predicción.

179
00:19:29,000 --> 00:19:35,000
En este caso, la que tiene mayor peso es la variable,

180
00:19:35,000 --> 00:19:39,000
la segunda variable, la variable de soporte,

181
00:19:39,000 --> 00:19:42,000
es el tráfico de la página web, ¿vale?

182
00:19:42,000 --> 00:19:47,000
Entonces, esto sirve para analizar de dónde vienen las ventas

183
00:19:47,000 --> 00:19:51,000
y una persona que esté en negocio podrá tomar decisiones

184
00:19:51,000 --> 00:19:55,000
porque también en una de estas variables también hay rebajas de Navidad,

185
00:19:55,000 --> 00:19:58,000
por ejemplo, y cosas parecidas, ¿vale?

186
00:19:58,000 --> 00:20:01,000
Esto, de todas maneras, que ya tenemos todas estas partes,

187
00:20:01,000 --> 00:20:06,000
los juntamos todos en una pantalla en negro.

188
00:20:06,000 --> 00:20:09,000
¿Lo juntamos todos en un llango?

189
00:20:09,000 --> 00:20:12,000
En llango, no, tampoco.

190
00:20:12,000 --> 00:20:15,000
Lo juntamos todo en un llango.

191
00:20:15,000 --> 00:20:20,000
Que de todas maneras, bueno, la entrada de datos sería un poco como ese formulario

192
00:20:20,000 --> 00:20:23,000
que yo había visto de FastApi, ¿vale?

193
00:20:23,000 --> 00:20:27,000
La ventaja es que tendremos la interfaz gráfica enseguida,

194
00:20:27,000 --> 00:20:30,000
es decir, no tendremos que descargar los datos

195
00:20:30,000 --> 00:20:34,000
y servirla en el dashboard anterior.

196
00:20:34,000 --> 00:20:41,000
También, pues, lo que nos sirve es para guardar todos los datos que hemos usado

197
00:20:41,000 --> 00:20:46,000
para esta predicción en concreto, tanto los datos de entrada

198
00:20:46,000 --> 00:20:50,000
como las predicciones que nos permiten hacer el gráfico.

199
00:20:50,000 --> 00:20:58,000
Los guardaremos y podemos volver atrás, ver lo que hemos hecho,

200
00:20:58,000 --> 00:21:02,000
reproducir el mismo experimento porque se guarda todo

201
00:21:02,000 --> 00:21:06,000
y intentar reproducir, cambiar parámetros.

202
00:21:06,000 --> 00:21:13,000
También, y esto es una parte que es muy bonita, ya lo veréis.

203
00:21:13,000 --> 00:21:17,000
Se lo están imaginando.

204
00:21:17,000 --> 00:21:21,000
No solo sacamos las peticiones, también sacamos métricas estadísticas.

205
00:21:21,000 --> 00:21:25,000
Tenemos métricas estadísticas para que ayuden bastante

206
00:21:25,000 --> 00:21:28,000
en el tema de analizar lo que vemos ahí.

207
00:21:28,000 --> 00:21:31,000
Es decir, no basta sacar un gráfico, tenemos que saber el error,

208
00:21:31,000 --> 00:21:34,000
tenemos que saber la media y todo lo demás.

209
00:21:34,000 --> 00:21:37,000
Estos datos también los damos con el modelo.

210
00:21:37,000 --> 00:21:43,000
Y, bueno, y es muy bonito.

211
00:21:43,000 --> 00:21:47,000
Bueno, y básicamente eso sería la estructura del app.

212
00:21:47,000 --> 00:21:51,000
Tenemos una interfaz, tenemos el núcleo

213
00:21:51,000 --> 00:21:57,000
y tenemos casos que nos gusta enseñaros de...

214
00:21:57,000 --> 00:22:01,000
Bueno, bueno, bueno, bueno, vamos a poner el PDF para que ustedes puedan

215
00:22:01,000 --> 00:22:06,000
en la grabación lo que decimos, intentarlo a coplarlo a lo que están en el PDF.

216
00:22:06,000 --> 00:22:11,000
Y teníamos el holograma también, que queríamos mostrarlos en 3D.

217
00:22:11,000 --> 00:22:17,000
Yo estaba montando en un pequeño pony que subía una serie temporal.

218
00:22:17,000 --> 00:22:19,000
Ya está, esto está grabado.

219
00:22:19,000 --> 00:22:22,000
Esto está grabado, vale, vale.

220
00:22:22,000 --> 00:22:25,000
Bueno, es el único, es igual.

221
00:22:25,000 --> 00:22:27,000
Bueno, no sé.

222
00:22:27,000 --> 00:22:29,000
Y, bueno, y...

223
00:22:29,000 --> 00:22:33,000
Abrimos la serie de preguntas y después ya veremos si...

224
00:22:33,000 --> 00:22:35,000
No hemos podido predecir esto, ¿eh?

225
00:22:35,000 --> 00:22:37,000
Exacto, eso.

226
00:22:37,000 --> 00:22:40,000
Y, bueno, voy a poner el... Perdón.

227
00:22:40,000 --> 00:22:46,000
Este es un proceso estocástico

228
00:22:46,000 --> 00:22:51,000
con muchas contingencias que nos han pasado aquí, pero bueno.

229
00:22:51,000 --> 00:22:55,000
¿Qué tal, Luis? Porque sí. Perdón, perdón.

230
00:22:55,000 --> 00:22:59,000
Sí, sí, no sé si tiene alguna pregunta. Yo sé que es difícil porque... Sí.

231
00:22:59,000 --> 00:23:01,000
Sí.

232
00:23:01,000 --> 00:23:03,000
Por el micrófono.

233
00:23:03,000 --> 00:23:06,000
Sí. Perdón.

234
00:23:06,000 --> 00:23:09,000
¿Qué tiene que hacer con el micrófono?

235
00:23:09,000 --> 00:23:13,000
¿Oye? Sí, sí.

236
00:23:13,000 --> 00:23:15,000
Tengo alguna preguntilla, sí.

237
00:23:15,000 --> 00:23:21,000
He comentado que utilizabéis los lacs, entiendo como variables explicativas del modelo, ¿no?

238
00:23:21,000 --> 00:23:27,000
Has mencionado que usabais pues ACF y PACF, me ha parecido ver la diapositiva.

239
00:23:27,000 --> 00:23:28,000
Sí.

240
00:23:28,000 --> 00:23:30,000
Para elegir los mejores lacs.

241
00:23:30,000 --> 00:23:31,000
Sí.

242
00:23:31,000 --> 00:23:35,000
¿Cómo lo haces? Porque el PACF únicamente busca la correlación lineal,

243
00:23:35,000 --> 00:23:39,000
o la autocorrelación lineal, quitando la cruz a compuesto autogonal.

244
00:23:39,000 --> 00:23:46,000
¿Cómo hacéis para elegir los mejores lacs, evitando también que estén las variables explicativas con las entres?

245
00:23:46,000 --> 00:23:48,000
Sí. Me gustaría entender un poco...

246
00:23:48,000 --> 00:23:49,000
Vale. Está bueno.

247
00:23:49,000 --> 00:23:51,000
Perfecto. En los modelos a Arima...

248
00:23:51,000 --> 00:23:53,000
Perdón.

249
00:23:53,000 --> 00:23:55,000
Sorry. Que estoy...

250
00:23:55,000 --> 00:23:57,000
Bien, bien, bien.

251
00:23:57,000 --> 00:24:01,000
Vale. Este es el problema que tiene todos los modelos autoregresivos o ni variante.

252
00:24:01,000 --> 00:24:03,000
Ese es el... Quite la cuestión.

253
00:24:03,000 --> 00:24:11,000
Lo que yo había comentado de las autocorrelaciones y las correlaciones locales y globales o parciales o autocorrelaciones parciales,

254
00:24:11,000 --> 00:24:12,000
tiene que ver con esto.

255
00:24:12,000 --> 00:24:14,000
Y esto tú tienes que hacer.

256
00:24:14,000 --> 00:24:16,000
Y sí tienes que buscar una correlación lineal.

257
00:24:16,000 --> 00:24:17,000
Y tienes que ver.

258
00:24:17,000 --> 00:24:20,000
Entonces como tú veas cómo es el comportamiento de la serie,

259
00:24:20,000 --> 00:24:24,000
si la serie es cíclica, pues tú ves que el correlograma es cíclico también, no te sirve.

260
00:24:24,000 --> 00:24:28,000
Entonces tienes que tratar la serie previamente haciendo diferenciación o lo que sea.

261
00:24:28,000 --> 00:24:31,000
Pero esa es la técnica que yo había dicho que está en el libro.

262
00:24:31,000 --> 00:24:33,000
Entonces no es lo que yo había hablado aquí.

263
00:24:33,000 --> 00:24:38,000
Lo que nosotros queremos es que método podemos sustituir eso de no tener que hacer ese trabajo.

264
00:24:38,000 --> 00:24:42,000
Porque ese trabajo nos permite calcular todos los estadísticos que necesitamos.

265
00:24:42,000 --> 00:24:43,000
Tenemos que calcular.

266
00:24:43,000 --> 00:24:49,000
Los estadísticos es conseguir toda la estadística de Fuehler

267
00:24:49,000 --> 00:24:52,000
para ver cómo se comportan los errores para ver las correlaciones.

268
00:24:52,000 --> 00:24:54,000
Y eso lo tienes que hacer a mano.

269
00:24:54,000 --> 00:24:57,000
Proponer un modelo y después que propones el modelo,

270
00:24:57,000 --> 00:25:01,000
tú niar ese modelo que has propuesto y después ver cómo es el comportamiento predictivo.

271
00:25:01,000 --> 00:25:02,000
Son cinco pasos.

272
00:25:02,000 --> 00:25:05,000
Eso lo evitamos utilizando el XGBoost.

273
00:25:05,000 --> 00:25:07,000
Entonces, ¿qué es lo que hace el XGBoost?

274
00:25:07,000 --> 00:25:12,000
El XGBoost al final es un árbol de decisiones que está ahí programado, que es un discriminador.

275
00:25:12,000 --> 00:25:16,000
Y lo que va es buscando, va ordenando todas las variables,

276
00:25:16,000 --> 00:25:19,000
de tal manera que va a haber, cuando tú construyas el modelo,

277
00:25:19,000 --> 00:25:22,000
cómo están correlacionadas a cada uno de ellos.

278
00:25:22,000 --> 00:25:24,000
No, mira, que yo quería usar la tiza.

279
00:25:24,000 --> 00:25:27,000
Es que no tengo, por cierto.

280
00:25:27,000 --> 00:25:29,000
No he pegado una.

281
00:25:29,000 --> 00:25:33,000
¿Vale? La idea es, yo tengo el modelo.

282
00:25:33,000 --> 00:25:39,000
Entonces yo tenía la parte diferencial, con las constantes,

283
00:25:39,000 --> 00:25:45,000
más otra constante con la suma de la parte de retardos,

284
00:25:45,000 --> 00:25:55,000
que es la variable t-p, perdón, t-i, más la de las medias móviles,

285
00:25:55,000 --> 00:26:04,000
que la habíamos llamado Q, J, igual a 1, hasta y, t-j, más la parte estocástica.

286
00:26:04,000 --> 00:26:07,000
La parte estocástica hay que analizarle eso.

287
00:26:07,000 --> 00:26:10,000
Y yo tengo que asegurar que sea un ruido blanco.

288
00:26:10,000 --> 00:26:15,000
No me da tiempo de explicarlo, pero lo pueden buscar, es que propiedad debe seguir.

289
00:26:15,000 --> 00:26:19,000
Y los correlogramas me permiten ver cuáles son estas relaciones.

290
00:26:19,000 --> 00:26:24,000
Utilizarle que YBUS, lo que hago es que utilizando árboles de decisión,

291
00:26:24,000 --> 00:26:26,000
él va haciendo ese árbol de decisión,

292
00:26:26,000 --> 00:26:32,000
yo me voy quedando con el mejor resultado y voy sumando los que son mejores.

293
00:26:32,000 --> 00:26:35,000
Y eso es lo que me va a decir cuáles son las correlaciones,

294
00:26:35,000 --> 00:26:42,000
cuando lo poquito que él se vio que tenía distribuido un gráfico de barritas que tenían ahí.

295
00:26:42,000 --> 00:26:50,000
Estos simplemente son las variables explicativas que hemos puesto en el modelo,

296
00:26:50,000 --> 00:26:53,000
que son todos estos términos desarrollados en la suma,

297
00:26:53,000 --> 00:26:56,000
todas las constantes que aparecen aquí en las sumas.

298
00:26:56,000 --> 00:26:58,000
Y así es como yo podemos ver.

299
00:26:58,000 --> 00:27:01,000
Y eso es lo que te va a decir por qué la variable primera que le encontró

300
00:27:01,000 --> 00:27:04,000
no fue ninguno de los lag, sino la variable primera fue

301
00:27:04,000 --> 00:27:08,000
la variable externa de la predicción de la venta de la gente que iba.

302
00:27:08,000 --> 00:27:11,000
Entonces hemos sustituido un modelo por el otro.

303
00:27:11,000 --> 00:27:17,000
Obviamente, perdón, que los modelos Arimas son mejores para predecir.

304
00:27:17,000 --> 00:27:22,000
Tiene mejor, cuando tú haces todas las métricas estadísticas, son mejores los resultados,

305
00:27:22,000 --> 00:27:25,000
pero cuesta mucho mantenerlo en producción.

306
00:27:25,000 --> 00:27:28,000
Y esto te dan nuestro modelo que lo hemos probado,

307
00:27:28,000 --> 00:27:32,000
que no pudimos mostrarle con un conjunto de datos reales de cliente.

308
00:27:32,000 --> 00:27:35,000
De hecho, teníamos una predicción muy interesante,

309
00:27:35,000 --> 00:27:39,000
porque teníamos una predicción de una serie que iba, vino la pandemia,

310
00:27:39,000 --> 00:27:43,000
después se recuperaron las ventas y nosotros pudimos hacer la predicción,

311
00:27:43,000 --> 00:27:48,000
aún considerando cómo se habían caído el comportamiento.

312
00:27:50,000 --> 00:27:52,000
Hay otra pregunta.

313
00:27:52,000 --> 00:27:55,000
Entonces son dos modelos que uno sustituye al otro.

314
00:27:55,000 --> 00:27:58,000
Vale.

315
00:28:00,000 --> 00:28:03,000
Si hay años que no escribían un pizarro, ¿eh?

316
00:28:03,000 --> 00:28:08,000
Pero en la cámara hay una cosa de la gráfica que has puesto izquierda.

317
00:28:08,000 --> 00:28:11,000
Si, supongo que hay un 2, que hay una variable DOOMNY o algo,

318
00:28:11,000 --> 00:28:15,000
porque cuando vino el coronavirus había un escalón, por ejemplo,

319
00:28:15,000 --> 00:28:18,000
la serie temporales del paro.

320
00:28:18,000 --> 00:28:20,000
No, porque...

321
00:28:20,000 --> 00:28:22,000
Entonces, yo en los modelos que hacían series temporales,

322
00:28:22,000 --> 00:28:25,000
la variable DOOMNY, para ver ese cambio,

323
00:28:25,000 --> 00:28:28,000
y me estás comentando que gracias a la librería con vuestro modelo,

324
00:28:28,000 --> 00:28:30,000
eso no hace falta.

325
00:28:30,000 --> 00:28:32,000
No, no, no lo he dicho eso.

326
00:28:32,000 --> 00:28:34,000
Lo que he dicho es que yo había...

327
00:28:34,000 --> 00:28:36,000
Perdón.

328
00:28:40,000 --> 00:28:42,000
Vale, no, no es exactamente eso lo que he dicho.

329
00:28:42,000 --> 00:28:46,000
Lo que he dicho es que la serie temporal histórica

330
00:28:46,000 --> 00:28:49,000
se comportaba de esta manera.

331
00:28:49,000 --> 00:28:53,000
Yo podría haber considerado otra variable externa

332
00:28:53,000 --> 00:28:55,000
que simulara la pandemia,

333
00:28:55,000 --> 00:28:59,000
pero como yo no sabía que estaba y no sabía cómo simularla,

334
00:28:59,000 --> 00:29:02,000
esta variable son ventas online,

335
00:29:02,000 --> 00:29:05,000
y aunque hayan sido muy bajas las ventas, siempre había ventas.

336
00:29:05,000 --> 00:29:08,000
Y esto ya pertenecía a la serie temporal.

337
00:29:08,000 --> 00:29:11,000
Entonces, lo que hace nuestro modelo es conseguir todas las correlaciones.

338
00:29:11,000 --> 00:29:13,000
Fíjense que este modelo,

339
00:29:13,000 --> 00:29:15,000
de la gente que ha utilizado los modelos de Arima,

340
00:29:15,000 --> 00:29:19,000
deja de ser estacionaria,

341
00:29:19,000 --> 00:29:21,000
tiene tendencias,

342
00:29:21,000 --> 00:29:23,000
y entonces yo tendría que hacer diferenciación.

343
00:29:23,000 --> 00:29:26,000
¿Cuántas diferenciaciones para volver esto estacionario?

344
00:29:26,000 --> 00:29:28,000
Vale.

345
00:29:28,000 --> 00:29:30,000
Ah, perdón.

346
00:29:30,000 --> 00:29:32,000
¿Llegó? Sí, no, no llegó.

347
00:29:32,000 --> 00:29:35,000
Y entonces, claro, aquí lo que hicimos fue

348
00:29:35,000 --> 00:29:39,000
solo consiguiendo meter esta serie temporal,

349
00:29:39,000 --> 00:29:43,000
hemos tratado los datos para convertirlos en un conjunto de datos

350
00:29:43,000 --> 00:29:46,000
de Machine Learning

351
00:29:46,000 --> 00:29:49,000
para modelos supervisados,

352
00:29:49,000 --> 00:29:52,000
de tal manera que con el discriminador que usa el XGBoost

353
00:29:52,000 --> 00:29:54,000
pueda conseguir todas estas correlaciones.

354
00:29:54,000 --> 00:29:56,000
Y esa es la gráfica que me gustaría mostrar,

355
00:29:56,000 --> 00:29:58,000
porque ahí quedaría bastante.

356
00:29:58,000 --> 00:30:01,000
Que se ve

357
00:30:01,000 --> 00:30:03,000
cómo fue esa predicción.

358
00:30:03,000 --> 00:30:05,000
No es muy buena, obviamente,

359
00:30:05,000 --> 00:30:08,000
pero sí da el comportamiento y la tendencia.

360
00:30:08,000 --> 00:30:11,000
Pero no es exactamente eso.

361
00:30:11,000 --> 00:30:13,000
Aunque nuestro modelo está hecho para nosotros,

362
00:30:13,000 --> 00:30:15,000
nosotros llamamos escenarios,

363
00:30:15,000 --> 00:30:17,000
las variables dummy,

364
00:30:17,000 --> 00:30:19,000
tú puedes introducir escenarios también,

365
00:30:19,000 --> 00:30:21,000
catastróficos o menos catastróficos si quieres.

366
00:30:21,000 --> 00:30:23,000
Y también los podría considerar.

367
00:30:23,000 --> 00:30:25,000
Vale.

368
00:30:25,000 --> 00:30:27,000
¿Tenía alguna pregunta más?

369
00:30:27,000 --> 00:30:42,000
Podemos darle un aplauso a todos.

