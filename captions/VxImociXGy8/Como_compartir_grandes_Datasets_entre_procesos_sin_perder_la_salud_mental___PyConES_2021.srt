1
00:00:00,000 --> 00:00:16,000
Bueno, ya estamos aquí. Al fin y al final se me ha extendido con las preguntas.

2
00:00:16,000 --> 00:00:20,000
Sí, sí, estamos ahí como... si fijan en el tiempo, ¿no?

3
00:00:20,000 --> 00:00:24,000
Me ha parecido tan guay el tema que digo voy a me pondría aquí a hablar todo el rato.

4
00:00:24,000 --> 00:00:32,000
Bueno, vamos con la siguiente charla, ¿vale? Que se llama ¿Cómo compartir grandes data sets entre procesos sin perder la salud mental?

5
00:00:32,000 --> 00:00:37,000
Y bueno, trata un poco sobre eso, cómo trabajar con lo que deja esto. Big data, de verdad.

6
00:00:37,000 --> 00:00:41,000
Bueno, no sé si es Big data porque si la Wikipedia no ha Big data, esto tampoco.

7
00:00:41,000 --> 00:00:46,000
Pero bueno, te cará cuando trabajaos con muchos datos y utilizados para la edificación.

8
00:00:46,000 --> 00:00:58,000
Hay que tener en cuenta que esta paralelización puede no ser suficiente realmente si al final este intercambio de datos entre los distintos procesos paralelos, pues no está bien hecho.

9
00:00:58,000 --> 00:01:02,000
Entonces bueno, hay que saber qué cositas son importantes para que hasta el final funcione.

10
00:01:02,000 --> 00:01:06,000
Espero que hayan tenido bien de que va la charla y si no nos lo dirán ahora.

11
00:01:06,000 --> 00:01:16,000
Y salga de lado y puedes presentar a quien ha pasado. La charla nos va a dar Juan Francisco Huete Verdejo, es entusiasta de aprender cosas nuevas,

12
00:01:16,000 --> 00:01:23,000
enfrentarse cada día a retos nuevos, construir desde cero para analizar y diseñar la solución a un problema, es su pasión.

13
00:01:23,000 --> 00:01:29,000
Él comenta que es profeta de buenas prácticas, código limpio, bien documentado y testeador.

14
00:01:29,000 --> 00:01:32,000
Así que yo creo que lo reúne casi todo.

15
00:01:32,000 --> 00:01:36,000
Hola Juan Francisco. Hola.

16
00:01:36,000 --> 00:01:47,000
Bienvenido buenos días. Buen día. Vale, pues bueno, te dejamos la pantalla y esto nos lo vamos y bueno, todo tuyo, ¿vale?

17
00:01:47,000 --> 00:01:58,000
Suerte. Genial. Bueno pues como bien ha dicho mis compañeros, me llamo Juan Francisco Huete Verdejo, trabajo en Blutab, al Nivem Company.

18
00:01:58,000 --> 00:02:05,000
Y os voy a presentar, bueno, cómo compartir grandes dataset entre procesos, pues sin perder la salud mental, ¿no?

19
00:02:05,000 --> 00:02:12,000
Porque esto a priori parece algo sencillo, pero cuando te pones a implementarlo se complica un poco, ¿no?

20
00:02:12,000 --> 00:02:18,000
Entonces, bueno, vamos a comenzar con el planteamiento del problema, ¿vale?

21
00:02:18,000 --> 00:02:24,000
Bueno, tenemos una pipeline, ¿vale? donde tenemos varios steps.

22
00:02:24,000 --> 00:02:32,000
Cada uno de esos steps pues trabaja con un DataFrame, ¿vale? y retorna pues otro DataFrame, ¿vale?

23
00:02:32,000 --> 00:02:37,000
Por ejemplo aquí el Step 2 pues trabaja con el DataFrame A y el DataFrame B

24
00:02:37,000 --> 00:02:44,000
y su salida es el DataFrame E y en el Step 3 pues trabaja con el DataFrame C y con el DataFrame D

25
00:02:44,000 --> 00:02:47,000
y su salida es el DataFrame F.

26
00:02:47,000 --> 00:02:58,000
Aquí tenemos los tiempos que más o menos tardan a cada step en ejecutar y, bueno, pues este es un poco el escenario que vamos a trabajar, ¿vale?

27
00:02:58,000 --> 00:03:08,000
Entonces, a priori pues decimos, bueno, pues si tenemos el Next Step, el trabaja con el DataFrame A y el DataFrame B

28
00:03:08,000 --> 00:03:11,000
y el Step 3 trabaja con el DataFrame C y el D.

29
00:03:11,000 --> 00:03:21,000
Son totalmente independientes, uno no se pisa al otro, pues podríamos paralizarlo y con esto podríamos optimizar, ¿no?

30
00:03:21,000 --> 00:03:23,000
la ejecución de esta pipeline.

31
00:03:23,000 --> 00:03:32,000
Entonces, bueno, en este caso el Step 4, por ejemplo, pues ya como utiliza el DataFrame A

32
00:03:32,000 --> 00:03:43,000
pues bueno, por guardarnos un poco en salud, ¿no? Lo ponemos como consecutivo, ¿no? Al, o sea, como dependiente del Step 2 y del Step 3

33
00:03:43,000 --> 00:03:52,000
podemos crear este árbol y si analizamos toda la pipeline y todos los steps que tiene, cada una, bueno, cada uno de los steps que tiene la pipeline

34
00:03:52,000 --> 00:03:59,000
pues podríamos llegar a optimizar mucho, ¿no? Como planteamiento, la verdad que suena bien, ¿no?

35
00:03:59,000 --> 00:04:06,000
Además decimos, bueno, pues si el Step 2 tarda 15 segundos en ejecutar, el 3 tarda 12

36
00:04:06,000 --> 00:04:12,000
pues realmente al final lo que va a tardar estos dos steps en ejecutar

37
00:04:12,000 --> 00:04:19,000
va a ser lo que tarde el que más tarda de los dos, ¿no? Que es 15 segundos, nos hemos ahorrado a 12

38
00:04:19,000 --> 00:04:26,000
así que aquí es donde dices, madre mía, que odia, que bueno soy, ¿no? Voy a implementarlo ya y me voy a coronar

39
00:04:26,000 --> 00:04:34,000
pero entonces dices, bueno, voy a encasular cada uno de estos steps en un Worker de Celery

40
00:04:34,000 --> 00:04:42,000
y bueno, pues lo que voy a hacer a priori, ¿no? La primera idea que se puede ocurrir es pasar este DataFrame

41
00:04:42,000 --> 00:04:50,000
como argumento al Worker de Celery. Bueno, de lo primeras te va a decir que no es un Dyson serializable

42
00:04:50,000 --> 00:04:57,000
lo puede pasar adicionario, lo cual va a tardar mucho, así que ahí vamos a tener una puerta cerrada

43
00:04:57,000 --> 00:05:03,000
entonces dices, bueno, pues haz un Pickle de este DataFrame y lo paso por Pickle

44
00:05:03,000 --> 00:05:11,000
a lo cual, bueno, pues de Celery lo que te va a decir es lo primero

45
00:05:11,000 --> 00:05:17,000
que Pickle es un formato que tiene su riesgo de seguridad

46
00:05:17,000 --> 00:05:26,000
y que tienes que hacer una configuración extra para poder permitir unos argumentos en formato Pickle, ¿no?

47
00:05:26,000 --> 00:05:36,000
Entonces, bueno, lo asumimos, es una pipeline cerrada, los argumentos son cerrados

48
00:05:36,000 --> 00:05:45,000
los DataFrame que tenemos asumimos que todo es seguro y activamos la serialización Pickle, ¿no? en Celery

49
00:05:45,000 --> 00:05:55,000
entonces ya lo implementamos y lo que nos vamos a encontrar es que Pickle cuando el DataFrame supera a cierto tamaño

50
00:05:55,000 --> 00:06:00,000
tarda mucho en serializar, ¿vale? y en algunos casos si el DataFrame es muy grande

51
00:06:00,000 --> 00:06:05,000
Celery te va a decir que no puedes pasarle un argumento con un tamaño tan grande

52
00:06:05,000 --> 00:06:12,000
entonces aquí se nos viene nuestra idea abajo, ¿vale? lo primero porque no podemos pasar DataFrame muy grandes

53
00:06:12,000 --> 00:06:18,000
lo segundo porque estamos perdiendo más tiempo en hacer el Pickle que lo que tardamos en ejercer la pipeline

54
00:06:18,000 --> 00:06:21,000
o sea el ST

55
00:06:21,000 --> 00:06:25,000
entonces, pues bueno, decimos, ¿vale? pues lo que podemos hacer entonces

56
00:06:25,000 --> 00:06:34,000
describimos el DataFrame a disco, en CSV y luego lo recuperamos en cada uno de los ST

57
00:06:34,000 --> 00:06:41,000
esto es muy mala idea, ¿vale? porque al escribir a disco el disco ya sabemos que es lento

58
00:06:41,000 --> 00:06:45,000
aunque tengamos discos SSDs no solo suficientemente rápidos

59
00:06:45,000 --> 00:06:51,000
y entonces pues lo que va a pasar pues manda el mismo, ¿no? que vamos a tardar más en escribir

60
00:06:51,000 --> 00:06:57,000
y el leer eso DataFrame de un Worker a otro que lo que realmente tardan a ejecutar

61
00:06:57,000 --> 00:07:01,000
o sea que realmente esta paralización no nos está compensando

62
00:07:01,000 --> 00:07:05,000
antes cuando estaba en secuencial iba más rápido

63
00:07:05,000 --> 00:07:13,000
entonces no estamos optimizando nada sino que además estamos empeorando la eficiencia, ¿no?

64
00:07:13,000 --> 00:07:21,000
entonces aquí pues decimos, bueno y ahora qué, ¿no? esto es un cariñón sin salida o qué pasa

65
00:07:21,000 --> 00:07:28,000
bueno pues no, hay algunas alternativas, ¿vale? que son las que voy a presentar ahora

66
00:07:28,000 --> 00:07:30,000
habrá muchas más

67
00:07:30,000 --> 00:07:34,000
pero bueno, de las más interesantes pues son Redis

68
00:07:34,000 --> 00:07:36,000
Payar o Mastblastman

69
00:07:36,000 --> 00:07:42,000
Baix y bueno como última alternativa

70
00:07:42,000 --> 00:07:46,000
aunque no es de lo más eficiente pero está totalmente integrado en la nube

71
00:07:46,000 --> 00:07:49,000
sería Baix Mast S3

72
00:07:49,000 --> 00:07:53,000
vamos a comenzar con Redis, ¿vale?

73
00:07:53,000 --> 00:08:00,000
Redis pues bueno, muchos las conocemos, ¿no? es una base de datos en memoria

74
00:08:00,000 --> 00:08:02,000
clave valor

75
00:08:02,000 --> 00:08:10,000
es muy versátil porque bueno al final los valores pueden ser de múltiples tipos

76
00:08:10,000 --> 00:08:16,000
puede ser una raid de bytes, puede ser un diccionario, puede ser un integer, puede ser un string

77
00:08:16,000 --> 00:08:21,000
entonces bueno nos va a dar una versatilidad ahí que para resolver este problema

78
00:08:21,000 --> 00:08:26,000
siempre que la ETL no sea muy grande, que los dataframe no sean muy grandes

79
00:08:26,000 --> 00:08:31,000
bueno pues a lo mejor nos puede servir porque es sencillo de implementar

80
00:08:31,000 --> 00:08:34,000
y puede dar un buen resultado

81
00:08:34,000 --> 00:08:39,000
las ventajas que tiene, bueno pues que Redis es muy fácil de configurar

82
00:08:39,000 --> 00:08:41,000
es muy versátil

83
00:08:41,000 --> 00:08:47,000
y para pequeñas ETLs con dataframes más pequeños de 500M

84
00:08:47,000 --> 00:08:50,000
es una solución rápida y simple

85
00:08:50,000 --> 00:08:54,000
como inconveniente, bueno pues que tienen una limitación de los valores de 500M

86
00:08:54,000 --> 00:08:57,000
no podemos meter un dataframe más grande de 500M

87
00:08:57,000 --> 00:09:04,000
entonces si en nuestra ETL vamos a tener volumen de datos más grandes

88
00:09:04,000 --> 00:09:07,000
pues esta solución no nos va a valer

89
00:09:07,000 --> 00:09:12,000
y otro inconveniente es que volvemos a tener que serializar utilizando piquen

90
00:09:12,000 --> 00:09:16,000
pero debido a la propia limitación de los 500M

91
00:09:16,000 --> 00:09:21,000
pues bueno, realmente no estamos serializando un dataframe muy grande

92
00:09:21,000 --> 00:09:26,000
entonces posiblemente nos compensen el piquen de ese dataframe

93
00:09:26,000 --> 00:09:29,000
entonces el planteamiento sería el siguiente

94
00:09:29,000 --> 00:09:33,000
nosotros tenemos un worker de celeri donde tenemos un ST

95
00:09:33,000 --> 00:09:39,000
y queremos pasar este dataframe a la siguiente tarea de celeri

96
00:09:39,000 --> 00:09:42,000
para ejecutar el STEM2

97
00:09:42,000 --> 00:09:45,000
lo que hacemos es que cogemos este dataframe a

98
00:09:45,000 --> 00:09:50,000
le asignamos una clave y lo guardamos en Redis

99
00:09:50,000 --> 00:09:53,000
con el dataframe serializado con piquen

100
00:09:53,000 --> 00:09:56,000
pasamos por argumento, es a key

101
00:09:56,000 --> 00:10:00,000
al worker de celeri que es un, es a key va a ser un ST

102
00:10:00,000 --> 00:10:02,000
normal y corriente

103
00:10:02,000 --> 00:10:04,000
y ya con esa key accedemos a Redis

104
00:10:04,000 --> 00:10:06,000
obtenemos el dataframe

105
00:10:06,000 --> 00:10:11,000
serializado con piquen, lo deserializamos y ya podemos operar con él

106
00:10:11,000 --> 00:10:16,000
vale, entonces el código pues se quedan pues tres líneas prácticamente

107
00:10:16,000 --> 00:10:21,000
aquí en esta primera línea simplemente nos conectamos a un piquen

108
00:10:21,000 --> 00:10:25,000
aquí en esta primera línea simplemente nos conectamos a Redis

109
00:10:25,000 --> 00:10:30,000
hacemos el dam del dataframe, osea hacemos el piquen

110
00:10:30,000 --> 00:10:35,000
del dataframe y le asignamos esta clave en este caso es de F

111
00:10:35,000 --> 00:10:38,000
y lo guardamos en Redis

112
00:10:38,000 --> 00:10:42,000
lanzamos la tarea de celeri con el argumento

113
00:10:42,000 --> 00:10:45,000
que es la propia clave del dataframe

114
00:10:45,000 --> 00:10:48,000
vale, que hay guardado en Redis

115
00:10:48,000 --> 00:10:51,000
y simplemente la tarea de celeri lo tenemos que hacer es

116
00:10:51,000 --> 00:10:53,000
por volvernos a conectar a Redis

117
00:10:53,000 --> 00:10:56,000
obtenemos la clave que hemos pasado por argumento

118
00:10:56,000 --> 00:11:00,000
y hacemos un loa del dataframe, osea un piquen loa

119
00:11:00,000 --> 00:11:04,000
y así deserializamos otra vez el dataframe y ya lo tenemos para operar

120
00:11:04,000 --> 00:11:06,000
esto pues ofrece un benchmark

121
00:11:06,000 --> 00:11:08,000
pues para un dataframe pequeño

122
00:11:08,000 --> 00:11:11,000
de unos 10 megas pues 1,38 segundos

123
00:11:11,000 --> 00:11:13,000
que bueno, no está mal

124
00:11:13,000 --> 00:11:17,000
no es de los mejores tiempos pero no está mal

125
00:11:17,000 --> 00:11:20,000
y para un dataframe un poco más grande de 550 megas

126
00:11:20,000 --> 00:11:23,000
pues ya tenemos un error por la propia limitación que hemos dicho

127
00:11:23,000 --> 00:11:26,000
entonces bueno, es una solución muy simple

128
00:11:26,000 --> 00:11:29,000
vale, que para, como he dicho, para detalles muy sencillas

129
00:11:29,000 --> 00:11:33,000
pues a lo mejor nos compensa, no necesitamos más

130
00:11:33,000 --> 00:11:35,000
cuando tenemos algo más complejo

131
00:11:35,000 --> 00:11:39,000
vale, hago ya que requiere una transferencia de unos dataframe

132
00:11:39,000 --> 00:11:41,000
bastante más grandes

133
00:11:41,000 --> 00:11:47,000
podemos utilizar, pues pay arrow más plasma

134
00:11:47,000 --> 00:11:51,000
arrow es un formato de datos en memoria

135
00:11:51,000 --> 00:11:53,000
en formato columnar

136
00:11:53,000 --> 00:11:58,000
facilita la conversión de datos entre diferentes lenguajes

137
00:11:58,000 --> 00:12:02,000
y bueno, plasma es un almacén de objetos de arrow en memoria

138
00:12:02,000 --> 00:12:04,000
pay arrow más plasma

139
00:12:04,000 --> 00:12:06,000
realmente no está diseñado para esto

140
00:12:06,000 --> 00:12:12,000
está diseñado para convertir dataframes entre lenguajes

141
00:12:12,000 --> 00:12:15,000
es decir, yo tengo un dataframe en pandas

142
00:12:15,000 --> 00:12:18,000
y yo lo quiero pasar a mallab

143
00:12:18,000 --> 00:12:23,000
pues pay arrow y plasma tienen conectores en mucho el lenguaje

144
00:12:23,000 --> 00:12:28,000
entonces simplemente podríamos pasar ese dataframe

145
00:12:28,000 --> 00:12:33,000
en formato pandas, a formato arrow lo publicamos en plasma

146
00:12:33,000 --> 00:12:38,000
y desde mallab podríamos conectarnos a plasma

147
00:12:38,000 --> 00:12:42,000
y obtener ese dataframe y convertirlo a formato mallab

148
00:12:42,000 --> 00:12:45,000
o a formato r o a formato java o javascript

149
00:12:45,000 --> 00:12:48,000
la verdad que tiene un montón de conectores

150
00:12:48,000 --> 00:12:54,000
por los que transformar ese dataframe

151
00:12:54,000 --> 00:12:57,000
¿cuáles son las ventajas?

152
00:12:57,000 --> 00:12:59,000
bueno, por la principal ventaja

153
00:12:59,000 --> 00:13:02,000
es que tiene una serialización muy rápida

154
00:13:02,000 --> 00:13:05,000
la transmisión es muy rápida

155
00:13:05,000 --> 00:13:10,000
y bueno, la compatibilidad de la dataframe es muchos lenguajes

156
00:13:10,000 --> 00:13:13,000
inconvenientes, bueno, porque necesita un file system

157
00:13:13,000 --> 00:13:16,000
y esto puede complicar la computación distribuida

158
00:13:16,000 --> 00:13:20,000
y si se puede decir que es una ventaja

159
00:13:20,000 --> 00:13:23,000
solo es compatible con linux y con mac

160
00:13:23,000 --> 00:13:28,000
con windows no es compatible porque utiliza un socket de linux

161
00:13:28,000 --> 00:13:32,000
lo que vamos a hacer es aprovechar

162
00:13:32,000 --> 00:13:36,000
esto a nuestro favor para hacer una conversión de pandas

163
00:13:36,000 --> 00:13:40,000
a formato arrow, transpasar esto a plasma

164
00:13:40,000 --> 00:13:45,000
y leer en formato arrow desde plasma

165
00:13:45,000 --> 00:13:48,000
y volverlo a pasar a pandas

166
00:13:48,000 --> 00:13:53,000
entonces la verdad que nos puede solucionar el problema

167
00:13:53,000 --> 00:13:58,000
plasma viene en el mismo paquete de pyarrow

168
00:13:58,000 --> 00:14:01,000
simplemente tengo que hacer el pipi-stall de pyarrow

169
00:14:01,000 --> 00:14:04,000
ya tenemos el demonio de plasma

170
00:14:04,000 --> 00:14:08,000
hay que levantar una story con este comando

171
00:14:08,000 --> 00:14:11,000
que es plasmastore

172
00:14:11,000 --> 00:14:14,000
con el flag-m reservamos un espacio en memoria

173
00:14:14,000 --> 00:14:18,000
en bytes, aquí tengo poco menos de 8 gigas

174
00:14:18,000 --> 00:14:23,000
y tenemos que indicarle dónde va a crear el socket

175
00:14:23,000 --> 00:14:26,000
para la comunicación, no?

176
00:14:26,000 --> 00:14:29,000
entonces, en este caso, lo he hecho en tmp

177
00:14:29,000 --> 00:14:31,000
y aquí se crearía este socket

178
00:14:31,000 --> 00:14:34,000
una vez levantado este demonio ya estaría listo

179
00:14:34,000 --> 00:14:38,000
para poder transpasar los data frames

180
00:14:38,000 --> 00:14:41,000
en formato arrow hasta plasma

181
00:14:41,000 --> 00:14:43,000
¿Cómo lo vamos a hacer?

182
00:14:43,000 --> 00:14:45,000
bueno, pues fácil, ¿no?

183
00:14:45,000 --> 00:14:49,000
en este uno, lo que vamos a hacer es crear un object ID

184
00:14:49,000 --> 00:14:51,000
de plasma

185
00:14:51,000 --> 00:14:54,000
y con ese object ID

186
00:14:54,000 --> 00:14:57,000
lo que vamos a hacer luego es realizar el data frame

187
00:14:57,000 --> 00:15:00,000
en formato arrow

188
00:15:00,000 --> 00:15:04,000
y vamos a asignar ese object ID a ese data frame

189
00:15:04,000 --> 00:15:06,000
y lo vamos a guardar en plasma

190
00:15:06,000 --> 00:15:09,000
ese object ID lo vamos a pasar a stream

191
00:15:09,000 --> 00:15:13,000
y eso lo vamos a pasar como parámetro a la tas de c led

192
00:15:13,000 --> 00:15:15,000
que luego lo voy a utilizar para obtener

193
00:15:15,000 --> 00:15:18,000
para crear otro nuevo object ID de plasma

194
00:15:18,000 --> 00:15:21,000
y utilizarlo para obtener el data frame

195
00:15:21,000 --> 00:15:23,000
en formato arrow

196
00:15:23,000 --> 00:15:25,000
y una vez que lo tiene

197
00:15:25,000 --> 00:15:27,000
simplemente hay que hacer una conversión

198
00:15:27,000 --> 00:15:29,000
otra vez a formato pandas

199
00:15:29,000 --> 00:15:31,000
o al que necesitemos

200
00:15:31,000 --> 00:15:35,000
entonces, el código queda un poquito más complejo

201
00:15:35,000 --> 00:15:38,000
ahora vamos a hablar de la transferencia

202
00:15:38,000 --> 00:15:41,000
y entonces, aquí

203
00:15:41,000 --> 00:15:44,000
lo único que hay que hacer pues es

204
00:15:44,000 --> 00:15:47,000
nos conectamos al cliente de plasma

205
00:15:47,000 --> 00:15:49,000
con la ruta

206
00:15:49,000 --> 00:15:51,000
donde hemos creado el socket

207
00:15:51,000 --> 00:15:53,000
con el plasma store

208
00:15:53,000 --> 00:15:57,000
que hemos mencionado dos días positivas más arriba

209
00:15:57,000 --> 00:16:00,000
aquí, lo que hacemos es convertir

210
00:16:00,000 --> 00:16:02,000
el data frame de pandas

211
00:16:02,000 --> 00:16:04,000
a un record batch

212
00:16:04,000 --> 00:16:08,000
que es un data frame en formato arrow

213
00:16:08,000 --> 00:16:10,000
y una vez que tenemos hecho eso

214
00:16:10,000 --> 00:16:12,000
toda esta línea de código

215
00:16:12,000 --> 00:16:14,000
es para estimar el buffer que necesitamos

216
00:16:14,000 --> 00:16:16,000
para escribir en plasma

217
00:16:16,000 --> 00:16:19,000
para escribir en plasma lo necesita

218
00:16:19,000 --> 00:16:22,000
entonces, bueno, aquí se hace unos moqueros

219
00:16:22,000 --> 00:16:25,000
de lo que es el stream

220
00:16:25,000 --> 00:16:29,000
que se va a utilizar para subir ese data frame

221
00:16:29,000 --> 00:16:33,000
y se calcula pues esta variable de aquí

222
00:16:33,000 --> 00:16:36,000
el buffer, ¿vale? el tamaño del buffer

223
00:16:36,000 --> 00:16:38,000
una vez que tenemos el tamaño del buffer

224
00:16:38,000 --> 00:16:40,000
y que tenemos un buffer creado

225
00:16:40,000 --> 00:16:42,000
ya lo que hacemos es subir

226
00:16:42,000 --> 00:16:46,000
ese data frame arrow, ese record batch

227
00:16:46,000 --> 00:16:49,000
a plasma, ¿no? se crea un stream, un writer

228
00:16:49,000 --> 00:16:52,000
y con esto ya escribimos

229
00:16:52,000 --> 00:16:55,000
ese data frame en plasma

230
00:16:55,000 --> 00:16:58,000
cerramos el stream y con este comandito de aquí

231
00:16:58,000 --> 00:17:00,000
lo que hacemos es comitear

232
00:17:00,000 --> 00:17:02,000
y decirle, vale, ya está determinado

233
00:17:02,000 --> 00:17:04,000
mis referencias, ya está disponible

234
00:17:04,000 --> 00:17:07,000
para poder ser utilizado

235
00:17:07,000 --> 00:17:10,000
lo siguiente es coger este object ID

236
00:17:10,000 --> 00:17:12,000
que hemos creado aquí arriba, ¿vale?

237
00:17:12,000 --> 00:17:15,000
y que hemos utilizado para setear el

238
00:17:15,000 --> 00:17:19,000
el data frame en plasma, ¿vale?

239
00:17:19,000 --> 00:17:21,000
y lo pasamos a stream

240
00:17:21,000 --> 00:17:23,000
este object ID tiene un hash, ¿vale?

241
00:17:23,000 --> 00:17:25,000
y lo que vamos a utilizar es ese hash

242
00:17:25,000 --> 00:17:28,000
para pasárselo a la tarea de celeri

243
00:17:28,000 --> 00:17:31,000
entonces, la lectura

244
00:17:31,000 --> 00:17:33,000
simplemente como tenemos aquí el hash

245
00:17:33,000 --> 00:17:34,000
de ese object ID

246
00:17:34,000 --> 00:17:37,000
pues volvemos a... aquí lo volvemos a conectar

247
00:17:37,000 --> 00:17:39,000
al mismo socket

248
00:17:39,000 --> 00:17:42,000
que hemos creado con plasma store

249
00:17:42,000 --> 00:17:46,000
y con ese object ID

250
00:17:46,000 --> 00:17:48,000
en stream que tenemos, ¿vale?

251
00:17:48,000 --> 00:17:50,000
simplemente nos generamos otra vez

252
00:17:50,000 --> 00:17:52,000
otro object ID de plasma

253
00:17:52,000 --> 00:17:55,000
y con ese object ID, ¿vale?

254
00:17:55,000 --> 00:17:57,000
obtenemos el buffer necesario

255
00:17:57,000 --> 00:18:00,000
y con ese buffer ya podemos leer aquí

256
00:18:00,000 --> 00:18:02,000
el record batch

257
00:18:02,000 --> 00:18:06,000
que hemos subido desde la parte del...

258
00:18:06,000 --> 00:18:08,000
del cliente, ¿no?

259
00:18:08,000 --> 00:18:10,000
podemos obtener el record batch

260
00:18:10,000 --> 00:18:12,000
y con esta línea ya simplemente

261
00:18:12,000 --> 00:18:14,000
lo transformamos a pandas

262
00:18:14,000 --> 00:18:16,000
y ya volvemos a tener disponible

263
00:18:16,000 --> 00:18:19,000
nuestra fin de pandas, perdón

264
00:18:21,000 --> 00:18:23,000
y con esto, la verdad que obtenemos

265
00:18:23,000 --> 00:18:26,000
un benchmark bastante, bastante bueno, ¿no?

266
00:18:26,000 --> 00:18:28,000
tenemos para un data frame pequeño

267
00:18:28,000 --> 00:18:30,000
menos de un segundo

268
00:18:30,000 --> 00:18:32,000
y la frena de 500 megas 1,4 segundos

269
00:18:32,000 --> 00:18:34,000
y ya para data frames más grandes

270
00:18:34,000 --> 00:18:36,000
por ejemplo, 1, 3 crigas

271
00:18:36,000 --> 00:18:38,000
tenemos 5 segundos, que la verdad

272
00:18:38,000 --> 00:18:40,000
que está bastante bien, entonces, mi 10 es

273
00:18:40,000 --> 00:18:43,000
porque esta solución parece que

274
00:18:43,000 --> 00:18:46,000
nos va a aportar una buena solución

275
00:18:49,000 --> 00:18:54,000
luego, ya tenemos esa solución

276
00:18:54,000 --> 00:18:56,000
vamos con Baix

277
00:18:56,000 --> 00:18:58,000
Baix es otra solución, es un poco menos eficiente

278
00:18:58,000 --> 00:19:01,000
pero a lo mejor también nos vale

279
00:19:01,000 --> 00:19:03,000
porque es muy sencilla de implementar

280
00:19:03,000 --> 00:19:06,000
y da muy buenos resultados

281
00:19:06,000 --> 00:19:08,000
a comparación con Redis

282
00:19:08,000 --> 00:19:10,000
pues sí que podemos utilizar

283
00:19:10,000 --> 00:19:12,000
de la frame mucho más grandes, ¿vale?

284
00:19:14,000 --> 00:19:16,000
y bueno, no nos puede ser una buena solución

285
00:19:16,000 --> 00:19:19,000
Baix tiene... es una alternativa a pandas, ¿vale?

286
00:19:19,000 --> 00:19:22,000
es una librería, bueno, bastante joven

287
00:19:22,000 --> 00:19:25,000
y lo que hace es que mapea los datos

288
00:19:25,000 --> 00:19:29,000
en memoria, es decir, cuando lees los datos

289
00:19:29,000 --> 00:19:32,000
cuando tú le pasas un CSV para leerlo

290
00:19:32,000 --> 00:19:35,000
realmente no lo carga memoria, sino que hace

291
00:19:35,000 --> 00:19:37,000
lo que hace es mapear, ¿no?

292
00:19:37,000 --> 00:19:39,000
pues mapea las columnas un poco las características

293
00:19:39,000 --> 00:19:41,000
del data frame en memoria

294
00:19:41,000 --> 00:19:43,000
y todas las operaciones las hace en memoria

295
00:19:43,000 --> 00:19:45,000
y cuando... y solo se trae los datos

296
00:19:45,000 --> 00:19:47,000
cuando es necesario

297
00:19:47,000 --> 00:19:49,000
que tú necesitas dos columnas

298
00:19:49,000 --> 00:19:51,000
solo se traes a dos columnas para mostrarte

299
00:19:51,000 --> 00:19:53,000
y así consigue la eficiencia

300
00:19:53,000 --> 00:19:56,000
tiene una lectura que es súper rápida, ¿vale?

301
00:19:56,000 --> 00:20:00,000
debido a esto y también tiene la acción de cacheo

302
00:20:00,000 --> 00:20:02,000
puede realizar operaciones

303
00:20:02,000 --> 00:20:05,000
de calcular media, sumas, conteos de columnas

304
00:20:05,000 --> 00:20:09,000
de hasta un billón de filas por segundo

305
00:20:09,000 --> 00:20:11,000
que no es poco, ¿eh?

306
00:20:11,000 --> 00:20:13,000
un billón de filas por segundo

307
00:20:13,000 --> 00:20:15,000
tiene integración con cloud

308
00:20:15,000 --> 00:20:17,000
y como inconvenientes

309
00:20:17,000 --> 00:20:20,000
bueno, pues no está tan madura como pandas

310
00:20:20,000 --> 00:20:23,000
y no hay tanta comunidad como pandas

311
00:20:23,000 --> 00:20:26,000
hay veces que es complicado encontrar información

312
00:20:26,000 --> 00:20:28,000
o ayuda

313
00:20:28,000 --> 00:20:30,000
para trabajar con Baix

314
00:20:30,000 --> 00:20:32,000
lo que vamos a hacer es

315
00:20:32,000 --> 00:20:34,000
bueno, pues simplemente

316
00:20:34,000 --> 00:20:37,000
aquí vamos a escribir en disco duro, ¿vale?

317
00:20:37,000 --> 00:20:39,000
vamos a pasar el data frame de Baix

318
00:20:39,000 --> 00:20:41,000
vale, ya no es un data frame de pandas

319
00:20:41,000 --> 00:20:42,000
es un data frame de Baix

320
00:20:42,000 --> 00:20:45,000
HDF5 en el disco duro

321
00:20:45,000 --> 00:20:47,000
simplemente pasamos el pack

322
00:20:47,000 --> 00:20:50,000
y lo... a la tarea aceleri

323
00:20:50,000 --> 00:20:53,000
y leemos ese pack, ese HDF5

324
00:20:53,000 --> 00:20:56,000
para obtener otra vez el data frame de Baix

325
00:20:56,000 --> 00:20:59,000
y esto, pues la verdad que queda muy simple, ¿vale?

326
00:20:59,000 --> 00:21:01,000
simplemente tenemos aquí el pack

327
00:21:01,000 --> 00:21:03,000
aquí este data frame

328
00:21:03,000 --> 00:21:05,000
que es un data frame de Baix

329
00:21:05,000 --> 00:21:07,000
como he mencionado, lo exportamos a HDF5

330
00:21:07,000 --> 00:21:09,000
vale, pasamos la ruta

331
00:21:09,000 --> 00:21:11,000
y aquí simplemente tenemos que leer esa ruta

332
00:21:11,000 --> 00:21:13,000
y así obtenemos otra vez el data frame de Baix

333
00:21:13,000 --> 00:21:17,000
bueno, los tiempos son un poco más altos

334
00:21:17,000 --> 00:21:20,000
que la otra solución, que eran bastante buenos

335
00:21:20,000 --> 00:21:22,000
pero la sencilla del código

336
00:21:22,000 --> 00:21:23,000
hay veces que compensa

337
00:21:23,000 --> 00:21:25,000
aunque necesitamos más...

338
00:21:25,000 --> 00:21:27,000
un timing mejor, ¿no?

339
00:21:27,000 --> 00:21:30,000
y por último voy a hablar de Baix

340
00:21:30,000 --> 00:21:32,000
más S3

341
00:21:32,000 --> 00:21:35,000
aquí podemos utilizar la potencia I o de Baix

342
00:21:35,000 --> 00:21:37,000
orientado a la nube

343
00:21:37,000 --> 00:21:39,000
es mucho más fácil

344
00:21:39,000 --> 00:21:41,000
la computación distribuida, ¿vale?

345
00:21:41,000 --> 00:21:44,000
y podemos aprovechar la concurrencia de S3

346
00:21:44,000 --> 00:21:47,000
como inconvenientes, pues Baix no está más duro

347
00:21:47,000 --> 00:21:49,000
y es más lento que otras opciones

348
00:21:49,000 --> 00:21:51,000
un poquito más, bastante más lento

349
00:21:53,000 --> 00:21:55,000
lo que vamos a hacer aquí en esta alternativa

350
00:21:55,000 --> 00:21:59,000
es vamos a coger el data frame de Baix

351
00:21:59,000 --> 00:22:04,000
y lo vamos a guardar en disco duro, ¿vale?

352
00:22:04,000 --> 00:22:06,000
y lo vamos a hacer un split

353
00:22:06,000 --> 00:22:08,000
de un split de Linux, ¿vale?

354
00:22:08,000 --> 00:22:11,000
por ejemplo, utilizando la librería SH de Linux

355
00:22:11,000 --> 00:22:13,000
y lo vamos a dividir en chunks

356
00:22:13,000 --> 00:22:16,000
esos chunks lo vamos a subir a S3

357
00:22:16,000 --> 00:22:18,000
de forma paralela a todos

358
00:22:18,000 --> 00:22:21,000
entonces con S3 no tenemos la limitación

359
00:22:21,000 --> 00:22:23,000
de escritura en disco, ¿vale?

360
00:22:23,000 --> 00:22:25,000
podemos aprovechar toda la capacidad de red

361
00:22:25,000 --> 00:22:27,000
que tengamos en nuestra nube

362
00:22:27,000 --> 00:22:29,000
que puede ser hasta 100 girar por segundo

363
00:22:29,000 --> 00:22:31,000
para subir esos chunks a S3

364
00:22:31,000 --> 00:22:34,000
entonces luego vamos a pasar a esa lista de chunks

365
00:22:34,000 --> 00:22:37,000
a nuestra tarea de Celery

366
00:22:37,000 --> 00:22:39,000
¿vale? y vamos a leer esos chunks

367
00:22:39,000 --> 00:22:42,000
igualmente de S3 de forma concurrente, ¿vale?

368
00:22:42,000 --> 00:22:44,000
todos a la vez

369
00:22:44,000 --> 00:22:46,000
y con un K de Linux

370
00:22:46,000 --> 00:22:48,000
igual con la librería SH

371
00:22:48,000 --> 00:22:53,000
los vamos a volver a fusionar

372
00:22:53,000 --> 00:22:55,000
en un solo feature HDF5

373
00:22:55,000 --> 00:22:58,000
¿vale? y ya lo leemos normalmente con Baix

374
00:22:58,000 --> 00:23:01,000
aquí el código queda bastante más largo

375
00:23:01,000 --> 00:23:03,000
más complicado, que lo podéis ver en mi repositorio

376
00:23:03,000 --> 00:23:05,000
¿vale?

377
00:23:05,000 --> 00:23:08,000
y los benchmark son un poco más altos

378
00:23:08,000 --> 00:23:10,000
a contrar de que podemos utilizar

379
00:23:10,000 --> 00:23:12,000
la potencia de la nube

380
00:23:12,000 --> 00:23:14,000
y es más fácil distribuir las tareas

381
00:23:14,000 --> 00:23:17,000
porque ya de por sí S3 es distribuido

382
00:23:17,000 --> 00:23:19,000
y podemos acceder de distintas

383
00:23:19,000 --> 00:23:21,000
distintas estancias EC2

384
00:23:21,000 --> 00:23:23,000
o de distintas contenedores

385
00:23:23,000 --> 00:23:25,000
¿no?

386
00:23:25,000 --> 00:23:28,000
y bueno, simplemente como último

387
00:23:28,000 --> 00:23:30,000
decir que en Bluetooth

388
00:23:30,000 --> 00:23:32,000
estamos buscando profesionales

389
00:23:32,000 --> 00:23:34,000
que si queréis participar en proyectos innovadores

390
00:23:34,000 --> 00:23:36,000
una proactiva que siempre estaba buscando nuevos retos

391
00:23:36,000 --> 00:23:38,000
o si queréis trabajar en una empresa

392
00:23:38,000 --> 00:23:40,000
Great Place to Work

393
00:23:40,000 --> 00:23:42,000
pues que contactes conmigo en este correo

394
00:23:42,000 --> 00:23:44,000
o por Discord

395
00:23:44,000 --> 00:23:46,000
y bueno, muchas gracias

396
00:23:46,000 --> 00:23:48,000
espero que os haya gustado

397
00:23:48,000 --> 00:23:50,000
aquí tenéis mi repositorio Jihad, mi Twitter

398
00:23:50,000 --> 00:23:52,000
y mi correo por si queréis contactar conmigo

399
00:23:52,000 --> 00:23:55,000
y nada, espero que os haya gustado

400
00:23:55,000 --> 00:23:57,000
gracias

401
00:23:59,000 --> 00:24:01,000
Genial, Juan Francisco

402
00:24:01,000 --> 00:24:03,000
Genial

403
00:24:03,000 --> 00:24:05,000
gracias, estamos viendo

404
00:24:05,000 --> 00:24:07,000
veo que no hay preguntos

405
00:24:07,000 --> 00:24:09,000
o alguien está escribiendo

406
00:24:09,000 --> 00:24:11,000
pero no sé si hay preguntas

407
00:24:11,000 --> 00:24:13,000
Yo tengo una preguntita

408
00:24:13,000 --> 00:24:15,000
damos paso a la gente

409
00:24:15,000 --> 00:24:17,000
primero, si quieres

410
00:24:17,000 --> 00:24:19,000
No, es que no está escrita aún

411
00:24:19,000 --> 00:24:21,000
No, mira, yo te quería preguntar

412
00:24:21,000 --> 00:24:23,000
Juan Francisco, cuando hablaste

413
00:24:23,000 --> 00:24:25,000
de HDF5

414
00:24:25,000 --> 00:24:27,000
en Pandas también tenemos la posibilidad

415
00:24:27,000 --> 00:24:29,000
de volcar y leer desde HDF5

416
00:24:29,000 --> 00:24:31,000
es totalmente necesario

417
00:24:31,000 --> 00:24:33,000
para poder usar Pandas

418
00:24:33,000 --> 00:24:35,000
con el pipeline HDF5

419
00:24:35,000 --> 00:24:37,000
Sí, claro que puedes utilizar Pandas

420
00:24:37,000 --> 00:24:39,000
lo que pasa es que

421
00:24:39,000 --> 00:24:41,000
en Pandas es mucho más lento

422
00:24:41,000 --> 00:24:43,000
el tiempo que tarda

423
00:24:43,000 --> 00:24:45,000
Pandas en escribir en HDF5

424
00:24:45,000 --> 00:24:47,000
es muchísimo más lento que

425
00:24:47,000 --> 00:24:49,000
que en Pandas

426
00:24:49,000 --> 00:24:51,000
al final, bueno, en Pandas

427
00:24:51,000 --> 00:24:53,000
como he dicho, como utiliza

428
00:24:53,000 --> 00:24:55,000
mapeos en memoria

429
00:24:55,000 --> 00:24:57,000
realmente no se está trayendo los datos

430
00:24:57,000 --> 00:24:59,000
entonces

431
00:24:59,000 --> 00:25:01,000
no trae los datos

432
00:25:01,000 --> 00:25:03,000
si lo vuelve a escribir

433
00:25:03,000 --> 00:25:05,000
sino que muchas veces hace

434
00:25:05,000 --> 00:25:07,000
copias o copia nada más

435
00:25:07,000 --> 00:25:09,000
que los datos que necesita

436
00:25:09,000 --> 00:25:11,000
entonces saca mucho más

437
00:25:11,000 --> 00:25:13,000
está mucho más optimizado en ese sentido

438
00:25:13,000 --> 00:25:15,000
Vale, vale, gracias

439
00:25:17,000 --> 00:25:19,000
¿Hay alguna preguntita ahí?

440
00:25:21,000 --> 00:25:23,000
No, pero también tengo

441
00:25:23,000 --> 00:25:25,000
ah bueno, no mío

442
00:25:25,000 --> 00:25:27,000
así que ahí alguien ha escrito

443
00:25:27,000 --> 00:25:29,000
Ricardo, la verdad, pregunta

444
00:25:29,000 --> 00:25:31,000
si estás en la nube, ¿por qué no usar Spark

445
00:25:31,000 --> 00:25:33,000
directamente?

446
00:25:33,000 --> 00:25:35,000
¿Por qué no usar? Perdón

447
00:25:35,000 --> 00:25:37,000
Spark, Spark

448
00:25:37,000 --> 00:25:39,000
Vale, esto es algo que no he comentado

449
00:25:39,000 --> 00:25:41,000
¿verdad?

450
00:25:41,000 --> 00:25:43,000
Está claro que

451
00:25:43,000 --> 00:25:45,000
con Spark esto

452
00:25:45,000 --> 00:25:47,000
lo solucionamos, ¿no?

453
00:25:47,000 --> 00:25:49,000
Lo que pasa es que hay veces

454
00:25:49,000 --> 00:25:51,000
que te encuentras en la situación

455
00:25:51,000 --> 00:25:53,000
yo ahora mismo, bueno, estoy en

456
00:25:53,000 --> 00:25:55,000
un proyecto

457
00:25:55,000 --> 00:25:57,000
donde ya tenían un ATL

458
00:25:57,000 --> 00:25:59,000
en Python, en Shampana

459
00:25:59,000 --> 00:26:01,000
y hay veces que

460
00:26:01,000 --> 00:26:03,000
realmente la refactorización a Spark

461
00:26:03,000 --> 00:26:05,000
no es viable por temas de presupuesto

462
00:26:05,000 --> 00:26:07,000
por temas de

463
00:26:07,000 --> 00:26:09,000
bueno, por tiempo

464
00:26:09,000 --> 00:26:11,000
y demás

465
00:26:11,000 --> 00:26:13,000
entonces, esto está orientado

466
00:26:13,000 --> 00:26:15,000
para cuando no puedes usar Spark

467
00:26:15,000 --> 00:26:17,000
para cuando tienes esta limitación

468
00:26:17,000 --> 00:26:19,000
que lo tienes realizado en Pandas

469
00:26:19,000 --> 00:26:21,000
y te tienes que apañar

470
00:26:21,000 --> 00:26:23,000
con lo que hay

471
00:26:23,000 --> 00:26:25,000
que dices, bueno, si no

472
00:26:25,000 --> 00:26:27,000
si estás en Pandas, ¿por qué vas a utilizar Vax?

473
00:26:27,000 --> 00:26:29,000
Bueno, porque tú puedes transformar un dataframe

474
00:26:29,000 --> 00:26:31,000
de Pandas a Vax simplemente

475
00:26:31,000 --> 00:26:33,000
con un comando, o sea, Vax te permite

476
00:26:33,000 --> 00:26:35,000
obtener ese dataframe de Pandas

477
00:26:35,000 --> 00:26:37,000
y empezar a operar en Vax, ¿vale?

478
00:26:37,000 --> 00:26:39,000
y luego volver a volcarlo

479
00:26:39,000 --> 00:26:41,000
a Pandas

480
00:26:41,000 --> 00:26:43,000
es un poco la

481
00:26:43,000 --> 00:26:45,000
orientación de esta charla, ¿no?

482
00:26:45,000 --> 00:26:47,000
cuando no puedes usar Spark

483
00:26:47,000 --> 00:26:49,000
cuando ya te has encontrado esto y tienes que optimizar, ¿no?

484
00:26:49,000 --> 00:26:51,000
muy bien, sí, sí

485
00:26:51,000 --> 00:26:53,000
Hay otra pregunta

486
00:26:53,000 --> 00:26:55,000
es un poco larga

487
00:26:55,000 --> 00:26:57,000
si

488
00:26:57,000 --> 00:26:59,000
la puedo leer

489
00:26:59,000 --> 00:27:01,000
Dice que se sale un poco presentado

490
00:27:01,000 --> 00:27:03,000
que la charla se habla del paso

491
00:27:03,000 --> 00:27:05,000
de un dataframe, una vez ésta ya está cargado

492
00:27:05,000 --> 00:27:07,000
pero no se, no sabes

493
00:27:07,000 --> 00:27:09,000
si te has encontrado alguna vez con que el primer cuello de botella

494
00:27:09,000 --> 00:27:11,000
está en el proceso

495
00:27:11,000 --> 00:27:13,000
de la fase de extracción de datos

496
00:27:13,000 --> 00:27:15,000
cuando éstos están en múltiples ficheros

497
00:27:15,000 --> 00:27:17,000
de Excel

498
00:27:17,000 --> 00:27:19,000
o sea, sí, que alternatías a otro con la acaba de Pandas

499
00:27:19,000 --> 00:27:21,000
lo puedes leer en Discord también

500
00:27:21,000 --> 00:27:23,000
a ver

501
00:27:23,000 --> 00:27:25,000
a ver, sí, porque es muy larga la pregunta

502
00:27:25,000 --> 00:27:27,000
es que el cuello es

503
00:27:27,000 --> 00:27:29,000
sí, sí, han acostado

504
00:27:29,000 --> 00:27:31,000
de verdad

505
00:27:31,000 --> 00:27:33,000
sobre el cuello de botella la fase de extracción de datos

506
00:27:33,000 --> 00:27:35,000
cuando éstos están en múltiples ficheros

507
00:27:35,000 --> 00:27:37,000
X, L, S, X

508
00:27:37,000 --> 00:27:39,000
bueno, es seguro, es seguro

509
00:27:39,000 --> 00:27:41,000
vale, sí

510
00:27:41,000 --> 00:27:43,000
bueno, hay veces que

511
00:27:43,000 --> 00:27:45,000
tenemos ese cuello de botella ahí, ¿no?

512
00:27:45,000 --> 00:27:47,000
que tenés que

513
00:27:47,000 --> 00:27:49,000
la fase de carga de datos

514
00:27:49,000 --> 00:27:51,000
pues, pues bueno

515
00:27:51,000 --> 00:27:53,000
hay veces que no puedes

516
00:27:53,000 --> 00:27:55,000
esquivarla, ¿no?

517
00:27:55,000 --> 00:27:57,000
esta charla, bueno, trata más

518
00:27:57,000 --> 00:27:59,000
de cómo mover esos datos, ¿no?

519
00:27:59,000 --> 00:28:01,000
ya los tenemos

520
00:28:01,000 --> 00:28:03,000
cargados

521
00:28:03,000 --> 00:28:05,000
ahora necesitamos moverlos sin perder

522
00:28:05,000 --> 00:28:07,000
la eficiencia, ¿no?

523
00:28:07,000 --> 00:28:09,000
para

524
00:28:09,000 --> 00:28:11,000
que paralizarnos con Pense, ¿no?

525
00:28:11,000 --> 00:28:13,000
que realmente no tardemos más

526
00:28:13,000 --> 00:28:15,000
en mover ese dato que lo que hemos tardado

527
00:28:15,000 --> 00:28:17,000
en ejecutar el step, ¿no?

528
00:28:17,000 --> 00:28:19,000
pero bueno, igualmente

529
00:28:19,000 --> 00:28:21,000
en Baix

530
00:28:21,000 --> 00:28:23,000
puedes solucionarte el problema

531
00:28:23,000 --> 00:28:25,000
porque al final leer un fichero

532
00:28:25,000 --> 00:28:27,000
en Baix es cuestión

533
00:28:27,000 --> 00:28:29,000
de milisegundos, aunque

534
00:28:29,000 --> 00:28:31,000
tengas gigas

535
00:28:31,000 --> 00:28:33,000
entonces

536
00:28:33,000 --> 00:28:35,000
bueno, por esa vía

537
00:28:35,000 --> 00:28:37,000
a lo mejor se puede solucionar, ¿no?

538
00:28:37,000 --> 00:28:39,000
ahora mismo

539
00:28:39,000 --> 00:28:41,000
desconozco si Baix

540
00:28:41,000 --> 00:28:43,000
puede exportar un fichero XLSX

541
00:28:43,000 --> 00:28:45,000
pero bueno

542
00:28:45,000 --> 00:28:47,000
pasándolos a CSV por ejemplo

543
00:28:47,000 --> 00:28:49,000
podríamos solucionarlo, ¿no?

544
00:28:49,000 --> 00:28:51,000
mmm

545
00:28:51,000 --> 00:28:53,000
vale, preguntan también

546
00:28:53,000 --> 00:28:55,000
bueno, la última pregunta y después al resto ya

547
00:28:55,000 --> 00:28:57,000
si hay una más por disco directamente

548
00:28:57,000 --> 00:28:59,000
pregunta Mariano

549
00:28:59,000 --> 00:29:01,000
si este tipo de ETLs

550
00:29:01,000 --> 00:29:03,000
son usados en tonos de disney intelligence

551
00:29:03,000 --> 00:29:05,000
o están más orientados a machine learning

552
00:29:05,000 --> 00:29:07,000
pues

553
00:29:07,000 --> 00:29:09,000
bueno, esta

554
00:29:09,000 --> 00:29:11,000
ETL pues al final pueden estar

555
00:29:11,000 --> 00:29:13,000
orientadas a lo que tú quieras, ¿no?

556
00:29:13,000 --> 00:29:15,000
al final

557
00:29:15,000 --> 00:29:17,000
puedes utilizarla para business intelligence

558
00:29:17,000 --> 00:29:19,000
o puedes utilizarla para

559
00:29:19,000 --> 00:29:21,000
machine learning, aquí el caso es

560
00:29:21,000 --> 00:29:23,000
eso que cuando tú tienes

561
00:29:23,000 --> 00:29:25,000
en tu pipeline, ¿no?

562
00:29:25,000 --> 00:29:27,000
de transformación, cuando vas transformando tus datos

563
00:29:27,000 --> 00:29:29,000
entre

564
00:29:29,000 --> 00:29:31,000
en distintos steps, ¿no?

565
00:29:31,000 --> 00:29:33,000
bueno, pues

566
00:29:33,000 --> 00:29:35,000
el traspaso de eso

567
00:29:35,000 --> 00:29:37,000
el data frames entre los distintos steps

568
00:29:37,000 --> 00:29:39,000
pues las alternativas

569
00:29:39,000 --> 00:29:41,000
pero al final la finalidad

570
00:29:41,000 --> 00:29:43,000
de lo que es la pipeline

571
00:29:43,000 --> 00:29:45,000
pues bueno, esto es un poco independiente, ¿no?

572
00:29:45,000 --> 00:29:47,000
esta

573
00:29:47,000 --> 00:29:49,000
metodología puedes utilizar tanto

574
00:29:49,000 --> 00:29:51,000
para machine learning como para business intelligence

575
00:29:51,000 --> 00:29:53,000
mmm

576
00:29:53,000 --> 00:29:55,000
muy bien

577
00:29:55,000 --> 00:29:57,000
perfecto, pues bueno, dejamos ya

578
00:29:57,000 --> 00:29:59,000
yo tenía preguntas por mi cuenta

579
00:29:59,000 --> 00:30:01,000
pero no os voy a hacer porque no hay tiempo

580
00:30:01,000 --> 00:30:03,000
así que te describiré por

581
00:30:03,000 --> 00:30:05,000
discos, ¿vale?

582
00:30:05,000 --> 00:30:07,000
muchas gracias por estar aquí

583
00:30:07,000 --> 00:30:09,000
gracias

584
00:30:09,000 --> 00:30:37,000
bienvenido

