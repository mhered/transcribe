1
00:00:00,000 --> 00:00:25,760
Pondía familia. En primer lugar, le agradeceros a ver si has despertado tan pronto.

2
00:00:25,760 --> 00:00:37,760
Me ha frustrado que a vosotras también.

3
00:00:37,760 --> 00:00:55,760
Las que habéis visto el título de la charla, igual le habéis pensado, porque me estoy levantando tan pronto un domingo.

4
00:00:55,760 --> 00:01:07,760
La inteligencia artificial parece una falla muy grande, con mucha filigrana, con mogollón de cosas y perdemos lo importante.

5
00:01:07,760 --> 00:01:20,760
Soy Maiximénez, doctora informática, maestra en Bellas Artes. Este segundo punto va a tener referencia en cómo abordamos modelos transplencias 200 minutos después.

6
00:01:20,760 --> 00:01:26,760
Trabajo como investigadora ingeniera de investigación en DeepMind.

7
00:01:26,760 --> 00:01:38,760
Puedes encontrar aquí. Luego podemos hablar si me les dais cualquier cosa. Y por qué en una conferencia de Python, en vez de venir aquí y hablar de Python,

8
00:01:38,760 --> 00:01:40,760
vengo a hablar de inteligencia artificial.

9
00:01:40,760 --> 00:01:54,760
En primer lugar, voy a confesar que era muy difícil elegir de qué iba a hablar, porque me van a respetar menos como ingeniera si os hablo de cosas éticas, de filosofía.

10
00:01:54,760 --> 00:02:01,760
Pero creo que es tan relevante que tenemos que hablar. Más que una charla, es una intervención.

11
00:02:01,760 --> 00:02:15,760
Python es el lenguaje más utilizado. ¿Y para qué se utiliza Python? Pues en la encuesta de jade prints nos dice que se utiliza fundamentalmente para data analysis y machine learning.

12
00:02:15,760 --> 00:02:28,760
Así que hay grandes posibilidades de que aquí hay ingenieras, ingenieras y ingenieros que vayamos a implementar los modelos que afecten a la sociedad.

13
00:02:28,760 --> 00:02:39,760
Por lo tanto, nosotros somos quienes vamos a disparar. Es importante que sepamos que estamos haciendo con los modelos que estamos entrenando.

14
00:02:39,760 --> 00:02:50,760
Para las veteranas de la inteligencia artificial, había como el rumor de que volvería un invierno en la inteligencia artificial.

15
00:02:50,760 --> 00:03:00,760
La inteligencia artificial no es algo que haya surgido hace dos días. Llevamos muchísimos tiempos de dos 40, se lleva investigando en esto.

16
00:03:00,760 --> 00:03:17,760
El primer invierno fue con el paper de Minx. En el cual se consideraba si los perceptronas iban a funcionar los primeros algoritmos.

17
00:03:17,760 --> 00:03:28,760
Y ahí fue como, ¡ostras! Igual no funciona nada, estamos perdiendo el tiempo porque realmente todos los algoritmos que habían funcionado eran en ejemplos de juguetes muy pequeños.

18
00:03:28,760 --> 00:03:37,760
Parece que cae hasta que aparece Backpropagation. El algoritmo de Backpropagation nos permitió entrenar modelos que empezaban a funcionar.

19
00:03:37,760 --> 00:03:52,760
El segundo invierno de la inteligencia artificial coincide con la burbuja del punto com. Y a partir de 2010, 2000, empieza el deep learning y empezamos a ver aplicaciones realmente fascinantes.

20
00:03:52,760 --> 00:04:08,760
No sabía que los ordenadores podían ser capaces de hacer estas cosas. Tenemos el primer ordenador que gana a un campeón de JEDREZ, tenemos Alpha Seal, Alpha Star,

21
00:04:08,760 --> 00:04:22,760
que anamos a KO, que anamos en un juego tan complejo como Starcraft. Aunque parece que pueda haber otro invierno, yo creo que estamos en la era del diamante.

22
00:04:22,760 --> 00:04:30,760
Existen muchas posibilidades y por lo tanto, existen muchas posibilidades de hacer cosas que cambien brutalmente la sociedad.

23
00:04:30,760 --> 00:04:42,760
Para las que habéis leído ese libro, La Era del Diamante, es un libro en el cual hay una tecnología casi mágica que divide la sociedad mucho más en clases.

24
00:04:42,760 --> 00:04:51,760
Bueno, nos voy a hacer spoilers porque el libro es muy muy guay, pero bueno, creo que estamos en la Era del Diamante y es importante que hablemos.

25
00:04:51,760 --> 00:05:01,760
Y por qué me decidí, yo tenía muchas dudas y por qué me decidí finalmente hablar de esto, pues revisando una propuesta de charla para un PYKON,

26
00:05:01,760 --> 00:05:11,760
leí esta propuesta que decía algo así. Como en esta charla veremos cómo implementar en muy pocas líneas de PYKON un reconocedor automático de caras

27
00:05:11,760 --> 00:05:22,760
para un dron casero para tener vigilancia en tu propia casa. Es verdad, podemos en 20 minutos una charla de 20 minutos con un dron casero,

28
00:05:22,760 --> 00:05:31,760
reconocer caras y reconocer quién está entrando en tu perímetro. Pero las implicaciones de eso son brutales.

29
00:05:31,760 --> 00:05:43,760
Estamos teniendo un estado de vigilancia policial en el cual somos sheriffs de nuestras propias casas sin tener en cuenta las implicaciones de esa tecnología.

30
00:05:43,760 --> 00:05:50,760
Creemos que esa tecnología es 100% segura, 100% precisa y no lo es.

31
00:05:50,760 --> 00:06:03,760
Y voy a llevar por las fallas, porque las fallas para mí es el perfecto ejemplo de lo que puede ser la inteligencia artificial,

32
00:06:03,760 --> 00:06:11,760
porque las valenciámonas hacemos... Bueno, esto es una falla. Dejarme antes que explique que son las fallas para las que no sois de la terretad.

33
00:06:11,760 --> 00:06:21,760
La falla es un monumento como este. Esta es mi falla, mi falla de la falla en mi barrio. El señor este aquí del final es mi yayo.

34
00:06:21,760 --> 00:06:31,760
Y es un monumento que hacemos los valencianos que plantamos el día 14, se quema el día 19, y se quema todo aquello que ha caído mal en el año.

35
00:06:31,760 --> 00:06:45,760
Nos reímos de todo lo que ha caído mal y lo quemamos. Y empezamos la primavera, empezamos el año nuevo, empezamos la regeneración quemando todo aquello que ha caído mal y empezando de nuevo.

36
00:06:45,760 --> 00:06:52,760
Yo creo que es un buen momento para que nos planteemos todo aquello que hay que quemar y todo aquello que hay que construir de nuevo.

37
00:06:52,760 --> 00:07:02,760
Bueno, esta es mi falla de un barro obrero. La pagamos entre todas las falleras y falleros. Ah, no, detrás, adelante.

38
00:07:02,760 --> 00:07:11,760
Esta es una falla de sección especial. La sección especial es básicamente el fan. Las grandísimas empresas de tech.

39
00:07:11,760 --> 00:07:16,760
Son fallas en las cuales normalmente están en el centro, en zonas muy adineradas.

40
00:07:16,760 --> 00:07:25,760
Y se permiten. Es el mismo año. Y no sé si os dais cuenta en escala, pero es fácilmente diez veces más grande que mi falla.

41
00:07:25,760 --> 00:07:37,760
Y aquí empiezan. Quiero haceros una pregunta. ¿Crees que las fallas tienen ética, tienen valores?

42
00:07:37,760 --> 00:07:44,760
Bueno, como entiendo que no vais a conocer los detalles intrínsecos de la fiesta, os digo que sí.

43
00:07:44,760 --> 00:07:52,760
Y os lo demuestro con estas dos imágenes. La imagen de la izquierda es un boceto que se publicó en un lliubrete falla.

44
00:07:52,760 --> 00:08:04,760
Un lliubrete falla son pequeñas revistas en las cuales se publicitan las fallas, se escriben poemas, es como una pequeña recolección

45
00:08:04,760 --> 00:08:14,760
del arte de las valencianas del barrio. Este lliubrete falla no se ve nada, pero pone proletaris, univos.

46
00:08:14,760 --> 00:08:21,760
Lo hicieron los intelectuales valencianos en el 37. Tú querías estar en Valenciana en el 37. Estas gente tampoco.

47
00:08:21,760 --> 00:08:33,760
Y aun así hicieron ese libreto. Tuvieron que salir porque spoiler alerta la cultura valenciana y va a necesitar toda la ayuda que pudiera recabar.

48
00:08:33,760 --> 00:08:39,760
A la izquierda es una falla plantán toledo por los franquistas, con toda la ideología falangista.

49
00:08:39,760 --> 00:08:48,760
Lamentablemente las fallas venían con estos valores de comunidad, de construcción común.

50
00:08:48,760 --> 00:09:04,760
Nos está costando mucho, pero ganaron los otros y las fallas se parecen bastante más a los otros que a los valores originales.

51
00:09:04,760 --> 00:09:07,760
Y ahora quiero que levantemos todas las manos.

52
00:09:07,760 --> 00:09:19,760
Sí, estupendo. Quiero que bajéis las manos las que creáis que la tecnología es neutra.

53
00:09:19,760 --> 00:09:26,760
Quiero que se queden levantadas las manos de quien cree que la tecnología es neutra.

54
00:09:26,760 --> 00:09:42,760
Muy poquitas, pero que bien, tengo que convencer a muy poca gente. La tecnología no es neutra, amigas. Como las fallas, existen valores en la inteligencia artificial, en la tecnología.

55
00:09:42,760 --> 00:09:48,760
Y no te falta que lo discutamos porque las científicas ya han hecho ese trabajo por nosotras.

56
00:09:48,760 --> 00:09:54,760
Este es un paper que se publicó en Facto. Facto es una conferencia que yo os la recomiendo fuertísimo.

57
00:09:54,760 --> 00:09:59,760
Me voy a leer todos los papers y me llenan todos muchísimo.

58
00:09:59,760 --> 00:10:14,760
Y lo que hicieron estas investigadoras es buscar todos los papers que se habían publicado en las mayores conferencias, New Reap, ICLR, y evaluar con los valores que decían que estaban contribuyendo.

59
00:10:14,760 --> 00:10:27,760
Y los mayores, la parte de arriba de la columna, son performance, generalización, construir sobre trabajos anteriores, eficiencia, novedad.

60
00:10:27,760 --> 00:10:34,760
Los usuarios y los derechos de sus usuarios y los problemas están totalmente desaparecidos.

61
00:10:34,760 --> 00:10:40,760
De hecho, los derechos de los usuarios están en naranja y en azul lila.

62
00:10:40,760 --> 00:10:54,760
Son los principios éticos que rejan la investigación. Como podéis ver, es la cola muy aguda de los papers que se publican.

63
00:10:54,760 --> 00:11:07,760
Y no sólo eso, es que sólo el 4% explicaba las aplicaciones éticas de los papers y un 98% no aumentaba ninguna consecuencia para las usuarias.

64
00:11:07,760 --> 00:11:11,760
Todas sabemos que eso no es cierto. Es improbable, imposible.

65
00:11:11,760 --> 00:11:24,760
Otro de los cambios que se ha visto entre 2008 y 2018, entre esos dos años, es que las grandes universidades y el Big Tech han colapsado ese espacio.

66
00:11:24,760 --> 00:11:29,760
Tiene sentido si pensamos en el tamaño de los modelos en los cuales estamos entrenando.

67
00:11:29,760 --> 00:11:36,760
Es muy difícil que con mi una GPU con la que yo estudié mi doctorado pueda entrenar un modelo como GoFer.

68
00:11:36,760 --> 00:11:40,760
Eso limita mucho el acceso a la investigación.

69
00:11:42,760 --> 00:11:46,760
¿Cómo se reflejan los valores? Se reflejan en sesgos.

70
00:11:46,760 --> 00:11:53,760
Estas dos imágenes son imágenes de noticias que pasaron en fallas del año pasado a la derecha.

71
00:11:53,760 --> 00:12:00,760
Esta fallera mayor recibió amenazas de muerte porque era una persona racializada.

72
00:12:00,760 --> 00:12:06,760
Y, oh, Dios mío, no vaya a ser una fallera mayor palenciana, una persona que no sea blanca.

73
00:12:06,760 --> 00:12:10,760
Y a la izquierda, esta mujer quería vestir de Saraway.

74
00:12:10,760 --> 00:12:13,760
Saraway es el traje tradicional de los hombres.

75
00:12:13,760 --> 00:12:21,760
Nos vestimos con los trajes tradicionales del siglo XVIII y ella quería vestirse de Saraway porque se sentía mucho más cómoda

76
00:12:21,760 --> 00:12:27,760
acompañando a uno de los actos a su pareja que quería vestir con el vestido de fallera.

77
00:12:27,760 --> 00:12:32,760
Los valores se identifican en sesgos.

78
00:12:32,760 --> 00:12:35,760
¿Qué son los sesgos algorímicos?

79
00:12:35,760 --> 00:12:41,760
Los sesgos algorímicos son errores sistemáticos que establecen, que renuncian el status quo.

80
00:12:41,760 --> 00:12:47,760
Creando sistemas injustos, obviamente, porque el status quo es lo justo.

81
00:12:47,760 --> 00:12:50,760
Empezamos con esta investigación.

82
00:12:50,760 --> 00:12:55,760
Esta doctora quería hacer un proyecto completamente diferente.

83
00:12:55,760 --> 00:13:00,760
Pero se dio cuenta que cuando utilizaba los reconocedores faciales,

84
00:13:00,760 --> 00:13:05,760
pues, sorpresa, sorpresa, nadie, ningún reconocedor le reconocía la cara.

85
00:13:05,760 --> 00:13:07,760
No reconocía que había una cara humana.

86
00:13:07,760 --> 00:13:12,760
Hasta que estaba este pequeño artefacto, está más que la blanca.

87
00:13:12,760 --> 00:13:15,760
Entonces dijo, espera un momento.

88
00:13:15,760 --> 00:13:17,760
Espera un momento.

89
00:13:17,760 --> 00:13:20,760
Creo que esto puede ser un sesgo.

90
00:13:20,760 --> 00:13:26,760
Y generó un nuevo dataset, a por ciento de políticos de países africanos,

91
00:13:26,760 --> 00:13:28,760
y lo evaluó.

92
00:13:28,760 --> 00:13:35,760
Y se dio cuenta de que los modelos reconocían mayoritariamente mucho mejor a hombres blancos

93
00:13:35,760 --> 00:13:39,760
y se reconocían mucho peor a mujeres racializadas.

94
00:13:39,760 --> 00:13:41,760
Y esta es la diferencia.

95
00:13:41,760 --> 00:13:45,760
La diferencia del gap es más de un 30% a uno de los modelos comerciales de la época.

96
00:13:45,760 --> 00:13:49,760
El dataset está disponible y podéis utilizarlo.

97
00:13:49,760 --> 00:13:53,760
Una nota al pie es recordada.

98
00:13:53,760 --> 00:13:57,760
Y ellas lo dicen que consideran géneros masculino y femenino,

99
00:13:57,760 --> 00:14:04,760
porque no existen datasets etiquetados con géneros más allá del binario.

100
00:14:04,760 --> 00:14:07,760
Y para que...

101
00:14:07,760 --> 00:14:10,760
Video cannot be load, but hope.

102
00:14:10,760 --> 00:14:12,760
Hope, hope, hope.

103
00:14:12,760 --> 00:14:15,760
Vale.

104
00:14:15,760 --> 00:14:20,760
Este día me llegó a Twitter el otro día y me pareció fantástico.

105
00:14:20,760 --> 00:14:23,760
A la izquierda pone mail, a la de chafí mail,

106
00:14:23,760 --> 00:14:27,760
y con pequeños gestos,

107
00:14:27,760 --> 00:14:32,760
esta persona investigadora era capaz de engañar al sistema.

108
00:14:32,760 --> 00:14:36,760
Volvemos a hablar de Quirnes.

109
00:14:36,760 --> 00:14:40,760
Estáis muy enfados los hombres, os pasa algo, estáis bien.

110
00:14:40,760 --> 00:14:45,760
Porque parece que si gruñe es como...

111
00:14:50,760 --> 00:14:53,760
Bueno, es que me parece fascinante este vídeo.

112
00:14:53,760 --> 00:14:55,760
Ehe...

113
00:14:55,760 --> 00:14:59,760
¿Ves? Estás enfadados, os pasa algo.

114
00:14:59,760 --> 00:15:10,760
Bueno, volvó a la presentación.

115
00:15:10,760 --> 00:15:14,760
Y volvó y respiro con un segundito.

116
00:15:17,760 --> 00:15:20,760
Estos sesgos algorímicos, ¿cómo se traducen?

117
00:15:20,760 --> 00:15:24,760
Pues se traducen en un montón de ámbitos que nos tocan.

118
00:15:24,760 --> 00:15:28,760
Vamos a empezar con el sistema laboral.

119
00:15:28,760 --> 00:15:30,760
Sabemos...

120
00:15:30,760 --> 00:15:35,760
Os voy a hacer una super sorpresa que la industria no existe parida.

121
00:15:35,760 --> 00:15:40,760
Entonces, lo que les pasó a Amazon es que cuando entrenaron un modelo

122
00:15:40,760 --> 00:15:46,760
capaz de predecir quién iba a ser el candidato mejor para una posición de ingeniero

123
00:15:46,760 --> 00:15:50,760
y no estoy utilizando el término incorrectamente en ingeniero,

124
00:15:50,760 --> 00:15:56,760
los modelos obviamente prefieran que los mejores ingenieros eran hombres

125
00:15:56,760 --> 00:15:59,760
y por lo tanto tenían que ser hombres.

126
00:15:59,760 --> 00:16:03,760
Fue un gran escándalo, Amazon tuvo que retirar el modelo.

127
00:16:03,760 --> 00:16:12,760
Aun así, se estima que el 72% de los currículums nunca van a ser leídos por un ser humano.

128
00:16:12,760 --> 00:16:18,760
O sea que tenemos que curar nuestros currículums para que sean entendibles por máquinas.

129
00:16:18,760 --> 00:16:27,760
Obviamente, si en el sistema laboral aparecen estos sesgos,

130
00:16:27,760 --> 00:16:31,760
aparecen también en el sistema educativo.

131
00:16:31,760 --> 00:16:34,760
El sistema educativo utiliza sistemas de aprendizaje automático

132
00:16:34,760 --> 00:16:37,760
para ranquear universidades en Estados Unidos,

133
00:16:37,760 --> 00:16:41,760
precisamente es particularmente mucho más grave

134
00:16:41,760 --> 00:16:45,760
por cómo se confía en el sistema educativo en Estados Unidos,

135
00:16:45,760 --> 00:16:48,760
pero también se reenquían los estudiantes.

136
00:16:48,760 --> 00:16:56,760
Este caso pasó en Estados Unidos en este año, no, el 20, perdón,

137
00:16:56,760 --> 00:17:00,760
el 2022 es como un mashup de años.

138
00:17:00,760 --> 00:17:03,760
En el cual, como no se podían hacer los exames,

139
00:17:03,760 --> 00:17:08,760
unos exames similares a la selectividad, pues dijeron, no pasa nada.

140
00:17:08,760 --> 00:17:12,760
Cogemos la nota de los profesores, de las profesoras

141
00:17:12,760 --> 00:17:16,760
y lo calibramos con un algoritmo.

142
00:17:16,760 --> 00:17:25,760
Y, oh, sorpresa, sorpresa, pasó que las escuelas en ámbitos mucho más ricos

143
00:17:25,760 --> 00:17:34,760
tuvieron muchas mejores notas, es decir, el sistema correlaba clase con inteligencia.

144
00:17:34,760 --> 00:17:43,760
Y, bien, tuvieron que tirar atrás todos los resultados, les hacer algoritmo.

145
00:17:43,760 --> 00:17:48,760
Son escándalos que han pasado, entonces, pues han tenido que retirar.

146
00:17:48,760 --> 00:17:56,760
Otro ámbito en el que también pasan estas cosas es el bancario.

147
00:17:56,760 --> 00:18:00,760
En 2019, a Pelsaco, unas tarjetas de crédito

148
00:18:00,760 --> 00:18:06,760
y personas muy importantes como Wozniak, se dieron cuenta que, curiosamente,

149
00:18:06,760 --> 00:18:11,760
sus parejas recibían menores líneas de crédito que ellos.

150
00:18:11,760 --> 00:18:18,760
Y se abrió una investigación, las tarjetas de crédito eran emitidas por Goldman Sachs,

151
00:18:18,760 --> 00:18:23,760
Goldman Sachs dijo, y el género no es uno de las variables que nosotras teníamos en cuenta,

152
00:18:23,760 --> 00:18:26,760
nos pone explicitamente en el algoritmo.

153
00:18:26,760 --> 00:18:34,760
Lo cual no significa que no estén en el algoritmo, porque estas variables pueden ser inferidas

154
00:18:34,760 --> 00:18:38,760
a través de un montón de otras variables.

155
00:18:38,760 --> 00:18:43,760
La investigación concluyó que no existían vallas, no existían sesgos, adelante,

156
00:18:43,760 --> 00:18:50,760
pero lo que es particularmente relevante es que ningún ingeniera era capaz de explicar

157
00:18:50,760 --> 00:18:58,760
por qué esas diferencias de líneas de crédito con personas que no deberían tener diferencias en línea de crédito.

158
00:18:58,760 --> 00:19:04,760
Ya entramos en los que yo creo que son los ámbitos más peligrosos y que por lo tanto están más legislados,

159
00:19:04,760 --> 00:19:09,760
pero son muy peligrosos el sistema judicial y el sistema médico.

160
00:19:09,760 --> 00:19:14,760
En el caso del sistema judicial ya hemos visto antes que las mujeres racializadas

161
00:19:14,760 --> 00:19:22,760
son las personas que peor se identifican, y por lo tanto en Inglaterra, como era,

162
00:19:22,760 --> 00:19:35,760
era un 90%. Un 90% de las personas detenidas y identificadas son inocentes,

163
00:19:35,760 --> 00:19:41,760
lo cual ya han pasado el trauma de ir a una comisaría que te detenga para decir,

164
00:19:41,760 --> 00:19:46,760
oops, mi algoritmo iba mal. Y luego hay mis sistemas como Predpol,

165
00:19:46,760 --> 00:19:51,760
os prometo que cada vez que alguien me vuelve a decir Predpol,

166
00:19:51,760 --> 00:19:54,760
a mí hay una niúrana en mi cerebro que dice, no, por qué.

167
00:19:54,760 --> 00:19:59,760
Son sistemas de predicción donde poner policías en las comunidades.

168
00:19:59,760 --> 00:20:06,760
Y de nuevo, o sorpresa, sorpresa, las comunidades con minorías étnicas

169
00:20:06,760 --> 00:20:13,760
son donde más policía se pone, generando un trauma para las comunidades.

170
00:20:13,760 --> 00:20:19,760
El sistema médico, esto es un caso como Qt, es un caso Mony,

171
00:20:19,760 --> 00:20:25,760
pueden tener muchas complicaciones, pero bueno, esta es como la versión más,

172
00:20:25,760 --> 00:20:29,760
es más softy del problema. Es que se estaba investigando,

173
00:20:29,760 --> 00:20:34,760
tenían un modelo de convivicional que funcionaba muy bien,

174
00:20:34,760 --> 00:20:40,760
y dije, ostras, estaría muy guay para predecir cáncer de pulmón.

175
00:20:40,760 --> 00:20:46,760
Y pensaron, estaría muy bien saber qué sería parte más relevante,

176
00:20:46,760 --> 00:20:48,760
igual para decirle a las medicadas,

177
00:20:48,760 --> 00:20:52,760
mira, tienes que mirar en esta zona de pulmón, o estas características.

178
00:20:52,760 --> 00:20:57,760
Y mirando donde miraba la red, se dieron cuenta que miraba,

179
00:20:57,760 --> 00:21:05,760
ahí está, que miraba este punto, decían, no hay nada ahí,

180
00:21:05,760 --> 00:21:07,760
no hay pulmón, en el hombro no hay pulmón,

181
00:21:07,760 --> 00:21:14,760
como hay un sistema que es capaz de reconocer cáncer pulmonar en el hombro.

182
00:21:14,760 --> 00:21:17,760
Lo que había ahí era una marca del hospital,

183
00:21:17,760 --> 00:21:23,760
el hospital marcaba las biografías con cáncer, con una marca en el hombro,

184
00:21:23,760 --> 00:21:28,760
por lo tanto lo que estaba mirando era las marcas, no estaba aprendiendo nada,

185
00:21:28,760 --> 00:21:31,760
el modelo era perfecto, absolutamente perfecto,

186
00:21:31,760 --> 00:21:34,760
habían humanas que habían identificado el cáncer antes.

187
00:21:37,760 --> 00:21:41,760
Y aquí me podéis decir, por recolecta más datos,

188
00:21:41,760 --> 00:21:44,760
si el problema es que la mayor parte de los dataset

189
00:21:44,760 --> 00:21:48,760
han sido recolectados con hombres blancos en el centro,

190
00:21:48,760 --> 00:21:52,760
porque no recolectamos más datos y ya está.

191
00:21:52,760 --> 00:21:58,760
Pues sí y no, porque el histórico y el contexto de cómo se recojan los datos,

192
00:21:58,760 --> 00:22:00,760
también rara de se tienen cuenta.

193
00:22:00,760 --> 00:22:03,760
Un claro ejemplo es este dataset que existe,

194
00:22:03,760 --> 00:22:06,760
este escalón en el gobierno de Estados Unidos,

195
00:22:06,760 --> 00:22:10,760
que son personas que han sido, son los mack shots, las cánceras,

196
00:22:10,760 --> 00:22:13,760
imágenes que te hacen cuando te detiene,

197
00:22:13,760 --> 00:22:17,760
y estas personas no han dado su consentimiento para pertenecer a esos dataset.

198
00:22:17,760 --> 00:22:21,760
Nosotras no deberíamos ser capaces de entrenar con usuarias

199
00:22:21,760 --> 00:22:24,760
que no han dado su consentimiento entusiástico.

200
00:22:26,760 --> 00:22:29,760
Y luego hay un dataset super conocido que es MitchNet,

201
00:22:29,760 --> 00:22:33,760
que ha sido etiquetado y cuando, de hecho, se retiquetó hace poco,

202
00:22:33,760 --> 00:22:38,760
porque es cuando se evaluaron las etiquetas que se habían utilizado,

203
00:22:38,760 --> 00:22:43,760
se dieron cuenta que había muchas etiquetas racistas, sexistas, xenófobas,

204
00:22:43,760 --> 00:22:47,760
así que es importante.

205
00:22:47,760 --> 00:22:52,760
Y luego en el caso del lenguaje, el lenguaje, el contexto es muy importante.

206
00:22:52,760 --> 00:22:55,760
Y haremos mucho de las comunidades Queer,

207
00:22:55,760 --> 00:22:59,760
y es porque es el tema que yo más controlo,

208
00:22:59,760 --> 00:23:02,760
pero las comunidades Queer reclamamos el lenguaje.

209
00:23:02,760 --> 00:23:06,760
Por ejemplo, no es lo mismo que yo llamé a una amiga,

210
00:23:06,760 --> 00:23:12,760
bollo, bollera, que una persona fuera del colectivo me llame bollera.

211
00:23:12,760 --> 00:23:15,760
Es un concepto completamente diferente,

212
00:23:15,760 --> 00:23:22,760
y la reclamación del lenguaje por parte de las comunidades es no trivial.

213
00:23:22,760 --> 00:23:28,760
No podemos decir, esta palabra es apropiada o no.

214
00:23:28,760 --> 00:23:32,760
Hay comunidades que deben y quieren utilizar ese lenguaje,

215
00:23:32,760 --> 00:23:38,760
y otras comunidades que no deberían utilizar, que es ofensivo, es tóxico.

216
00:23:38,760 --> 00:23:46,760
Hay otro punto en el de los trabajadores, en la recolección de datos,

217
00:23:46,760 --> 00:23:50,760
y es una cosa que muchas veces obvíamos.

218
00:23:50,760 --> 00:23:52,760
Yo creo que en un montón de charlas,

219
00:23:52,760 --> 00:23:55,760
y habéis visto que las trabajadoras en tech,

220
00:23:55,760 --> 00:23:59,760
somos una de las profesiones mejor pagadas.

221
00:23:59,760 --> 00:24:02,760
Sí, y no.

222
00:24:02,760 --> 00:24:06,760
La parte que no es que estas trabajadores en tech,

223
00:24:06,760 --> 00:24:10,760
las personas que etiquetan los datasets, cobran de media,

224
00:24:10,760 --> 00:24:16,760
en plataformas como Amazon, Tark, Mechanical Tark, cobran de medio dos dólares la hora.

225
00:24:16,760 --> 00:24:19,760
Y en casos excepcionales, hasta siete dólares.

226
00:24:19,760 --> 00:24:22,760
Eso es misterio, y son trabajadoras en tech.

227
00:24:22,760 --> 00:24:26,760
No podríamos hacer todos los modelos que entrenamos después,

228
00:24:26,760 --> 00:24:29,760
sin tener a personas que etiquetan los datasets.

229
00:24:29,760 --> 00:24:32,760
Y este trabajo estaba completamente en la sombra.

230
00:24:32,760 --> 00:24:41,760
Incluso cuando los datos son etiquetados de manera entusiasticamente,

231
00:24:41,760 --> 00:24:47,760
el usuario da un consentimiento entusiástico, como es, en los censos,

232
00:24:47,760 --> 00:24:53,760
cómo se utilizan y cómo se analizan esos datos, es político.

233
00:24:53,760 --> 00:24:55,760
Por ejemplo, en el caso de Escocia,

234
00:24:55,760 --> 00:25:03,760
Escocia ha hecho este año hablar con investigadores queer y intentar medir

235
00:25:03,760 --> 00:25:06,760
el historial trans de las personas en Escocia.

236
00:25:06,760 --> 00:25:10,760
Fue muy bien, honestamente.

237
00:25:10,760 --> 00:25:14,760
Hay un libro que explica esto espectacular.

238
00:25:14,760 --> 00:25:18,760
Lo que pasa cuando se analizan, porque Escocia hizo un análisis cualitativo,

239
00:25:18,760 --> 00:25:22,760
es decir, se sentó con las personas y habló con las personas.

240
00:25:22,760 --> 00:25:25,760
Cuando se analizan de forma cualitativa,

241
00:25:25,760 --> 00:25:31,760
lo que pasa es que grupos sobrerepresentados ocupan todo,

242
00:25:31,760 --> 00:25:34,760
grupos que son mayoritarios, acaban siendo sobrerepresentados,

243
00:25:34,760 --> 00:25:36,760
ocupando todo el espacio.

244
00:25:36,760 --> 00:25:41,760
Y luego hay una pescadilla que se muerde la cola,

245
00:25:41,760 --> 00:25:47,760
que es el recoger datos es el trabajo, no el modificar la vida de las personas.

246
00:25:47,760 --> 00:25:53,760
Así que no siempre recoger más datos es lo importante.

247
00:25:53,760 --> 00:25:58,760
Además de recoger datos, al fin de la normalidad,

248
00:25:58,760 --> 00:26:05,760
Facebook tenía hace millones de años muchísimas identidades sexuales.

249
00:26:05,760 --> 00:26:10,760
Y lo que pasaba por detrás es que se agregó.

250
00:26:10,760 --> 00:26:16,760
Obviamente pierdes todo el detalle y la parte interesante

251
00:26:16,760 --> 00:26:18,760
de esos datos.

252
00:26:18,760 --> 00:26:20,760
Y es que el género es performativo.

253
00:26:20,760 --> 00:26:23,760
Lo que hacemos las humanas es performar,

254
00:26:23,760 --> 00:26:26,760
recibimos feedback de la sociedad,

255
00:26:26,760 --> 00:26:28,760
nos llevamos mucho maquillaje,

256
00:26:28,760 --> 00:26:29,760
llevamos poco maquillaje,

257
00:26:29,760 --> 00:26:31,760
eres muy femenina, poco femenina.

258
00:26:31,760 --> 00:26:35,760
Y a partir de ahí ajustamos nuestra percepción de género.

259
00:26:35,760 --> 00:26:38,760
Cambia con las máquinas, hay un día que le dices,

260
00:26:38,760 --> 00:26:43,760
soy hombre, soy mujer, con suerte, no binario,

261
00:26:43,760 --> 00:26:46,760
con suerte algo más, pero ya está.

262
00:26:46,760 --> 00:26:50,760
Esa es la percepción que tienen las máquinas de nuestro género

263
00:26:50,760 --> 00:26:52,760
hasta que nos muramos.

264
00:26:52,760 --> 00:26:56,760
No tenemos una capacidad de interactuar

265
00:26:56,760 --> 00:27:00,760
y generar esa performidad de género.

266
00:27:07,760 --> 00:27:09,760
Hay veces que no solo el status quo,

267
00:27:09,760 --> 00:27:11,760
no solo la junta central,

268
00:27:11,760 --> 00:27:16,760
no solo el sistema.

269
00:27:16,760 --> 00:27:20,760
Hay veces que las usuarias somos también las responsables

270
00:27:20,760 --> 00:27:22,760
de lo que pasa.

271
00:27:22,760 --> 00:27:27,760
Estas fueron dos fallas que Plasmo hizo una artista fallera

272
00:27:27,760 --> 00:27:31,760
y que fueron mutiladas porque al parecer el cuerpo de una mujer

273
00:27:31,760 --> 00:27:33,760
es mutilable.

274
00:27:36,760 --> 00:27:39,760
¿Cómo se representa esto en inteligencia artificial?

275
00:27:39,760 --> 00:27:42,760
Prometo que a esta slide le daba como mil vueltas

276
00:27:42,760 --> 00:27:47,760
de cómo llamarla y ninguna era safer work.

277
00:27:47,760 --> 00:27:50,760
Porque a mí esto me enfada muy fuerte.

278
00:27:50,760 --> 00:27:54,760
Alguien decidió, bueno, un youtuber,

279
00:27:54,760 --> 00:27:56,760
decidió que quería ser famoso

280
00:27:56,760 --> 00:28:01,760
y entró en un modelo de GPT-3, lo fine-tuneo,

281
00:28:01,760 --> 00:28:06,760
lo ajusto, utilizando datos de Fort Champol.

282
00:28:06,760 --> 00:28:09,760
Si no sabéis qué es, mi señora buena,

283
00:28:09,760 --> 00:28:12,760
porque vivís en un mundo mejor que yo vivo,

284
00:28:12,760 --> 00:28:16,760
es lo peor de internet.

285
00:28:16,760 --> 00:28:18,760
La gente que hice políticamente incorrecto

286
00:28:18,760 --> 00:28:23,760
cuando quieren ser fascistas, racistas, fantasia,

287
00:28:23,760 --> 00:28:25,760
gente que hice, ¿por qué?

288
00:28:25,760 --> 00:28:29,760
Además, esta persona no solo lo entró,

289
00:28:29,760 --> 00:28:32,760
sino que lo desplegó públicamente.

290
00:28:32,760 --> 00:28:36,760
Y el objetivo era generar los contenidos más tóxicos.

291
00:28:36,760 --> 00:28:38,760
Complea su objetivo.

292
00:28:38,760 --> 00:28:42,760
Este modelo estuvo hospedado en Hang & Face,

293
00:28:42,760 --> 00:28:45,760
Hang & Face lo desactivó, pero más de 1.402 cargas

294
00:28:45,760 --> 00:28:47,760
se hicieron en el modelo.

295
00:28:47,760 --> 00:28:49,760
Tenía un model card que luego hablaremos de él,

296
00:28:49,760 --> 00:28:53,760
tendría un dataset card, pero alguien decidió

297
00:28:53,760 --> 00:28:56,760
que era importante hacer esto.

298
00:28:58,760 --> 00:29:01,760
Y ahora quiero que nos vayamos muy, muy atrás.

299
00:29:01,760 --> 00:29:05,760
Este es el caballo Hans y su...

300
00:29:05,760 --> 00:29:06,760
y su...

301
00:29:06,760 --> 00:29:08,760
Hansler, su manejador.

302
00:29:08,760 --> 00:29:12,760
Y en 1900 se pensaba que Hans era inteligente,

303
00:29:12,760 --> 00:29:14,760
porque podía hacer operaciones matemáticas,

304
00:29:14,760 --> 00:29:17,760
podía interactuar, parecía que...

305
00:29:17,760 --> 00:29:19,760
que era un caballo superdotado.

306
00:29:19,760 --> 00:29:24,760
Y lo que hacía el caballo Hans era muy inteligente,

307
00:29:24,760 --> 00:29:27,760
pero no inteligente como entendemos la inteligencia humana,

308
00:29:27,760 --> 00:29:30,760
era muy inteligente porque era capaz de entender

309
00:29:30,760 --> 00:29:33,760
los gestos de su cuidador,

310
00:29:33,760 --> 00:29:36,760
de la persona que hacía los trucos con él.

311
00:29:36,760 --> 00:29:39,760
Entonces el cuidador a veces respiraba, era casi inconsciente,

312
00:29:39,760 --> 00:29:41,760
respiraba cuando tenía que dejar...

313
00:29:41,760 --> 00:29:43,760
Bueno, el caballo Hans estaba pataditas

314
00:29:43,760 --> 00:29:45,760
cuando tenía que hacer las operaciones

315
00:29:45,760 --> 00:29:49,760
y paraba cuando había acabado, cuando era el número correcto.

316
00:29:49,760 --> 00:29:52,760
Y pues su cuidador respiraba fuerte, descansaba,

317
00:29:52,760 --> 00:29:57,760
hacía gestos mínimos que hacían que el caballo Hans

318
00:29:57,760 --> 00:29:59,760
supiera que tenía que parar.

319
00:29:59,760 --> 00:30:02,760
Y cuando se investigó en profundidad se dieron cuenta

320
00:30:02,760 --> 00:30:04,760
de que el caballo Hans era muy inteligente,

321
00:30:04,760 --> 00:30:07,760
pero no sabía hacer operaciones matemáticas.

322
00:30:07,760 --> 00:30:10,760
Y esto pasó en este año, este año mismo,

323
00:30:10,760 --> 00:30:13,760
que un ingeniero de Google afirmó que en Lambda,

324
00:30:13,760 --> 00:30:16,760
uno de los grandes modelos de lenguaje, era sintiente.

325
00:30:16,760 --> 00:30:21,760
Yo prometo que he jugado con Lambda y es brutal,

326
00:30:21,760 --> 00:30:24,760
pero no es inteligente.

327
00:30:24,760 --> 00:30:26,760
Y está bien, eso es lo que queremos.

328
00:30:26,760 --> 00:30:30,760
Vale, y esta es la imagen que quiero que luego hablemos.

329
00:30:30,760 --> 00:30:34,760
Es una imagen, la primera imagen generada completamente por ordenador,

330
00:30:34,760 --> 00:30:35,760
a través de...

331
00:30:35,760 --> 00:30:37,760
Bueno, claro, hay muchísimas generadas por ordenador,

332
00:30:37,760 --> 00:30:41,760
generada por un modelo de difusión en el cual la persona escribió

333
00:30:41,760 --> 00:30:47,760
un trozo de texto y luego se generó esta imagen.

334
00:30:47,760 --> 00:30:51,760
Una imagen espectacular y muy racista,

335
00:30:51,760 --> 00:30:57,760
porque bebe del orientalismo, que es un movimiento

336
00:30:57,760 --> 00:31:00,760
en el cual los europeos llegaron a hacer...

337
00:31:00,760 --> 00:31:04,760
Ostras, esto es cute, vamos a copiarlo.

338
00:31:04,760 --> 00:31:09,760
Y sobreidentificaron lo que era el orientalismo.

339
00:31:09,760 --> 00:31:12,760
Cogieron las partes de la cultura que les parecían interesantes,

340
00:31:12,760 --> 00:31:17,760
completamente alienando a la cultura que generaba estas imágenes.

341
00:31:17,760 --> 00:31:20,760
Es una imagen espectacular.

342
00:31:20,760 --> 00:31:22,760
Y ahora pondría el fondo de pantalla.

343
00:31:22,760 --> 00:31:27,760
Creo que el arte es otra cosa y podemos hablar mucho de lo que es el arte.

344
00:31:27,760 --> 00:31:30,760
Y la inteligencia artificial...

345
00:31:30,760 --> 00:31:33,760
Me voy a beber.

346
00:31:33,760 --> 00:31:36,760
La inteligencia artificial tiene un coste.

347
00:31:36,760 --> 00:31:42,760
A veces lo ignoramos, pero tiene un coste.

348
00:31:42,760 --> 00:31:46,760
Os he contado que las fallas se queman.

349
00:31:46,760 --> 00:31:52,760
Que quemar un monumento de un edificio de tres pisos

350
00:31:52,760 --> 00:31:55,760
hecho por Expan en los últimos años,

351
00:31:55,760 --> 00:32:00,760
antiguamente grande madera de lo que les sobraba los carpinteros.

352
00:32:00,760 --> 00:32:03,760
Ahora se hace por Expan porque es más barato.

353
00:32:03,760 --> 00:32:10,760
Aunque hay una vuelta a la técnica tradicional,

354
00:32:10,760 --> 00:32:14,760
quemar fallas en un paper se descubrió que se identificó

355
00:32:14,760 --> 00:32:19,760
como las que se emitían del que se utilizaban los tóxicos.

356
00:32:19,760 --> 00:32:24,760
Para la invención, un millón de habitantes contra los que tenemos.

357
00:32:24,760 --> 00:32:27,760
Y esto de la izquierda,

358
00:32:27,760 --> 00:32:30,760
se hace con cursos aparte de las fallas.

359
00:32:30,760 --> 00:32:32,760
Con cursos de luces.

360
00:32:32,760 --> 00:32:37,760
Tenemos calles completamente iluminadas en los últimos años con LEDs,

361
00:32:37,760 --> 00:32:40,760
tradicionalmente con bombillas,

362
00:32:40,760 --> 00:32:46,760
consumiendo una cantidad de electricidad absolutamente desorbitante.

363
00:32:46,760 --> 00:32:51,760
¿Cómo se traduce esto en la inteligencia artificial?

364
00:32:51,760 --> 00:32:55,760
Pues la inteligencia artificial tiene un coste para la tierra,

365
00:32:55,760 --> 00:32:59,760
porque crear todos estos ordenadores, todas estas máquinas,

366
00:32:59,760 --> 00:33:04,760
se quiere minar muy asensivamente y refrigerar

367
00:33:04,760 --> 00:33:09,760
utilizando fundamentalmente agua y temas de refrigeración.

368
00:33:09,760 --> 00:33:13,760
Que tiene un coste muy evidente para la tierra.

369
00:33:13,760 --> 00:33:16,760
Dejame que repita esto porque a mí esta frase

370
00:33:16,760 --> 00:33:18,760
parece absolutamente crítica.

371
00:33:18,760 --> 00:33:22,760
La minería nunca ha sido rentable,

372
00:33:22,760 --> 00:33:25,760
ni para el medio ambiente, ni para las personas.

373
00:33:25,760 --> 00:33:28,760
Es rentable porque ignoramos el coste que tiene

374
00:33:28,760 --> 00:33:31,760
para los cuerpos de las personas que tienen que minar

375
00:33:31,760 --> 00:33:33,760
y para el medio ambiente.

376
00:33:33,760 --> 00:33:36,760
Y luego hay metáforas como la nube,

377
00:33:36,760 --> 00:33:39,760
que nos hacen pensar que la tecnología es limpia,

378
00:33:39,760 --> 00:33:43,760
que es totalmente inocua, pero no es verdad.

379
00:33:43,760 --> 00:33:47,760
La nube son un montón de ordenadores en una granja en Iowa,

380
00:33:47,760 --> 00:33:51,760
consumiendo electricidad verde,

381
00:33:51,760 --> 00:33:54,760
pero electricidad no podemos olvidar

382
00:33:54,760 --> 00:33:59,760
que incluso las energías renovables tienen un coste.

383
00:33:59,760 --> 00:34:02,760
Y este artículo es un poco antiguo,

384
00:34:02,760 --> 00:34:05,760
se llama Green and Yide, y os lo recomiendo muy fuerte.

385
00:34:05,760 --> 00:34:10,760
Y lo que hacían era estimar lo que había costado

386
00:34:10,760 --> 00:34:12,760
entrar en un modelo grande.

387
00:34:12,760 --> 00:34:17,760
Un modelo grande en aquella época eran 200 millones de parámetros.

388
00:34:17,760 --> 00:34:20,760
Y estimaban que entrenar esos modelos

389
00:34:20,760 --> 00:34:26,760
consumía tanto como la vida útil de cinco coches.

390
00:34:26,760 --> 00:34:29,760
Y es que a veces nos centramos

391
00:34:29,760 --> 00:34:33,760
en mejorar uno por ciento la precisión de los modelos,

392
00:34:33,760 --> 00:34:36,760
sin tener en cuenta todas las implicaciones

393
00:34:36,760 --> 00:34:39,760
que tienen en entrenar todos estos modelos.

394
00:34:41,760 --> 00:34:43,760
Hay un poco de esperanza.

395
00:34:43,760 --> 00:34:46,760
Este paper yo le leo con un poco de visión crítica,

396
00:34:46,760 --> 00:34:49,760
pero las tarjetas gráficas han mejorado bastante.

397
00:34:49,760 --> 00:34:52,760
Esta es una comparativa entre los V100 de Nvidia

398
00:34:52,760 --> 00:34:54,760
entrenando GPT-3, Contra Clam,

399
00:34:54,760 --> 00:34:58,760
que es un modelo diez veces más grande,

400
00:34:58,760 --> 00:35:00,760
entrenados con TPU-V4,

401
00:35:00,760 --> 00:35:04,760
y las emisiones han bajado radicalmente.

402
00:35:04,760 --> 00:35:08,760
Aun así, los modelos han crecido casi linealmente.

403
00:35:08,760 --> 00:35:13,760
Entonces, si tenemos formas más eficientes de leer,

404
00:35:13,760 --> 00:35:18,760
de entrenar modelos, no inocuas.

405
00:35:21,760 --> 00:35:25,760
Y ahora es cuando has cuento que os he mentido todo este rato.

406
00:35:25,760 --> 00:35:27,760
Yo hablando desde las fallas,

407
00:35:27,760 --> 00:35:30,760
son monumentos, cosas que quemamos,

408
00:35:30,760 --> 00:35:34,760
y algunas, igual, sabéis que las fallas son verbenas y fiestas,

409
00:35:34,760 --> 00:35:38,760
pero las fallas, fundamentalmente, son comunidad.

410
00:35:38,760 --> 00:35:43,760
Es mi ya yo, que era zapatero reflejado en la falla,

411
00:35:43,760 --> 00:35:46,760
porque la comunidad agradecía sus servicios,

412
00:35:46,760 --> 00:35:48,760
estaba jubilando.

413
00:35:48,760 --> 00:35:50,760
La falla es comunidad.

414
00:35:50,760 --> 00:35:53,760
Esto es una muy xeranga, no es un castillo,

415
00:35:53,760 --> 00:35:56,760
porque las Valencianas también hacemos torres humanas,

416
00:35:56,760 --> 00:35:59,760
y a mí las muy xerangas me parecen maravillosas,

417
00:35:59,760 --> 00:36:02,760
porque es confiar completamente en la comunidad,

418
00:36:02,760 --> 00:36:06,760
en que te vas a sostener y en que puedes subir para arriba.

419
00:36:06,760 --> 00:36:09,760
Y la falla de la izquierda,

420
00:36:09,760 --> 00:36:12,760
esto es una falla, es una falla experimental

421
00:36:12,760 --> 00:36:14,760
que se quemó en el Burning Man,

422
00:36:14,760 --> 00:36:17,760
porque sorprendentemente, en el Burning Man,

423
00:36:17,760 --> 00:36:19,760
no quemaban cosas.

424
00:36:19,760 --> 00:36:20,760
Y puedo que yo, la Valenciana,

425
00:36:20,760 --> 00:36:23,760
decir, queridas, se llama Burning,

426
00:36:23,760 --> 00:36:26,760
que me, que me, que me.

427
00:36:26,760 --> 00:36:29,760
Estas fallas experimentales

428
00:36:29,760 --> 00:36:33,760
hacen, llaman a la creatividad y a la inteligencia,

429
00:36:33,760 --> 00:36:35,760
y a la comunidad.

430
00:36:35,760 --> 00:36:38,760
Las Valencianas somos un pueblo de,

431
00:36:38,760 --> 00:36:40,760
somos un pueblo de creativas,

432
00:36:40,760 --> 00:36:42,760
un pueblo en el cual hay músicas,

433
00:36:42,760 --> 00:36:44,760
hay talento por todos lados,

434
00:36:44,760 --> 00:36:46,760
y tenemos mucha comunidad,

435
00:36:46,760 --> 00:36:48,760
muchas fallas de estas alternativas

436
00:36:48,760 --> 00:36:51,760
que se hacen por las vecinas.

437
00:36:51,760 --> 00:36:54,760
Yo recomiendo que si venís a Valencia,

438
00:36:54,760 --> 00:36:57,760
vayáis, venís a Valencia y a las fallas.

439
00:36:57,760 --> 00:36:58,760
Primero que os pongáis sapones,

440
00:36:58,760 --> 00:37:02,760
porque nos gustan mucho, mucho, mucho los petardos.

441
00:37:02,760 --> 00:37:06,760
Y luego que vayáis a ver las fallas alternativas.

442
00:37:06,760 --> 00:37:08,760
También vayáis a las desecciones especial,

443
00:37:08,760 --> 00:37:10,760
porque son espectaculares,

444
00:37:10,760 --> 00:37:14,760
pero aquí es donde está la falla de verdad.

445
00:37:14,760 --> 00:37:18,760
Y ahora tengo un par de recomendaciones

446
00:37:18,760 --> 00:37:23,760
en función de vuestros perfiles.

447
00:37:23,760 --> 00:37:26,760
Ingenieres, transparencia.

448
00:37:26,760 --> 00:37:28,760
No vamos a ser perfectas,

449
00:37:28,760 --> 00:37:31,760
pero podemos ser transparentes en lo que hacemos.

450
00:37:31,760 --> 00:37:34,760
Podemos auditar lo que hacemos.

451
00:37:34,760 --> 00:37:37,760
El generar datos siempre ha sido como

452
00:37:37,760 --> 00:37:42,760
la parte menos ingeniera, o la parte menos científica.

453
00:37:42,760 --> 00:37:43,760
No es verdad.

454
00:37:43,760 --> 00:37:46,760
Los datos determinan en gran medida

455
00:37:46,760 --> 00:37:48,760
que lo pasa detrás.

456
00:37:48,760 --> 00:37:50,760
En este paper, que es un paper que surgió

457
00:37:50,760 --> 00:37:53,760
después del dataset card, dataset models,

458
00:37:53,760 --> 00:37:58,760
propone un ciclo en el cual se tiene en cuenta

459
00:37:58,760 --> 00:38:01,760
todos los ciclos para recoger dataset.

460
00:38:01,760 --> 00:38:03,760
Si el dataset es relevante,

461
00:38:03,760 --> 00:38:05,760
si realmente necesitas recoger dataset,

462
00:38:05,760 --> 00:38:07,760
quiénes son las personas involucradas.

463
00:38:07,760 --> 00:38:10,760
Y en cada etapa se dejan documentos

464
00:38:10,760 --> 00:38:14,760
en los cuales se escriben la motivación.

465
00:38:14,760 --> 00:38:17,760
Cuestionémonos absolutamente todo lo que tenemos.

466
00:38:17,760 --> 00:38:20,760
Cuestionémonos si hay sesgos, como son.

467
00:38:20,760 --> 00:38:22,760
Y con esos cuestionamientos,

468
00:38:22,760 --> 00:38:24,760
publicamos un dataset,

469
00:38:24,760 --> 00:38:29,760
una hoja de especificaciones de nuestros datasets.

470
00:38:29,760 --> 00:38:32,760
Porque yo cuando voy a gastar vuestros datasets,

471
00:38:32,760 --> 00:38:37,760
quiero saber qué está entrenando mi modelo.

472
00:38:37,760 --> 00:38:40,760
Transparencia en los datasets.

473
00:38:40,760 --> 00:38:42,760
Pues al otro lado, transparencia en los modelos.

474
00:38:42,760 --> 00:38:44,760
Y no es que documentar absolutamente

475
00:38:44,760 --> 00:38:46,760
todos los casos de uso,

476
00:38:46,760 --> 00:38:48,760
quiénes va a estar implicado.

477
00:38:48,760 --> 00:38:52,760
Esto es un ejemplo de los primeros

478
00:38:52,760 --> 00:38:55,760
de cómo se escribiría un model card.

479
00:38:55,760 --> 00:38:57,760
Y cuando se publica un paper,

480
00:38:57,760 --> 00:39:00,760
se publica si estáis inclinando un dataset nuevo

481
00:39:00,760 --> 00:39:04,760
con el dataset y si es un modelo nuevo con el model card.

482
00:39:04,760 --> 00:39:06,760
Porque no vamos a cambiar las cosas,

483
00:39:06,760 --> 00:39:08,760
pero por lo menos vamos a ser muy conscientes

484
00:39:08,760 --> 00:39:10,760
de qué estamos haciendo.

485
00:39:10,760 --> 00:39:12,760
Y eso nos permite reflexionar

486
00:39:12,760 --> 00:39:15,760
en qué modelos, qué necesidades.

487
00:39:15,760 --> 00:39:18,760
Y la otra parte es transparencia en los recursos.

488
00:39:18,760 --> 00:39:21,760
Honestamente, muy pocas veces he visto papers

489
00:39:21,760 --> 00:39:25,760
que publican cuánto ha costado entrenar los modelos.

490
00:39:25,760 --> 00:39:29,760
Pero os recomiendo si podéis que lo publicéis.

491
00:39:29,760 --> 00:39:32,760
Que publicéis cuánto CO2 ha consumido

492
00:39:32,760 --> 00:39:34,760
entrar a vuestro modelo,

493
00:39:34,760 --> 00:39:37,760
tanto en el entrenamiento como en el ajuste de parámetros.

494
00:39:37,760 --> 00:39:41,760
Si podéis que utilicéis energías,

495
00:39:41,760 --> 00:39:46,760
elegid la fuente de entrenamiento más con energías renovables.

496
00:39:46,760 --> 00:39:49,760
Y luego, si... Esto es mucho más nicho.

497
00:39:49,760 --> 00:39:52,760
Pero si sois editoras de revistas,

498
00:39:52,760 --> 00:39:56,760
que no tengáis solamente en cuenta la eficiencia

499
00:39:56,760 --> 00:40:00,760
y el perfón, en tal vez en cuenta

500
00:40:00,760 --> 00:40:04,760
otros tipos de cosas de los modelos.

501
00:40:04,760 --> 00:40:09,760
A veces, la mejora de la eficiencia no es valida la pena

502
00:40:09,760 --> 00:40:13,760
teniendo que entrenar a un modelo 10, 30 veces más grande.

503
00:40:13,760 --> 00:40:19,760
Y este es un sitio donde caemos entre ingenieras y investigadoras.

504
00:40:19,760 --> 00:40:24,760
Hay varios papers que cuestionan las leyes de escalado.

505
00:40:24,760 --> 00:40:27,760
Al principio se creía que si queríamos subir en precisión,

506
00:40:27,760 --> 00:40:33,760
teníamos que linealmente más datos, más ciclos de entrenamiento.

507
00:40:33,760 --> 00:40:37,760
Y... a ver...

508
00:40:37,760 --> 00:40:40,760
Esto es como donde estábamos.

509
00:40:40,760 --> 00:40:44,760
Y la gente de Chinchilla dijo...

510
00:40:44,760 --> 00:40:49,760
Véis aquí la cantidad de billones de parámetros que tiene GPT-3,

511
00:40:49,760 --> 00:40:51,760
Gopher y Chinchilla.

512
00:40:51,760 --> 00:40:54,760
Chinchilla simplemente entrenó un modelo mucho más pequeño,

513
00:40:54,760 --> 00:40:56,760
mucho más tiempo.

514
00:40:56,760 --> 00:40:59,760
Y obtuvo igual o mucho mejor performance.

515
00:40:59,760 --> 00:41:05,760
El 90% del gasto del consumo en CO2

516
00:41:05,760 --> 00:41:07,760
no está en entrenamiento, está en inferencia,

517
00:41:07,760 --> 00:41:09,760
está cuando servimos los modelos.

518
00:41:09,760 --> 00:41:13,760
Por lo tanto, un modelo mucho más pequeño consume mucho menos CO2.

519
00:41:13,760 --> 00:41:15,760
Y si sois investigadoras,

520
00:41:15,760 --> 00:41:18,760
pensad en otro tipo de modelos.

521
00:41:18,760 --> 00:41:20,760
No todo tiene que ser transformers,

522
00:41:20,760 --> 00:41:24,760
que también pueden ser, pero no tienen que ser.

523
00:41:24,760 --> 00:41:28,760
Pensad en distillation, persar en retrieval.

524
00:41:28,760 --> 00:41:30,760
Y para las ingenieras,

525
00:41:30,760 --> 00:41:34,760
aquí es donde esta conferencia puede ser muy útil para vosotras.

526
00:41:34,760 --> 00:41:36,760
Que es que vais a país muy eficiente,

527
00:41:36,760 --> 00:41:41,760
va a determinar cuánto cuesta entrenar vuestros modelos.

528
00:41:41,760 --> 00:41:47,760
Y esto es mi pet beef, la cosa que más había mi vida.

529
00:41:47,760 --> 00:41:49,760
Que la gente me diga...

530
00:41:49,760 --> 00:41:51,760
modelo de T-Plearn es modelo de caja negra.

531
00:41:51,760 --> 00:41:54,760
Maricarme es modelo de caja negra porque no te has sentado a investigar.

532
00:41:54,760 --> 00:41:59,760
Porque es muy, muy complejo, eso te lo compro.

533
00:41:59,760 --> 00:42:02,760
Pero esto es del paper Decision Transformers,

534
00:42:02,760 --> 00:42:05,760
en los cuales miraban las primeras capas de transformers,

535
00:42:05,760 --> 00:42:09,760
porque hay muchas cosas, pero para ello investigación ahora mismo,

536
00:42:09,760 --> 00:42:11,760
súper interesante.

537
00:42:11,760 --> 00:42:14,760
Y si veis en los juegos de Atari, por ejemplo aquí,

538
00:42:14,760 --> 00:42:17,760
está mirando donde está el submarino.

539
00:42:17,760 --> 00:42:21,760
Se puede ver que estas son las partes de atención.

540
00:42:21,760 --> 00:42:26,760
Por lo tanto, vamos a investigar la representación

541
00:42:26,760 --> 00:42:28,760
si va a capturar los modelos.

542
00:42:28,760 --> 00:42:32,760
A lo mejor tenemos una red sobre dimensionada,

543
00:42:32,760 --> 00:42:35,760
a lo mejor no nos hace falta un modelo tan grande.

544
00:42:37,760 --> 00:42:41,760
Mi otra crítica es, somos ingenieras.

545
00:42:41,760 --> 00:42:43,760
Yo no soy investigadora social,

546
00:42:43,760 --> 00:42:45,760
yo no soy filosófa.

547
00:42:45,760 --> 00:42:47,760
Yo tengo la suerte de trabajar con muchas filosofas

548
00:42:47,760 --> 00:42:52,760
que me explican qué cosas son importantes.

549
00:42:52,760 --> 00:42:55,760
Entonces traigamos a filosofas, a lingüistas,

550
00:42:55,760 --> 00:43:00,760
a personas que se dedican a trabajar como comunidades.

551
00:43:00,760 --> 00:43:02,760
Porque nosotros solo somos ingenieras,

552
00:43:02,760 --> 00:43:10,760
no podemos poner soluciones técnicas absolutamente todos los problemas.

553
00:43:10,760 --> 00:43:20,760
Otra de las cosas que recomiendo que promováis el diseño colaborativo.

554
00:43:20,760 --> 00:43:22,760
En la parte...

555
00:43:22,760 --> 00:43:25,760
Esto tiene dos ejes, empoderamiento,

556
00:43:25,760 --> 00:43:27,760
cómo de empoderadas están las comunidades,

557
00:43:27,760 --> 00:43:29,760
y repciprocidad,

558
00:43:29,760 --> 00:43:32,760
cómo de reflexión pueden ser las comunidades

559
00:43:32,760 --> 00:43:34,760
que interactúan con nuestros modelos.

560
00:43:34,760 --> 00:43:37,760
En el modelo de desarrollo transaccional es el mechanical TARC.

561
00:43:37,760 --> 00:43:41,760
Le damos a unos etiquetadores un conjunto de datos,

562
00:43:41,760 --> 00:43:44,760
y decimos, me voy a gastar, ya te cuento para algo.

563
00:43:44,760 --> 00:43:46,760
No, te interesa.

564
00:43:46,760 --> 00:43:48,760
En el otro lado está el compromiso pasivo,

565
00:43:48,760 --> 00:43:55,760
que les cuestiona que las empresas o las instituciones montan,

566
00:43:55,760 --> 00:43:59,760
deciden qué es lo que es bueno para las comunidades.

567
00:44:00,760 --> 00:44:04,760
En el otro extremo está la consulta de las personas interesadas.

568
00:44:04,760 --> 00:44:07,760
Les consultamos a la comunidad, pero la comunidad no tiene capacidad

569
00:44:07,760 --> 00:44:10,760
de hacer, de implementar.

570
00:44:10,760 --> 00:44:14,760
Y el co-desarrollo es algo que es, honestamente, muy complejo,

571
00:44:14,760 --> 00:44:18,760
pero si invertimos, probablemente podemos llegar a las soluciones tecnológicas

572
00:44:18,760 --> 00:44:20,760
útiles realmente para la comunidad.

573
00:44:20,760 --> 00:44:24,760
En el cual lo que hacemos es, en lugar de hacer un desarrollo transaccional

574
00:44:24,760 --> 00:44:29,760
con las comunidades, involucrarlas, entender cuáles son sus necesidades,

575
00:44:29,760 --> 00:44:34,760
y investigar en función de esas necesidades de la comunidad,

576
00:44:34,760 --> 00:44:38,760
de darle acceso a sus datos, de darles empoderarlas.

577
00:44:39,760 --> 00:44:43,760
Y esto es muy complejo, porque sobre todo las que veníamos del OpenSource,

578
00:44:43,760 --> 00:44:47,760
tenemos licencias, estamos muy acostumbradas a licencias como GPI,

579
00:44:47,760 --> 00:44:52,760
y a veces no es simple compartir OpenSource,

580
00:44:52,760 --> 00:44:54,760
no es simple compartir nuestros modelos,

581
00:44:54,760 --> 00:44:59,760
porque igual no queremos que haya alguien que se quiera hacer famoso en YouTube

582
00:44:59,760 --> 00:45:03,760
haciendo un modelo extremadamente tóxico.

583
00:45:03,760 --> 00:45:09,760
Por lo tanto, hay otras formas de otras licencias, como Hippocratic,

584
00:45:09,760 --> 00:45:16,760
en el cual la idea fundamental es que no se violen los derechos humanos,

585
00:45:16,760 --> 00:45:22,760
y Ride liderado por Have Your Face, creo que es, bueno,

586
00:45:22,760 --> 00:45:25,760
una de las empresas que hace Responsible I.I.,

587
00:45:25,760 --> 00:45:30,760
en la cual se limitan los accesos a sistemas críticos,

588
00:45:30,760 --> 00:45:34,760
como el judicial y el sanitario.

589
00:45:36,760 --> 00:45:39,760
Y ahora como usuarias, como usuarias,

590
00:45:39,760 --> 00:45:44,760
a veces sentimos que no tenemos capacidad de influir en los modelos,

591
00:45:44,760 --> 00:45:46,760
de influir en lo que pasa en nuestro alrededor.

592
00:45:46,760 --> 00:45:51,760
Pues yo os quiero animar, en primer lugar, a que entendamos lo que está pasando,

593
00:45:51,760 --> 00:45:55,760
intentar aprender, comprender los modelos,

594
00:45:55,760 --> 00:45:57,760
cuestionarnos siempre los resultados,

595
00:45:57,760 --> 00:46:05,760
no vayamos a ser una persona que creamos que Hans es el caballó más inteligente del mundo.

596
00:46:05,760 --> 00:46:09,760
Demandar, creo que esa palabra es castellano,

597
00:46:09,760 --> 00:46:13,760
pero pedir diseños participativos,

598
00:46:13,760 --> 00:46:20,760
exigir que queremos ser parte de las soluciones tecnológicas que nos afectan,

599
00:46:20,760 --> 00:46:22,760
y por último, apoyo mutuo.

600
00:46:22,760 --> 00:46:27,760
Es importante que creemos un tejido conectado,

601
00:46:27,760 --> 00:46:31,760
en el cual, como en las fallas, la falla es comunidad,

602
00:46:31,760 --> 00:46:34,760
y si somos comunidad, somos muchísimo más fuertes.

603
00:46:36,760 --> 00:46:40,760
Con esto, estos son los recursos de Les Spicy Amor Spicy,

604
00:46:40,760 --> 00:46:43,760
de menos Hot Take, no sé, hablar en castellano ya,

605
00:46:43,760 --> 00:46:49,760
y los Papers, ahora luego os dejaré todo esto en Twitter,

606
00:46:49,760 --> 00:46:50,760
porque es mucho.

607
00:46:50,760 --> 00:46:53,760
Esto es la falla de Corona, mi falla favorita de la vida.

608
00:46:53,760 --> 00:46:58,760
Pone, yo para ser feliz, voy a un camión,

609
00:46:58,760 --> 00:47:00,760
yo para ser feliz quiero un camión,

610
00:47:00,760 --> 00:47:05,760
estoy un camión de Prosegur, que es una empresa que es el camión que llego a dinero,

611
00:47:05,760 --> 00:47:07,760
hicieron cosas muy guais.

612
00:47:07,760 --> 00:47:09,760
Corona es la falla más interesante de Polencia.

613
00:47:09,760 --> 00:47:15,760
Y con esto, estoy a vuestra disposición para que me hagáis todas las preguntas que queráis.

614
00:47:15,760 --> 00:47:20,760
Muchas gracias.

615
00:47:46,760 --> 00:47:51,760
Bueno, mientras lo pensáis, espero que me habéis visto en Twitter,

616
00:47:51,760 --> 00:47:54,760
pero tenemos estancias de investigación en DeepMind,

617
00:47:54,760 --> 00:48:00,760
y hacemos investigación en estos temas muy atúcticos,

618
00:48:00,760 --> 00:48:02,760
y nos interesa muy fuerte.

619
00:48:04,760 --> 00:48:05,760
Hola, May.

620
00:48:05,760 --> 00:48:06,760
Hola.

621
00:48:06,760 --> 00:48:07,760
Enhorabuena.

622
00:48:07,760 --> 00:48:08,760
Por la charla.

623
00:48:08,760 --> 00:48:09,760
Estoy aquí, estoy aquí.

624
00:48:09,760 --> 00:48:10,760
Hola.

625
00:48:10,760 --> 00:48:11,760
Estoy aquí, primera fila.

626
00:48:11,760 --> 00:48:15,760
Vale, enhorabuena por la charla.

627
00:48:15,760 --> 00:48:24,760
Hace no mucho estuve en una charla sobre la deontología o la tal que tiene que seguir la IA,

628
00:48:24,760 --> 00:48:28,760
y es verdad que todos los términos eran filosóficos y tal,

629
00:48:28,760 --> 00:48:31,760
sobre lo que tenemos que aprender mucho, de acuerdo.

630
00:48:31,760 --> 00:48:34,760
Pero me voy a ir a algo un poco más práctico.

631
00:48:34,760 --> 00:48:37,760
Me ha llamado la atención cuando has hablado de las licencias,

632
00:48:37,760 --> 00:48:40,760
y has nombrado otras licencias.

633
00:48:40,760 --> 00:48:42,760
Solamente quiero tu opinión.

634
00:48:42,760 --> 00:48:45,760
Si ya es difícil hacer que se respete un AGPL,

635
00:48:45,760 --> 00:48:50,760
y porque cuando te la rompen, te tienes que meter en muchos follones para que la respeten,

636
00:48:50,760 --> 00:48:54,760
y al final pasas, imagínate una hipocrática.

637
00:48:54,760 --> 00:48:55,760
Sí.

638
00:48:55,760 --> 00:48:57,760
Cuéntame, qué opinas.

639
00:48:57,760 --> 00:49:00,760
O sea, qué te podría hacer para mejorar eso?

640
00:49:00,760 --> 00:49:02,760
Se puede hacer pocas cosas.

641
00:49:02,760 --> 00:49:05,760
Creo que lo que tenemos que cambiar es un poco la mentalidad,

642
00:49:05,760 --> 00:49:07,760
igual no hay que compartir todas las cosas,

643
00:49:07,760 --> 00:49:12,760
porque nosotros estudiamos mucho el impacto que tienen los modelos,

644
00:49:12,760 --> 00:49:19,760
y tenemos un grupo que son filosóficas que piensan en todas las implicaciones sociales,

645
00:49:19,760 --> 00:49:23,760
y creo que las ingenieras tenemos que cambiar la mentalidad

646
00:49:23,760 --> 00:49:28,760
de que compartir el modelo, compartir el software, compartir los pesos,

647
00:49:28,760 --> 00:49:31,760
es la forma de avanzar la ciencia.

648
00:49:31,760 --> 00:49:37,760
Hay veces que publicar que estás trabajando en algo, o publicar el artículo.

649
00:49:37,760 --> 00:49:44,760
Ya le estás dando pistas a gente que quiera hacer el mal de cómo se puede implementar.

650
00:49:44,760 --> 00:49:48,760
Hay veces que simplemente decir, hemos hecho esto,

651
00:49:48,760 --> 00:49:53,760
abre el camino de posibilidad, abre el imaginario para gente que se podía hacer,

652
00:49:53,760 --> 00:49:55,760
por lo tanto, lo voy a implementar yo.

653
00:49:55,760 --> 00:49:57,760
Creo que hay que cambiar un poco la mentalidad,

654
00:49:57,760 --> 00:50:00,760
creo que hay que plantearse mucho, mucho, mucho, qué se comparte,

655
00:50:00,760 --> 00:50:03,760
y cómo se comparte, porque hagan face,

656
00:50:03,760 --> 00:50:07,760
que tiene hagan face son la gente que hace rail, que lidera rail,

657
00:50:07,760 --> 00:50:15,760
y tuvo durante días, forchampol, que es además el model car todavía está disponible,

658
00:50:15,760 --> 00:50:20,760
el model car es claramente, mi objetivo es ser el tronal modelo de lenguaje más tóxico.

659
00:50:20,760 --> 00:50:26,760
Pues eso, se hizo, se hizo, se hizo.

660
00:50:26,760 --> 00:50:31,760
Nosotras tenemos muy claro que por eso Dimine quiere ir muy deprisa,

661
00:50:31,760 --> 00:50:38,760
porque queremos liderar y ser ejemplo de cómo hacer las cosas de manera ética.

662
00:50:38,760 --> 00:50:41,760
Yo te voy a dar mi opinión, solamente como humilde opinión,

663
00:50:41,760 --> 00:50:45,760
y creo que no solamente nosotros tenemos que cambiar la mentalidad,

664
00:50:45,760 --> 00:50:49,760
pero por experiencia creo que eso no va a funcionar.

665
00:50:49,760 --> 00:50:53,760
Lo que tendría que cambiar es un poco la legislación.

666
00:50:53,760 --> 00:50:54,760
También.

667
00:50:54,760 --> 00:50:56,760
De acuerdo.

668
00:50:56,760 --> 00:51:00,760
Pero yo como ingenieras, no puedo cambiar la legislación,

669
00:51:00,760 --> 00:51:05,760
puedo cambiar lo que investigo y cómo lo comparto.

670
00:51:05,760 --> 00:51:07,760
Pero sí.

671
00:51:15,760 --> 00:51:17,760
Hola, muchas gracias por la charla.

672
00:51:17,760 --> 00:51:22,760
Te quería preguntar sobre los orígenes de los dataset,

673
00:51:22,760 --> 00:51:27,760
por ejemplo, con las presas que no daban permiso para que usaran sus fotos

674
00:51:27,760 --> 00:51:30,760
en el entrenamiento de modelos.

675
00:51:30,760 --> 00:51:34,760
Esto es una relación con las inteligencias artificiales de creación y procesamiento

676
00:51:34,760 --> 00:51:37,760
de imágenes que han ido surgiendo este año,

677
00:51:37,760 --> 00:51:42,760
porque en las comunidades digitales de artistas ha sido muy polémico.

678
00:51:42,760 --> 00:51:46,760
Aparte por la implicación en las condiciones laborales y todo eso,

679
00:51:46,760 --> 00:51:50,760
es porque los artistas no han dado permiso para que se usen sus imágenes

680
00:51:50,760 --> 00:51:55,760
de corpus de datos y supone incluso una infracción de la propiedad intelectual

681
00:51:55,760 --> 00:52:00,760
y que también se pueda extender a textos, traducciones, etcétera,

682
00:52:00,760 --> 00:52:03,760
en otro tipo de inteligencia artificial.

683
00:52:03,760 --> 00:52:05,760
Entonces, te quería preguntar si en el mundo académico

684
00:52:05,760 --> 00:52:07,760
se está hablando de la propiedad intelectual,

685
00:52:07,760 --> 00:52:10,760
cómo afecta esto, o aún no se la ha metido en manos.

686
00:52:10,760 --> 00:52:11,760
Sí.

687
00:52:11,760 --> 00:52:14,760
Bueno, yo estoy en la industria, pero estamos en una industria un poco rara,

688
00:52:14,760 --> 00:52:20,760
pero sí hablamos mucho del consentimiento entusiasco,

689
00:52:20,760 --> 00:52:25,760
además de las usuarias que dan sus datos y cómo estamos entrenando,

690
00:52:25,760 --> 00:52:32,760
porque incluso aunque no demos, aunque hayan datos que no son personal identificables,

691
00:52:32,760 --> 00:52:39,760
hay muchas técnicas de ataque a los modelos en los cuales se puede extraer información de las personas.

692
00:52:39,760 --> 00:52:41,760
Y ahora te voy a hablar como artista.

693
00:52:41,760 --> 00:52:45,760
No tenemos nada que temer, porque el arte es otra cosa.

694
00:52:45,760 --> 00:52:50,760
Bueno, y creo que como ingenieras, tampoco tenemos nada que pensar, que temer,

695
00:52:50,760 --> 00:52:57,760
porque el arte es la pulsión interna de generar y hablar de nuestra situación,

696
00:52:57,760 --> 00:53:02,760
queremos comunicar, y eso es una cosa que creo que estamos todavía muy lejos

697
00:53:02,760 --> 00:53:04,760
de inteligencia artificial para hacerlo.

698
00:53:04,760 --> 00:53:11,760
Y creo que siempre ahora falta humanas para las cosas que sean humanas críticas.

699
00:53:11,760 --> 00:53:13,760
¿Te lo has podido?

700
00:53:18,760 --> 00:53:23,760
Hola. Maravillosa charla, imprescindible charla.

701
00:53:23,760 --> 00:53:24,760
Gracias.

702
00:53:24,760 --> 00:53:29,760
Sueño con ser tu pie de nota, y por eso ocupo este pequeño espacio, perdón.

703
00:53:29,760 --> 00:53:36,760
Hoy, sobre todo lo que ha hablado Mai, hay aplicaciones de Python con librerías de Python que podéis usar.

704
00:53:36,760 --> 00:53:42,760
Hoy hay una charla que se llama, asegurando el tiro intervalos de confianza para tus modelos de machine learning.

705
00:53:45,760 --> 00:53:46,760
Pausa dramática.

706
00:53:47,760 --> 00:53:48,760
Perdón.

707
00:53:48,760 --> 00:53:51,760
Salvemos los pingüinos con el green computing.

708
00:53:52,760 --> 00:53:56,760
Análisis de red del discurso de odio cuirfóbico en Twitter.

709
00:53:56,760 --> 00:53:57,760
Eso es muy guay.

710
00:53:57,760 --> 00:53:59,760
Todas son muy guales, y esta más.

711
00:53:59,760 --> 00:54:01,760
Esa tengo muchas ganas de verla.

712
00:54:01,760 --> 00:54:07,760
Es súper de acuerdo. Ya me callo, perdón, pero recordaros que hay una charla hoy sobre explicabilidad local.

713
00:54:07,760 --> 00:54:10,760
¿Cómo interpretó la predicción?

714
00:54:12,760 --> 00:54:16,760
Ayer hubo una charla de Nerea Luis sobre ética también.

715
00:54:16,760 --> 00:54:31,760
Y el primer día hubo un taller sobre exactitud, concertidumbre, donde se explicaba cómo poner en práctica todo lo que ha explicado Mai y la charla de Nieves, por supuesto.

716
00:54:33,760 --> 00:54:34,760
Muchísimas gracias.

717
00:54:34,760 --> 00:54:37,760
A ti, pues vamos. Maravillosa charla.

718
00:54:37,760 --> 00:54:39,760
¿Me has dado un segundo antes de cerrar?

719
00:54:39,760 --> 00:54:40,760
Sí. Bueno.

720
00:54:41,760 --> 00:54:42,760
Si hay más preguntas, ¡ah, toqué!

721
00:54:42,760 --> 00:54:45,760
Quiero hacer un pie de página al final del todo.

722
00:54:48,760 --> 00:54:49,760
¿Estamos? Vale.

723
00:54:50,760 --> 00:54:54,760
Me ha hecho mucha ilusión que me preguntéis mujeres porque además me habéis preguntado.

724
00:54:54,760 --> 00:54:56,760
Era con cuestión marca al final.

725
00:54:56,760 --> 00:55:10,760
A mí me pasa mucho que yo trabajo... Mi trabajo es esto, interpretabilidad.

726
00:55:10,760 --> 00:55:23,760
Y en las reuniones, aunque la mayoría de tech son hombres blancos, la mayoría de personas que trabajamos en esto somos mujeres, personas queer y personas racializadas.

727
00:55:23,760 --> 00:55:26,760
¿Qué os pasa? No os interesa estas cosas.

728
00:55:27,760 --> 00:55:31,760
No es responsabilidad nuestra arreglarlo todo. Lo arreglaremos lo que podamos.

729
00:55:31,760 --> 00:55:35,760
Pero también es responsabilidad vuestra que haga es vuestro granito.

730
00:55:35,760 --> 00:55:37,760
Y ya está, ya me callo. Muchas gracias.

731
00:55:37,760 --> 00:55:51,760
Gracias.

