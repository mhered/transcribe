1
00:00:00,000 --> 00:00:23,360
Bueno, vamos a comenzar. Aquí tenemos a Miguel Fernández, que nos va a hacer una introducción

2
00:00:23,360 --> 00:00:31,800
a la causalidad y, igual que antes, las preguntas las hacemos al final, ¿vale? Cuando tú quieras.

3
00:00:31,800 --> 00:00:40,640
Hola a todos, buenas tardes. Hoy voy a hacer una presentación sobre una introducción a

4
00:00:40,640 --> 00:00:46,360
la causalidad. Es un tema muy grande, así que simplemente puedes ver ciertos conceptos

5
00:00:46,360 --> 00:00:54,480
clave para potenciarmente animaros también a explorarlo. Entonces, antes de nada, soy Miguel

6
00:00:54,480 --> 00:01:03,200
Alarcón, soy un data scientist en The Hotels Network, que es una empresa que se dedica a mejorar

7
00:01:03,200 --> 00:01:12,760
la conversión del tráfico web de los hoteles. Tiene una parte de customización en la que

8
00:01:12,760 --> 00:01:18,160
aparecen notas la típica debucking de 5 personas mirando ahora, cosas que puede hacer el hotelero.

9
00:01:18,160 --> 00:01:26,400
Pueden personalizar su web y también se les ofrece una parte analítica para que puedan ver cómo

10
00:01:26,400 --> 00:01:33,480
está funcionando todo. Entonces, donde entramos con Data Science para entender a los usuarios y

11
00:01:33,480 --> 00:01:40,120
entender también a los hoteleros. Y en este entendimiento es donde entra en juego la causalidad.

12
00:01:40,120 --> 00:01:48,160
La charla voy a hablar un poco de la causalidad, que es lo que nos motiva a utilizarla,

13
00:01:48,160 --> 00:01:55,240
lo cual es el principal problema que nos encontramos. Y finalmente acabar con herramientas de cómo llevarla

14
00:01:55,240 --> 00:02:02,640
a cabo y una especie de comentarios finales de atención que hay que tener particularmente.

15
00:02:02,640 --> 00:02:10,680
Entonces, ¿qué es la causalidad? La causalidad es una ciencia que intenta ver cómo ciertas

16
00:02:10,680 --> 00:02:19,360
cosas afectan a otras cosas, cómo una causa afecta a un efecto. Entonces, esto para que nos sirve,

17
00:02:19,360 --> 00:02:24,000
pues nos sirve para ver la relación que hay entre las diferentes variables de nuestro sistema.

18
00:02:25,280 --> 00:02:31,040
Dentro de estas variables, ¿cómo podemos ver el efecto que tiene cada una,

19
00:02:31,040 --> 00:02:37,840
cuantizar este efecto que tiene? En otra, en base a esto podemos tener un cierto tratamiento,

20
00:02:37,840 --> 00:02:43,320
que puede ser un medicamento, si quiere probar un medicamento. ¿Qué efecto tiene este medicamento?

21
00:02:43,320 --> 00:02:50,680
Luego en la tasa de supervivencia de la gente. Y llevándolo al extremo sería plantear unos

22
00:02:50,680 --> 00:02:55,400
hipotéticos casos llamados contrafactuales que es, vale, ya una persona le da el medicamento,

23
00:02:55,400 --> 00:03:01,440
¿qué habría pasado si no se lo hubiera dado? Que es algo que, por temas legales o por simplemente

24
00:03:01,440 --> 00:03:08,000
imposibilidad es el pasado, no tenemos acceso a ello. Entonces, vamos a empezar con un poco de notación.

25
00:03:09,320 --> 00:03:16,240
Tenemos lo que sería el tratamiento, que normalmente por simplificar lo consideramos

26
00:03:16,240 --> 00:03:22,640
binario 0, 1, hay potenciales extensiones a ello, pero sería, por ejemplo, en el caso del fármaco,

27
00:03:22,640 --> 00:03:29,160
aplicar el fármaco o no aplicarlo. Cuando lo aplicamos lo llamaríamos el grupo de tratamiento,

28
00:03:29,160 --> 00:03:35,760
cuando lo aplicamos, que sería la variable cuando te vale 0, es el grupo de control. Y luego,

29
00:03:35,760 --> 00:03:40,880
por otro lado, tenemos el outcome, que en este caso también por simplificar vamos a considerar

30
00:03:40,880 --> 00:03:46,880
lo binario 1, la persona sobrevive, 0 la persona no sobrevive. Dentro de esto tenemos nuestra

31
00:03:46,880 --> 00:03:52,400
muestra, que la muestra al final se compone de la gente que ha recibido el tratamiento, la gente

32
00:03:52,400 --> 00:04:00,080
que está en el grupo de control. Dentro de esto tenemos lo que observamos, es eso, son dos

33
00:04:00,080 --> 00:04:04,440
poblaciones distintas, la que ha recibido el tratamiento y la que ha recibido el control. Si

34
00:04:04,440 --> 00:04:09,880
queremos medir cómo de bien funciona el medicamento, no podemos sencillamente comparar una contra la

35
00:04:09,880 --> 00:04:14,960
otra, porque las personas no son las mismas. Entonces, ¿qué es lo que pretende la causalidad?

36
00:04:14,960 --> 00:04:23,120
Montar un sistema en el que diga, vale, y si aquí tuviera toda la población y lo comparara

37
00:04:23,120 --> 00:04:28,920
con toda la otra población, que es lo que se denomina intervenciones. Si hubiera intervenido

38
00:04:28,920 --> 00:04:36,120
en el sistema para poner toda la población con tratamiento, que es lo que se llama DUT,

39
00:04:36,120 --> 00:04:41,840
te iguala 1, pues sería aplicar el tratamiento, te iguala 0, sería no aplicarlo. Entonces,

40
00:04:41,840 --> 00:04:48,320
centriamos toda esta población con la que sí que podemos comparar. Y en base a esto,

41
00:04:48,320 --> 00:04:54,560
el efecto esperado medio sería, vale, yo tengo mi población original, me he simulado mis dos

42
00:04:54,560 --> 00:05:01,200
poblaciones, la que sería con todo el tratamiento, la que sería si todo fuera control y me calculo

43
00:05:01,200 --> 00:05:08,720
la esperanza que tengo en el grupo de tratamiento frente al otro. Con un poco de suerte, si

44
00:05:08,720 --> 00:05:12,560
la diferencia es positiva, quiere decir que mi tratamiento está funcionando, si la esperanza

45
00:05:12,560 --> 00:05:18,160
es negativa, quiere decir no solo que no esté funcionando, sino que es perjudicial. Y aquí

46
00:05:18,160 --> 00:05:25,120
tenéis la expresión matemática para los que les guste. ¿Cuál es el problema de la causalidad?

47
00:05:25,120 --> 00:05:30,880
El principal problema de la causalidad es que hay un montón de cosas que tener en cuenta.

48
00:05:30,880 --> 00:05:37,400
Entonces, por conceptualizarlo de alguna forma, lo que hacemos es dibujarlo en grafos. Los

49
00:05:37,400 --> 00:05:43,720
grafos lo que dicen son los diferentes elementos y cómo interactúan los unos con los otros.

50
00:05:43,720 --> 00:05:49,400
Entonces, aquí vamos a tener un problema que vamos a decir que es cómo el hecho de yo estudiar

51
00:05:49,400 --> 00:05:56,240
en la universidad afecta a mi probabilidad de tener un puesto en el que cobre más. Por

52
00:05:56,240 --> 00:06:00,560
simplicidad, esto va a ser todo ficticio o he estudiado o no he estudiado o tengo un

53
00:06:00,560 --> 00:06:07,560
trabajo en el que me paga mucho o me paga un poco y le metemos otra variable que es la

54
00:06:07,560 --> 00:06:13,040
capacidad adquisitiva de los padres si es alta o baja. Entonces, lo que podemos ver es que

55
00:06:13,040 --> 00:06:18,400
el hecho de estudiar en la universidad claramente afecta a él que tenga un trabajo mejor o peor,

56
00:06:18,400 --> 00:06:23,920
pero también está el otro factor de si mis padres tienen un mayor poder adquisitivo,

57
00:06:23,920 --> 00:06:30,040
es más probable que yo vaya a la universidad. Pero también podría ser que como estoy mejor

58
00:06:30,040 --> 00:06:35,600
conectado porque tengo sus contactos, porque mis padres tienen un mayor poder adquisitivo,

59
00:06:35,600 --> 00:06:42,000
yo voy a acabar cobrando más. Entonces, ¿cuánto del efecto realmente de estudiar en la universidad

60
00:06:42,000 --> 00:06:48,360
me lleva aquí y cuánto es debido a que mis padres tenían un mayor o menor poder adquisitivo?

61
00:06:48,360 --> 00:06:53,200
Aquí es donde lo vamos a mirar con la causalidad. Entonces, es muy importante distinguir este

62
00:06:53,200 --> 00:07:00,720
efecto causal que tenemos del otro efecto que nos viene por esta variable que se llaman confounders,

63
00:07:00,720 --> 00:07:08,120
que es debido a una asociación, simplemente entre las variables. Entonces, planteamos el problema,

64
00:07:08,120 --> 00:07:14,200
habíamos dicho, todo datos simulados, esto me lo inventa yo, y podemos decir, ¿vale? Pues,

65
00:07:14,200 --> 00:07:19,720
vamos a medir para cada, para la gente que no había estudiado en la universidad, ¿cuántos de

66
00:07:19,720 --> 00:07:29,200
ellos acaban con un trabajo bien pagado? Y vemos que es el 18%. Y ahora hacemos lo mismo, pero para

67
00:07:29,200 --> 00:07:35,000
los que sí que habían estudiado en la universidad. Y vemos que se dispara al 77. Perfecto. Pues,

68
00:07:35,000 --> 00:07:43,480
el efecto entonces es, si yo estudio en la universidad, voy a tener una probabilidad del 59% más de tener

69
00:07:43,480 --> 00:07:50,080
un empleo bien pagado. Y si nos quedamos aquí, realmente esto es falso. ¿Cómo podemos hacerlo?

70
00:07:50,080 --> 00:07:57,720
Pues, realmente, lo que tenemos es, la variable que nos está metiendo ruido es lo que cobran los

71
00:07:57,720 --> 00:08:03,920
padres. Entonces, si vamos mirando, si aislamos cada uno de los grupos, el que cobra más y el

72
00:08:03,920 --> 00:08:09,600
que cobra menos, y los miramos de manera separada, ahí ya sí que el efecto es más puro. Entonces,

73
00:08:09,600 --> 00:08:14,960
en este caso, tenemos el grupo de la izquierda en el que los padres no cobran mucho, el grupo de la

74
00:08:14,960 --> 00:08:19,640
derecha en el que sí que cobran, y hacemos el mismo estudio. Miramos cuál es el porcentaje de gente

75
00:08:19,640 --> 00:08:26,040
que tiene un empleo bien pagado habiendo estudiado y sin haber estudiado en la universidad. Y vemos

76
00:08:26,040 --> 00:08:32,240
que aquí, en esta subpoblación, nos aumenta un 48 y en esta otra subpoblación nos aumenta un 52.

77
00:08:33,520 --> 00:08:39,400
Ahora, para calcular el efecto medio, simplemente miramos cómo de grande es esta población con

78
00:08:39,400 --> 00:08:44,920
respecto al total y cómo de grande es la otra con respecto al total. Hacemos una media ponderada y,

79
00:08:44,920 --> 00:08:51,880
al final, obtenemos un valor del 49. Este 49, si lo comparamos con el 59, ya ha bajado un poco,

80
00:08:51,880 --> 00:09:02,360
ya le hemos quitado el ruido debido a lo que cobran los padres. Entonces, esto de aquí nos permite

81
00:09:02,360 --> 00:09:10,520
ver dos cosas. Uno, que el efecto medio es 49 y luego otra cosa es, pero para mi caso concreto,

82
00:09:10,520 --> 00:09:14,320
en el que sé que mis padres tienen un poder adquisitivo alto o sé que tienen un poder adquisitivo bajo,

83
00:09:14,320 --> 00:09:21,840
ya sé que mi probabilidad de aumentar en 52 o en un 48. Limitación es que tiene esto.

84
00:09:24,840 --> 00:09:29,280
¿Qué es lo que hemos hecho aquí? Tenemos nuestra población total basada en un montón de personas

85
00:09:29,280 --> 00:09:36,840
y hemos roto el espacio en dos partes. Dentro de estas partes, como ya son puras,

86
00:09:36,840 --> 00:09:41,640
ya no tenemos confounders de por medio, así que podemos decir, vale, pues me calculo,

87
00:09:41,640 --> 00:09:49,200
la probabilidad para el tratamiento, la probabilidad para el grupo de control y el arresto. La limitación

88
00:09:49,200 --> 00:09:53,480
que tiene esto es que nuestro ejemplo era muy sencillo, pero y si ahora nos cogemos otro,

89
00:09:53,480 --> 00:09:58,560
en el que las variables hay muchísimas más que entran en juego, muchas más que afectan,

90
00:09:58,560 --> 00:10:05,760
y acabamos con nuestro espacio que esta primera solo podía tener dos valores, pero ahora metemos

91
00:10:05,760 --> 00:10:10,320
otra que puede tener tres y ahora metemos otra que lo dividen dos y ahora otra que lo dividen dos.

92
00:10:10,320 --> 00:10:16,200
Acabamos teniendo una población superfragmentada de manera que cualquier estudio que hagamos

93
00:10:16,200 --> 00:10:21,440
estadístico cada vez tiene menos significancia. Es posible que al final el número que acabamos

94
00:10:21,440 --> 00:10:29,600
dando sea fruto del ruido. ¿Cómo podemos solucionar esto? Al final es un tema de aquí

95
00:10:29,600 --> 00:10:34,720
habíamos ido cogiendo subpoblaciones cada vez más pequeñas y sí, intentamos hacernos una

96
00:10:34,720 --> 00:10:41,800
población simulada, pero que lo abarque todo y es el concepto de las pseudo poblaciones. En las

97
00:10:41,800 --> 00:10:47,600
pseudo poblaciones nosotros tenemos aquí nuestra población total y la separamos en dos, la que ha

98
00:10:47,600 --> 00:10:53,800
recibido el tratamiento y la que no. Y ahora lo que queremos es simular esta población total y lo

99
00:10:53,800 --> 00:11:02,200
vamos a hacer en base a cómo de probable, o sea, cuántas muestras tengo alrededor de mi grupo y cuántas

100
00:11:02,200 --> 00:11:08,960
no, más o menos. Entonces, si tengo una probabilidad muy baja de pertenecer a mi grupo, quiere decir

101
00:11:08,960 --> 00:11:16,360
que si doy mucho más peso a esta muestra, voy a cubrir más espacio, por decirlo de alguna forma,

102
00:11:16,360 --> 00:11:22,960
si a mi alrededor tengo 10 muestras del otro grupo, si me doy un peso 10 veces superior,

103
00:11:22,960 --> 00:11:28,440
voy a estar supliendo todas estas muestras que tengo a mi alrededor. Entonces, como hago esto,

104
00:11:28,440 --> 00:11:35,760
en base a la probabilidad de tener a gente de mi grupo cerca y, represando las muestras,

105
00:11:35,760 --> 00:11:40,480
vemos que conseguimos un espacio un poco más completo y si hacemos lo mismo con el otro,

106
00:11:40,480 --> 00:11:44,760
ya tenemos el espacio para el grupo de control también completo. Y ahora con estas pseudo

107
00:11:44,760 --> 00:11:53,840
poblaciones sí que podemos hacer esa comparación. Aquí entramos ya en el mundo machine learning,

108
00:11:53,840 --> 00:12:00,760
espero que nos asuste lo que estamos haciendo es esto mismo de calcular la probabilidad del tratamiento

109
00:12:00,760 --> 00:12:07,200
esta vez, de pertenecer al grupo de tratamiento en base a todas estas variables que nos estaban

110
00:12:07,200 --> 00:12:14,240
metiendo en ruido. En nuestro caso concreto, simplemente la variable era el nivel adquisitivo

111
00:12:14,240 --> 00:12:20,400
de los padres y el tratamiento es estudio o no estudio en la universidad. Entrenamos un modelo

112
00:12:20,400 --> 00:12:27,920
y sacamos la probabilidad de pertenecer al grupo de tratamiento que es lo que se llama el propensity

113
00:12:27,920 --> 00:12:34,920
score. Y esta probabilidad luego lo que tenemos que hacer es pesar las muestras de tratamiento

114
00:12:34,920 --> 00:12:40,520
dividiendo por la probabilidad de manera que si mi probabilidad de pertenecer al tratamiento era del

115
00:12:40,520 --> 00:12:46,320
10 por 100 voy a multiplicarla por 10 porque estoy dividiendo por 0 a 1, entonces lo de un mucho más

116
00:12:46,320 --> 00:12:53,120
peso esa muestra que era muy representativa del otro grupo. Entonces para el grupo de tratamiento lo

117
00:12:53,120 --> 00:12:58,320
dividimos por eso y para el de control lo hacemos dividido por la probabilidad de pertenecer al

118
00:12:58,320 --> 00:13:05,600
grupo de control que sería 1 menos la probabilidad anterior. Y en base a esto ya tenemos cada una

119
00:13:05,600 --> 00:13:12,680
de las muestras el peso que tiene y podemos calcular los positivos. Yo igual como era una muestra muy rara

120
00:13:12,680 --> 00:13:20,120
cuento como 10 personas. Si yo he convertido voy a contar también como 10 conversiones. Si he sobrevivido

121
00:13:20,120 --> 00:13:27,560
cuento como 10 supervivientes, si he muerto pues cuento como 0 supervivientes. Y en base a esto ahora

122
00:13:27,560 --> 00:13:35,880
tenemos las poblaciones originales y cuando pesamos las muestras vemos que la población total ya se nos

123
00:13:35,880 --> 00:13:42,560
queda igual y esta población total pesando los positivos vemos que nos queda aquí un 21 por 100 y

124
00:13:42,560 --> 00:13:50,360
un 70 por 100 y casualidad o magia si restamos uno menos el otro vemos que nos sale este 49 por 100

125
00:13:50,360 --> 00:14:00,560
de diferencia. Entonces ¿qué herramientas tenemos? Si vosotros como yo sois un poco vagos y no queréis

126
00:14:00,560 --> 00:14:07,920
hacer tanto cálculo. Hay herramientas open source como por ejemplo estaría DUI que es una librería

127
00:14:07,920 --> 00:14:16,080
de Microsoft que también está haciendo con colaboración con Amazon y lo que nos permite es este

128
00:14:16,080 --> 00:14:21,560
modelito que teníamos antes, este grafo lo podemos hacer con código diciendo vale pues de aquí

129
00:14:21,560 --> 00:14:27,240
voy aquí de esta otra voy a esta otra y de la última voy a esta me he definido todas las

130
00:14:27,240 --> 00:14:33,240
relaciones y me creé un modelo causal en el que identifico cuál es el tratamiento y cuál es el

131
00:14:33,240 --> 00:14:40,160
efecto cuál es la variable de outcome. Esto de aquí ahora digo haz tus cálculos identificáme qué

132
00:14:40,160 --> 00:14:46,040
operaciones tienes que hacer para resolverme el problema y me lo resuelves y ya directamente me

133
00:14:46,040 --> 00:14:54,520
da el 49 pero habiendo tardado tres minutos en vez de media hora. Aquí dentro en la parte de

134
00:14:54,520 --> 00:15:00,080
calcularme el valor que tiene le podemos meter varios modelos que tenga DUI dentro pero también

135
00:15:00,080 --> 00:15:06,800
hay varias librerías como causa lml y econom l que tienen una una ristra de algoritmos que se

136
00:15:06,800 --> 00:15:14,600
pueden utilizar para esto mismo. Ahora bien tenemos el ejemplo de tres elementos y lo queremos llevar

137
00:15:14,600 --> 00:15:19,560
al siguiente nivel que es lo que pasa si nuestro modelo es mucho más complejo si es mucho más

138
00:15:19,560 --> 00:15:26,120
complejo y tenemos este grafo aquí cosas que se relacionan con otras podemos intentar simplificarlo

139
00:15:26,120 --> 00:15:33,320
mirando localmente cómo se comportan las cosas pues localmente podría ser cómo puedo conseguir el

140
00:15:33,320 --> 00:15:39,440
precio en base a las variables que me dependen de esta como puedo conseguir esta pues depende de

141
00:15:39,440 --> 00:15:46,200
esta y me voy desarrollando modelos pequeños que dependen de las variables para al final ir

142
00:15:46,200 --> 00:15:52,760
haciendo una cascada de efectos hasta llegar al valor final. Entonces esto primero definimos el

143
00:15:52,760 --> 00:16:01,880
grafo que sería con network x por ejemplo y luego vamos definiendo cada uno de los mecanismos causales

144
00:16:01,880 --> 00:16:06,720
para cada uno de los nodos que pueden ser pues para variables binarias le metemos un clasificador

145
00:16:06,720 --> 00:16:12,600
si no le podemos meter un regresor o para las variables del origen que no es que se puedan

146
00:16:12,600 --> 00:16:18,880
parecer con algo damos una distribución de los valores que esperamos que tenga y lo puede

147
00:16:18,880 --> 00:16:24,720
ser la distribución empírica. Entonces una vez que tenemos este modelo le damos al fit y ya

148
00:16:24,720 --> 00:16:31,920
internamente se encarga de entrenar e ir propagando todos estos cambios. Vale y esto muy bien ahora

149
00:16:31,920 --> 00:16:37,240
para que nos sirve nos sirve porque ya tenemos establecida la dinámica con la que todas las

150
00:16:37,240 --> 00:16:43,800
variables interactúan y esta dinámica nos permite decir vale pues si ahora yo subir al precio bajar

151
00:16:43,800 --> 00:16:49,480
el precio pues si era el precio que me apetezca cómo afectaría todo mi sistema y esto de aquí

152
00:16:49,480 --> 00:16:57,960
cuando llamo a generar muestras en base a una intervención me permite ver cuál sería mi

153
00:16:57,960 --> 00:17:03,640
dataset después de haber hecho todos estos cambios me simula el cambio y todo lo que se propaga

154
00:17:03,640 --> 00:17:11,320
hacia abajo. Dentro de causa de la ML tiene una ristra de algoritmos muy grandes que si

155
00:17:11,320 --> 00:17:16,640
metalernes que si basados en árboles que si intervenciones para propensitis cost también

156
00:17:16,640 --> 00:17:22,360
ofrece bastantes cosas y luego metéricas para que podamos evaluarlo. No me voy a meter dentro de

157
00:17:22,360 --> 00:17:27,160
esto al final no es que haya un método que sea mejor que otro es como ese celerno no hay un

158
00:17:27,160 --> 00:17:32,120
algoritmo mejor que otro es dependiendo del problema ahora que usar uno ahora que usar otro y os

159
00:17:32,120 --> 00:17:39,760
animo a probarlo. Lo que sí que tiene bastante interesante causa la ML es una parte para ver

160
00:17:39,760 --> 00:17:48,000
lo que sería el Aplis modeling. El Aplis modeling es que yo puedo tener un modelo que me dice vale

161
00:17:48,000 --> 00:17:55,240
el tratamiento va a mejorar un 5% la probabilidad de que la persona sobreviva o vamos a ponerlo

162
00:17:55,240 --> 00:18:00,880
con un ejemplo más de telés un 5% más de que la persona convierta y esto es perfecto pero igual

163
00:18:00,880 --> 00:18:07,200
el tratamiento es ofrecer un 20% de descuento. Entonces si estoy aumentando la probabilidad

164
00:18:07,200 --> 00:18:12,320
de conversión pero luego el revenue que tengo es menor igual no me merece la pena igual si el

165
00:18:12,320 --> 00:18:17,440
usuario ya iba a convertir me merece más la pena no mostrarle el descuento porque si no perdería

166
00:18:17,440 --> 00:18:23,440
dinero. Entonces el Aplis modeling lo que hace es muestra muestra va viendo cuál es el tratamiento

167
00:18:23,440 --> 00:18:31,680
que me daría más beneficios. Pongo un ejemplo de código en particular causa la ML que lo que

168
00:18:31,680 --> 00:18:36,920
me está dando es cuál va a ser el Aplift para cada uno de los tratamientos potencialmente más de

169
00:18:36,920 --> 00:18:43,440
uno y en base a este Aplift o sea que para la muestra uno el tratamiento uno sube un 20%

170
00:18:43,440 --> 00:18:51,640
pero el tratamiento bajo un 5. Para la muestra dos lo que sea en base a todo esto definen una

171
00:18:51,640 --> 00:18:57,800
función que al final es un helper no tiene nada de inteligencia artificial dentro que es para cada

172
00:18:57,800 --> 00:19:05,000
muestra miro la probabilidad de conversión que tenía a priori luego si le tengo que sumar algo

173
00:19:05,000 --> 00:19:10,160
porque aplico el tratamiento uno si le tengo que sumar algo por el tratamiento dos y luego los

174
00:19:10,160 --> 00:19:15,640
costes asociados a cada tratamiento que los costes pueden ser lo que decía de aplicar el

175
00:19:15,640 --> 00:19:23,200
tratamiento este 20% o puede ser que si le quiero enviar el código de descuento se lo envío por

176
00:19:23,200 --> 00:19:30,240
sms o a través de un tercero de manera que tengo que pagar sólo por enseñárselo entonces esto es

177
00:19:30,240 --> 00:19:38,120
lo que se llaman los costes de intervención y que ese me lo va a descontar aplique o no aplique

178
00:19:38,120 --> 00:19:44,960
el usuario el descuento entonces en base a todo esto lo que nos deja es vale pues ya se los costes

179
00:19:44,960 --> 00:19:50,880
que tiene cada uno la probabilidad que tendría y te digo la mejor alternativa para este usuario es el

180
00:19:50,880 --> 00:19:59,360
tratamiento uno el tratamiento dos o déjalo tranquilo no le enseñes nada y ahora llega la

181
00:19:59,360 --> 00:20:04,040
pregunta del millón que es todo esto es muy bonito pero yo le da una máquina me dice dale

182
00:20:04,040 --> 00:20:11,800
el tratamiento dos me lo creo no me lo creo el problema que tenemos es que en general no se

183
00:20:11,800 --> 00:20:17,760
puede demostrar que sean correctos simplemente porque puedo tener variables que no he tenido en cuenta

184
00:20:17,760 --> 00:20:22,360
mi sistema o que sé que afectan pero que no las he podido medir no las tengo entre mis datos

185
00:20:23,360 --> 00:20:29,200
entonces como puedo tener ruido por ahí lo único que puedo hacer es hacer test para ver si está

186
00:20:29,200 --> 00:20:35,640
mal pero sin ningún test me falla no sigo sin saber si está bien al menos sé que es un poco más

187
00:20:35,640 --> 00:20:42,240
robusto lo que me dice entonces entre los test podría ser el tratamiento placebo que es ya tengo

188
00:20:42,240 --> 00:20:48,600
mis datos y le cambia el tratamiento aleatoriamente ahí ya deja de haber una relación en principio

189
00:20:48,600 --> 00:20:54,080
entre el tratamiento real entre lo que sería un uno de tomado el medicamento y lo que sería

190
00:20:54,080 --> 00:20:58,880
el resultado de sobreviva o no porque puede ser uno puede ser cero le da igual y lo que me debería

191
00:20:58,880 --> 00:21:03,800
decir mis sistemas que el efecto medio es cero si me dice que es distinto de cero entonces ya

192
00:21:03,800 --> 00:21:09,560
sí que tengo otros problemas el modelo en sí no es válido lo mismo pasa si en vez de probar el

193
00:21:09,560 --> 00:21:17,040
tratamiento le cambio el outcome si le cambio el outcome me debería decir que es cero luego hay otra

194
00:21:17,040 --> 00:21:23,120
parte que es para ver lo lo sólido que es que es si en vez de utilizar el 100 por 100 de los datos

195
00:21:23,120 --> 00:21:30,920
me cojo un 70 un 80 por 100 y lo repito muchas veces me está variando mucho no si no me está

196
00:21:30,920 --> 00:21:36,720
variando mucho pues al menos sé el número me lo puedo creer si me varía un 30 por 100 tengo que

197
00:21:36,720 --> 00:21:43,680
ir con más cuidado y luego hay una parte que es el de añadir con founders puedo meter una variable

198
00:21:43,680 --> 00:21:50,080
que yo artificialmente hago que modifique el tratamiento y el outcome de manera que yo sí que

199
00:21:50,080 --> 00:21:58,000
sé lo que estoy añadiendo y si me modifica cuál era el efecto causal quiere decir que mi

200
00:21:58,000 --> 00:22:03,120
algoritmos está adaptando mucho a los datos y no me está mostrando realmente el efecto causal

201
00:22:03,120 --> 00:22:09,440
sino que puede ser debido a ruido y he puesto un ejemplo de con du ay cómo se podría hacer que

202
00:22:09,440 --> 00:22:16,000
se le puede poner el método que sea tiene una lista de métodos y al menos para ver si es robusto

203
00:22:18,400 --> 00:22:26,800
y ahora ya la última parte que sería monitorizándolo después en la vida real de no se es útil o no

204
00:22:26,800 --> 00:22:35,320
no es es útil esto de aquí tienes idealmente siempre la comparación que queremos es tener un grupo

205
00:22:35,320 --> 00:22:40,920
de control con el que poder decir vale pues estar de control aquí el tratamiento y la comparación

206
00:22:40,920 --> 00:22:46,880
siempre nos va a ser un poco más fácil pero si esto no puede ser porque no lo tenemos o aparte

207
00:22:46,880 --> 00:22:52,720
porque nos puede interesar tener siempre control podría ser vale de la gente a la que le he dado

208
00:22:52,720 --> 00:22:59,360
el tratamiento ahora yo pruebo diferentes modelos y veo que modelos me recomendaban para esos

209
00:22:59,360 --> 00:23:04,120
usuarios que modelos me recomiendan el tratamiento y que modelos no me lo recomendaban y puedo ver

210
00:23:04,120 --> 00:23:10,880
si para los que me lo recomendaban el efecto medio ha sido más positivo que para los otros igual

211
00:23:10,880 --> 00:23:15,960
que puedo hacer esto puedo meterme mucho más en detalle y ordenar a los usuarios ir por cuantiles

212
00:23:15,960 --> 00:23:22,280
y voy viendo el efecto que he obtenido en base a los cuantiles que es lo que pasa que si llevo esto a

213
00:23:22,280 --> 00:23:29,120
cada muestra individualmente es posible que no tenga significancia estadística y eso es algo que en

214
00:23:29,120 --> 00:23:35,760
general tenemos que perseguir siempre para podernos creer lo que nos está diciendo y eso sería todo

215
00:23:35,760 --> 00:23:48,120
alguna pregunta algún alguien

216
00:23:48,120 --> 00:24:09,520
buenas si que es cierto que hay siempre como cierto meme con la gente de data

217
00:24:10,520 --> 00:24:17,640
sobre todo con esta revolución que había en los últimos 10 años que intentan solucionar no

218
00:24:17,640 --> 00:24:24,200
siempre es verdad que intentan solucionar los problemas a base de char cargas y cargas de datos

219
00:24:24,200 --> 00:24:29,920
y ampliar todo lo que echan a lo que modelan para intentar resolver algo de una manera que no

220
00:24:29,920 --> 00:24:37,680
entienden muy bien utilizando este tipo de sistemas ves o notas ventajas a la hora de reducir la cantidad

221
00:24:37,680 --> 00:24:45,680
de datos o lo que echas a ciegas dentro de tu modelo normalmente depende de lo que estés buscando si

222
00:24:45,680 --> 00:24:51,800
lo que estás buscando es algo que te previa perdón si lo que estás buscando es algo que te

223
00:24:51,800 --> 00:24:59,200
prediga el outcome en base a las variables lo mejor normalmente es darle todas las variables

224
00:24:59,200 --> 00:25:05,880
porque aunque sean relaciones causales te las está utilizando para predecir entonces en ese caso

225
00:25:05,880 --> 00:25:11,280
no verías tanto el efecto que es lo que pasa si haces esto si vas modulando las diferentes partes

226
00:25:11,280 --> 00:25:19,480
por separado te permite tanto intervenir como estar quizá más adaptado a posibles cambios de dominio

227
00:25:19,480 --> 00:25:25,720
porque aquí sí que los módulos entienden los cambios en local y te sirve también para tema de

228
00:25:25,720 --> 00:25:31,800
explicabilidad no es lo mismo si me he entrenado un modelo con 100 variables que si tengo modelos

229
00:25:31,800 --> 00:25:37,320
locales entrenados con 5 que localmente puedo explicar mucho mejor no sé si he respondido

230
00:25:37,320 --> 00:25:42,560
tu pregunta si porque no sé la escalabilidad imagino que las calabilidades o segun vas agregando

231
00:25:42,560 --> 00:25:49,560
variables se irá resintiendo bastante no perdón rapide que no sé si imagino que además al hacerlo

232
00:25:49,560 --> 00:25:58,280
así la escalabilidad según vas agregando variables será será más complicado a relacionar no tienes

233
00:25:58,280 --> 00:26:03,480
el problema de que igual acabas con muchos modelos pero también son mucho más modulares pues trabajar

234
00:26:03,480 --> 00:26:08,560
en unos modelos muy concretos y mejorar la performance de esos ahí es lado del resto

235
00:26:14,080 --> 00:26:17,400
alguna pregunta más tengo la luz aquí no veo

236
00:26:21,920 --> 00:26:30,120
yo tenía una pregunta qué recomendaciones das para construir los modelos porque creo que

237
00:26:30,120 --> 00:26:39,600
me entiendo que se necesita se necesita entender mucho del dominio de problema para

238
00:26:39,600 --> 00:26:43,560
poder construir un modelo y depende de tu modelo eso

239
00:26:45,760 --> 00:26:52,400
si un buen modelo va a andar bien sino no para la causalidad en general es necesario bastante

240
00:26:52,400 --> 00:26:57,680
conocimiento del del industria en parte porque no he explicado cómo se construyen los grafos

241
00:26:57,680 --> 00:27:02,120
pero los grafos vienen de alguna parte hay herramientas que te permiten hacerlo pero tampoco

242
00:27:02,120 --> 00:27:08,400
son que le doy los datos y ya me devuelve el grafo necesito a alguien que tenga el conocimiento de

243
00:27:08,400 --> 00:27:15,720
cómo es el negocio para poder establecer todas estas relaciones igual que establezco las relaciones

244
00:27:15,720 --> 00:27:22,160
igual también hay cierto tipo de modelo porque la variable al final es una función conocida si

245
00:27:22,160 --> 00:27:27,480
es una función conocida no necesito meterle un modelo que me ocurre yo si la función está esta

246
00:27:27,480 --> 00:27:33,800
entonces en ese caso siempre es importante tener conocimiento del negocio pero en causalidad diría

247
00:27:33,800 --> 00:27:40,280
que es todavía más importante y se puede iterar o sea por ejemplo empezar con un modelo y

248
00:27:40,280 --> 00:27:46,680
decir bueno no me da buenos resultados empezar a agregar variables como haces para comparar

249
00:27:46,680 --> 00:27:54,160
si te está yendo mejor o peor en tu modelo se puede iterar como con todos los modelos realmente

250
00:27:54,160 --> 00:28:00,880
tú puedes establecer una métrica objetivo que digas a partir de esta mejora ya me merece

251
00:28:00,880 --> 00:28:05,720
la pena tenerlo porque si no es un sistema más que tengo que tener en producción lo que sea y

252
00:28:05,720 --> 00:28:12,200
una vez que ha superado ese tres por día lo tienes ya puedes ir mejorando probando diferentes

253
00:28:12,200 --> 00:28:19,840
algoritmos y en base a métricas de ganancia o de lo que sea tienes ves si mejoras o sin peoras y

254
00:28:19,840 --> 00:28:24,920
en caso de que mejores pues sigas construyendo en esa dirección gracias

255
00:28:34,880 --> 00:28:40,480
gracias muchas gracias por la charla muy interesante y quería preguntarte si recomendabas

256
00:28:40,480 --> 00:28:47,600
algún dataset de ejemplo que haya en internet que podamos usar para probar el modelo que ya

257
00:28:47,600 --> 00:28:55,880
tenga hecho como por decirlo así ese grafo y hasta hecho y entonces hay varios papers los

258
00:28:55,880 --> 00:29:04,120
puedo mandar por por discord también hay pues sitios en los que tienen tanto los datasets lo

259
00:29:04,120 --> 00:29:09,280
bueno que tiene causa la mle también es que tiene una funcionalidad como para poderte

260
00:29:09,280 --> 00:29:15,120
desarrollar tu dataset aunque sean ficticios y te pone la variable esta variable es informativa está

261
00:29:15,120 --> 00:29:20,880
solo está metiendo ruido y te permite jugar un poco con ellos también vale vale perfecto

262
00:29:33,800 --> 00:29:39,040
nadie va a preguntar nada no cobro por ellas pero bueno si queréis cobro tampoco es problema

263
00:29:39,040 --> 00:29:44,800
bueno pues ya un último aplauso nos despedimos y vamos al cofre

