1
00:00:00,000 --> 00:00:25,520
Hola, bienvenidas a esta sesión. Vamos a hablar con Victor, que nos va a contar que

2
00:00:25,520 --> 00:00:30,520
la inteligencia artificial en medicina no es oro todo lo que reluce.

3
00:00:30,520 --> 00:00:36,000
Muchas gracias. Bueno, lo primero, explicaros por qué llevo esto, porque es un poco raro

4
00:00:36,000 --> 00:00:40,280
ir con una camiseta así. Los que me sigáis en Twitter ya lo sabréis. Los que no, pues

5
00:00:40,280 --> 00:00:47,120
nada, es el equipo de Salamanca. Bueno, pues, relativamente nuevo, solo tiene nueve años,

6
00:00:47,120 --> 00:00:50,380
porque bueno, hace nueve años desapareció la Unión Deportiva de Salamanca, un equipo

7
00:00:50,380 --> 00:00:55,960
comítico de fútbol y unos cuantos locos nos decidimos hacer un equipo a nosotros,

8
00:00:55,960 --> 00:01:00,520
de fútbol popular. Y bueno, lo llevo porque primero ayer ganaron, y en segundo porque

9
00:01:00,520 --> 00:01:06,400
estoy muy orgulloso de equipos así que respetan al rival, que intentan promover que el deporte

10
00:01:06,400 --> 00:01:13,400
sea algo sano para todos, y bueno, porque me gusta. Y ya está. El brevin ciso, si queréis

11
00:01:13,400 --> 00:01:17,120
saber un poco más, buscáis unionistas de Salamanca y veis un poco cómo es, que a mí

12
00:01:17,120 --> 00:01:23,200
me parece que merece la pena de ver fuera. Ya está. Voy a la charla. Estuve pensando

13
00:01:23,200 --> 00:01:28,240
de que hablar un poco, porque tengo muchas cosas que podría contar, proyectos interesantes,

14
00:01:28,240 --> 00:01:31,680
que podíamos hablar de cosas técnicas, pero me pareció interesante abrir un poco el melón

15
00:01:31,680 --> 00:01:38,200
de que la inteligencia artificial en medicina, aunque parece muy bonito, pues no es oro todo

16
00:01:38,200 --> 00:01:43,320
lo que parece. Y nada, bueno, no quiero hablar mucho de mí, porque no quiero meterme en

17
00:01:43,320 --> 00:01:47,000
una presentación en la cual estoy hablando todo el rato de mí, solo deciros, porque

18
00:01:47,000 --> 00:01:50,920
vais a ver el símbolo de Philips ahí todo el rato a la derecha arriba, que trabajo para

19
00:01:50,920 --> 00:01:58,720
Philips, soy clínica scientist en el equipo de research en España, en Western Europe,

20
00:01:58,720 --> 00:02:03,240
hacemos trabajos desde Alemania hasta Portugal, todo ese área. Todos los países que veis

21
00:02:03,240 --> 00:02:07,280
que van de Alemania a Portugal, bueno, pues trabajamos un poco con hospitales, hacemos

22
00:02:07,280 --> 00:02:14,240
un poco de investigación y desarrollo. Y bueno, hasta hace muy poco vivían Salamanca,

23
00:02:14,240 --> 00:02:20,600
yo soy de Salamanca, ahora vivo aquí en Granada, y bueno, pues me organizaba un poco las charlillas

24
00:02:20,600 --> 00:02:23,480
de paydata Salamanca, etcétera, etcétera. Y bueno, ahí tenéis las redes sociales,

25
00:02:23,480 --> 00:02:28,280
si me queréis seguir, si no, pues igual, y correos por si me queréis contactar. Voy a

26
00:02:28,280 --> 00:02:35,560
lo bueno. Hace, pues, cuatro años llevaba justo un año trabajando en este trabajo y

27
00:02:35,560 --> 00:02:41,760
decidí echar una charla en la Picon de Málaga para hablar un poco de qué se hacía en Machine

28
00:02:41,760 --> 00:02:46,520
Learning aplicado a cardiología, porque bueno, pues mi primer proyecto grande con hospital

29
00:02:46,520 --> 00:02:51,000
fue allí en el servicio de cardiología, que tienen todo con Philips, y bueno, empezamos

30
00:02:51,000 --> 00:02:56,160
a trabajar un poco en distintos proyectos. Ahí sí que bueno, pues mi yo naif pensaba

31
00:02:56,160 --> 00:03:01,200
que todo era bonito, hablé mucho de proyectos muy chulos, de cómo desarrollarlos, tal, y

32
00:03:01,200 --> 00:03:07,360
mi yo cuatro años después, pues quiero contar un poco la realidad de lo que ha pasado, ¿no?

33
00:03:07,360 --> 00:03:11,920
en este momento. Si queréis, la charla está colgada y podéis verla. No hace falta para

34
00:03:11,920 --> 00:03:17,720
entenderla que voy a dar ahora. Una cosa que seguramente os ha pasado a muchos, no solo

35
00:03:17,720 --> 00:03:22,160
en medicina, es que veis este tipo de titulares y os quedáis un poco, pues no sé, para los

36
00:03:22,160 --> 00:03:27,480
que sabemos un poco del tema, suenan como a ciencia ficción y bueno, a mí me pone

37
00:03:27,480 --> 00:03:32,000
un poco de los nervios que pongamos titulares como una inteligencia artificial predice los

38
00:03:32,000 --> 00:03:36,880
infartos mejor que un doctor. ¿Qué pasa? Que una inteligencia artificial se ha levantado

39
00:03:36,880 --> 00:03:40,120
por la mañana, se ha desañado unos churros y ha dicho, bueno, venga, vamos a predecir,

40
00:03:40,120 --> 00:03:44,280
a ver, ¿a quién le va a dar un infarto? No, dejemos de hacer estos titulares porque

41
00:03:44,280 --> 00:03:48,980
estamos confundiendo a la gente. La inteligencia artificial la hacen gente por detrás, que

42
00:03:48,980 --> 00:03:54,720
desarrollan aplicaciones y es pues un equipo el que hace este tipo de cosas. También es

43
00:03:54,720 --> 00:03:59,280
un poco curioso que cuando vas, por ejemplo, al segundo, lo de una plataforma de inteligencia

44
00:03:59,280 --> 00:04:03,440
artificial detecta cánceres de pranques al 90%, pues claro, yo entiendo que este tipo

45
00:04:03,440 --> 00:04:07,800
de titulares va muy bien para clickbait, va muy bien para, pues yo que sé, llamar la

46
00:04:07,800 --> 00:04:12,160
atención y que entres y mires. Luego, cuando te vas a leer el paper de verdad, te vas a

47
00:04:12,160 --> 00:04:16,080
la parte de limitaciones y te das cuenta que en el artículo periodístico no han dicho

48
00:04:16,080 --> 00:04:21,120
absolutamente nada de eso. Han puesto esto como si fuera ya para mañana, que ya mañana

49
00:04:21,120 --> 00:04:26,880
vamos a tener esta plataforma y la vamos a poder disfrutar como pacientes. No, no es

50
00:04:26,880 --> 00:04:33,480
así. Y bueno, pues la parte del COVID, voy a hablar un poco más en detalle posteriormente,

51
00:04:33,480 --> 00:04:38,320
pero durante la pandemia, al menos a mí, me llamaba mucho la atención todo este tipo

52
00:04:38,320 --> 00:04:42,160
de artículos como inteligencia artificial para detectar COVID en radiografías, tal, no

53
00:04:42,160 --> 00:04:49,080
sé qué, bla, bla, bla, bla. Ok. Pero esta gente, pues sin tener un reglador muy periodístico,

54
00:04:49,080 --> 00:04:53,840
tampoco ve que muchos de estos artículos no tienen tampoco rigor científico. Es muy

55
00:04:53,840 --> 00:05:01,120
curioso porque, a Ambin Narayan, bueno, no lo pronunciaba bien, pero puso un tweet de

56
00:05:01,120 --> 00:05:06,960
un paper que escribió hace muy poco recientemente sobre una recupilación de revisiones bibliográficas

57
00:05:06,960 --> 00:05:13,280
de papers de inteligencia artificial que ponían un poco los fallos de los propios papers

58
00:05:13,280 --> 00:05:19,920
en metodología. Es muy gracioso porque cuando veis un poco los detalles de los tipos, la

59
00:05:19,920 --> 00:05:25,280
mayor parte de ellos son sobre medicina y vemos fallos tan tontos como decir, vale,

60
00:05:25,280 --> 00:05:29,560
pues para los que sabéis un poco de Machine Learning, sabéis que si vosotros hacéis un

61
00:05:29,560 --> 00:05:34,320
modelo predictivo, tenéis que tener un conjunto de train, o todo de test y si podés tener

62
00:05:34,320 --> 00:05:38,760
una avalización externa, estaría guay. Bueno, pues hay un porcentaje muy grande de publicaciones

63
00:05:38,760 --> 00:05:43,560
científicas que no tienen ni siquiera un test. O sea que directamente ajustan el modelo con

64
00:05:43,560 --> 00:05:49,760
el train, sacan las cosas con el train y te dicen, aquí tengo mi modelo 90%, pero es

65
00:05:49,760 --> 00:05:53,360
que ahí no va solo el problema, sino que es que los revisores dicen, ah, bueno, esto

66
00:05:53,360 --> 00:05:57,560
está bien, esto está muy bien, vamos a ver como Machine Learning y la Inteligencia Artificial

67
00:05:57,560 --> 00:06:04,800
sonan muy bien para nuestra journal, pues venga lo publicamos. Es una vergüenza, es

68
00:06:04,800 --> 00:06:11,200
una vergüenza y esto pasa mucho. Luego, bueno, obviamente también lo típico que es bastante,

69
00:06:11,200 --> 00:06:14,520
eso a lo mejor se podría justificar de otra manera, pero es decir, vale, pues no tenemos

70
00:06:14,520 --> 00:06:19,440
una validación externa, lo suficientemente buena para decir que nuestro modelo generaliza

71
00:06:19,440 --> 00:06:24,320
lo suficientemente bien. Y bueno, ya el Repocopetines, la selección de variables de, pues eso,

72
00:06:24,320 --> 00:06:27,680
hacemos la selección de variables sobre el mismo train y decimos, pues ya está, estas

73
00:06:27,680 --> 00:06:32,760
son las variables que vamos a usar y a veces, bueno, también lo de llamar a una regresión

74
00:06:32,760 --> 00:06:37,560
logística clásica, pues decir, ah, sí, tengo un algoritmo de Machine Learning, bueno, etcétera,

75
00:06:37,560 --> 00:06:42,160
etcétera. Hay otro problema y es un pay-per bastante interesante que escribieron hace,

76
00:06:42,160 --> 00:06:46,200
bueno, hace un par de años sobre cómo construir un equipo de Inteligencia Artificial dentro

77
00:06:46,200 --> 00:06:52,840
de un hospital y hablan precisamente de la desconexión que existe entre los que hacemos

78
00:06:52,840 --> 00:06:57,560
aplicaciones de Inteligencia Artificial con la gente que trabaja en el hospital o cómo

79
00:06:57,560 --> 00:07:01,880
está un poco cada uno en su mundo. Yo llego y digo, bueno, estoy en un centro de investigación

80
00:07:01,880 --> 00:07:07,480
y quiero datos, me contacto con el médico, le digo, quiero datos, vale, dámelos, ok,

81
00:07:07,480 --> 00:07:12,000
bueno, bueno, pero vamos a sacar algún pay-per, sí, sí, claro, dame, dame, hago mi modelo,

82
00:07:12,000 --> 00:07:16,360
lo saco y ahí se quedó todo, no? No hay ningún tipo de aplicabilidad, sí, un pay-per

83
00:07:16,360 --> 00:07:21,400
buenísimo, pero a la hora de la verdad vamos a hacer algo con esta aplicación, o se va

84
00:07:21,400 --> 00:07:28,520
a quedar solo en, no sé, hay un poco ese problema de los algoritmos silos que lo ha hablado

85
00:07:28,520 --> 00:07:32,920
también un poco, que hacemos un algoritmo que va muy bien para nuestro hospital, lo publicamos

86
00:07:32,920 --> 00:07:37,840
en un journal, pero luego en realidad pues no sabemos si puede ser esternarizable a otro

87
00:07:37,840 --> 00:07:42,640
tipo de hospitales, otro tipo de continentes, etcétera, etcétera. La falta de reproducibilidad,

88
00:07:42,640 --> 00:07:48,640
es otro problema, la falta de desarrollo posterior, como digo, los datos, eso es muy gracioso

89
00:07:48,640 --> 00:07:54,280
porque avisamos en Twitter, ha hablado hace muy poco sobre este tema de que la mayor parte

90
00:07:54,280 --> 00:07:58,920
de las cosas que hacemos y me incluyo porque también hago ese tipo de investigaciones son

91
00:07:58,920 --> 00:08:03,640
con datos retrospectivos, es decir, yo no me voy a poner a hacer un estudio clínico y

92
00:08:03,640 --> 00:08:09,480
a intentar ir cogiendo datos en tiempo real o posteriormente porque cuesta mucho tiempo

93
00:08:09,480 --> 00:08:14,520
dinero y desarrollo, pero sin embargo si voy a una base de datos y busco qué enfermedad

94
00:08:14,520 --> 00:08:20,880
tengo estenosis auttica, bueno, tal, pues me cojo todos estos datos y hago una aplicación,

95
00:08:20,880 --> 00:08:26,360
pues es más fácil, es más sencillo, pero claro, son todas redes retrospectivas, deberíamos

96
00:08:26,360 --> 00:08:31,000
de ver a posteriori si esto funciona o no. Y bueno, también la última que es bastante,

97
00:08:31,000 --> 00:08:34,840
yo lo he visto porque he estado dentro de un departamento de cardiología y bueno hay

98
00:08:34,840 --> 00:08:39,000
por aquí alguien que también puede dar fe de ello, que hay una desconheción bastante

99
00:08:39,000 --> 00:08:43,440
grande entre las personas que hacemos cosas técnicas con las personas que realmente están

100
00:08:43,440 --> 00:08:48,920
ahí trabajando con los pacientes todos los días. Estas son un poco las soluciones, ya

101
00:08:48,920 --> 00:08:54,160
las he ido diciendo un poco, pero bueno podéis leerlas un poco más en detalle, pues intentar

102
00:08:54,160 --> 00:08:59,040
hacer datos en tiempo real, evaluación de modelos de una manera más correcta o rigurosa,

103
00:08:59,040 --> 00:09:03,040
validaciones externas, integración de modelos, etcétera, etcétera, ¿no? Y sobre todo,

104
00:09:03,040 --> 00:09:08,640
bueno, el último punto, no me voy a centrar mucho porque ha habido unas charlas fenomenales

105
00:09:08,640 --> 00:09:14,800
sobre sesgos, bueno, hace nada, acabamos de estar hablando de ello, y equidad, transparencia

106
00:09:14,800 --> 00:09:21,440
y uso ético de los datos, ¿no? Y quiero hablar de una cosa que he mencionado antes, pero

107
00:09:21,440 --> 00:09:26,960
a mí me hizo mucha gracia porque también he vivido esto, las prisas por llegar los

108
00:09:26,960 --> 00:09:33,520
primeros, llega el COVID y bueno, hay un común momento de venga, vamos a intentar hacer algo

109
00:09:33,520 --> 00:09:41,440
para intentar ayudar a los médicos, venga, vamos a desde nuestro conocimiento como técnicos

110
00:09:41,440 --> 00:09:45,280
de aplicación de inteligencia artificial, tenemos que hacer algo para ayudarles, vale,

111
00:09:45,280 --> 00:09:50,560
guay, o sea, hasta ahí, todo perfecto, más de 2.000 publicaciones, solo en 2020, esto

112
00:09:50,560 --> 00:09:56,400
es un paper que está escrito sobre, sobre, analizando cada una de ellas, todas centradas

113
00:09:56,400 --> 00:10:04,520
en vamos a predecir COVID, ok, vale, perfecto, ninguna de ellas de las 2.000 es aprovechable,

114
00:10:04,520 --> 00:10:08,760
aprovechable clínicamente, ¿por qué? Por esa desconexión que hay entre médicos y personas

115
00:10:08,760 --> 00:10:16,400
que se ponen a decir que su nuevo modelo tiene un 96% de efectividad, genial, guay, lo sumo

116
00:10:16,400 --> 00:10:22,560
un poquito más, perfecto, pero no está bien, un montón de defectos metodológicos que ahora

117
00:10:22,560 --> 00:10:27,360
comentaremos y sesgos suyacentes. Algunos de los errores comunes de estas publicaciones

118
00:10:27,360 --> 00:10:32,240
que además no eran, eran bastante repetidos, los datos es abiertos, está muy igual para

119
00:10:32,240 --> 00:10:37,360
las competiciones, para hacer benchmarking, para lo que vosotros queráis, es genial, pero

120
00:10:37,360 --> 00:10:42,160
la mayor parte lo que pasa es que al final se sobreajustan a este tipo de modelos, es

121
00:10:42,160 --> 00:10:46,440
tan genial para ellos, pero luego en realidad pues no sirven. Otro de los problemas es el

122
00:10:46,440 --> 00:10:52,840
formato, nosotros en imagen médica si sabéis un poco usamos daikon, no usamos jpeg, jpeg

123
00:10:52,840 --> 00:10:58,000
está muy bien pues para reducir el dataset y que lo podamos tener, pero si tú no usas

124
00:10:58,000 --> 00:11:03,360
el formato que realmente va a tener que aplicar en el hospital la persona de qué te sirve

125
00:11:03,360 --> 00:11:09,000
tener un modelo de jpeg. Otra más, la desconexión total con la práctica

126
00:11:09,000 --> 00:11:12,560
de la clínica, yo lo entiendo que no puede que durante la pandemia no había tiempo para

127
00:11:12,560 --> 00:11:17,800
hablar con doctores, pero es muy gracioso que la mayor parte de los modelos que se hicieron,

128
00:11:17,800 --> 00:11:24,280
se hicieron sobre todo con vistas en rayos x que no usan los médicos, que no van a usar,

129
00:11:24,280 --> 00:11:30,400
entonces de qué te sirve un modelo de una vista que no usa nadie, vale. ¿Cómo se soluciona

130
00:11:30,400 --> 00:11:35,160
esto? Hay un montón de guías de buenas prácticas, de hecho en la charla de NREA del otro día

131
00:11:35,160 --> 00:11:41,160
hablo un poco de ellas, que básicamente son checklists de cosas que deberíamos de hacer

132
00:11:41,160 --> 00:11:47,120
con rigor científico para digamos tener una publicación un poco mejor hecha, pero eso

133
00:11:47,120 --> 00:11:52,960
no es todo porque esto es un meme muy clásico, correlación o es causalidad, tuvimos una

134
00:11:52,960 --> 00:11:59,360
charla muy chula ayer sobre causalidad de hecho, pero que es muy típico, o sea yo veo aquí

135
00:11:59,360 --> 00:12:04,000
que hay COVID, entonces pues ya está, ahí está el COVID y ahí tienes tu modelo, pero

136
00:12:04,000 --> 00:12:10,720
no te digo ni por qué, ni cuándo, ni nada. ¿Cuáles son las soluciones? Hay una charla

137
00:12:10,720 --> 00:12:16,660
justo ahora mismo paralela de vea sobre interpretabilidad y explicabilidad que estaría muy bien que

138
00:12:16,660 --> 00:12:22,880
pudieras asistir, pero obviamente habéis elegido venir a mí, no sé por qué, y luego bueno

139
00:12:22,880 --> 00:12:28,400
los modelos causales de lo que ya se habló el otro día y un poco de incertidumbre, para

140
00:12:28,400 --> 00:12:35,400
mi incertidumbre he hecho esta imagen con Dale, dos, un poco de un robot que dice I

141
00:12:35,400 --> 00:12:40,160
don't know, eso para mí es lo básico, hay modelos de incertidumbre súper complejos,

142
00:12:40,160 --> 00:12:45,760
pero para mí lo básico es que una aplicación diga no sé qué leches hay aquí, ya está,

143
00:12:45,760 --> 00:12:52,560
perdón por las palabras, y luego una pequeña publicidad al Puzcadas de Flussens que tienen

144
00:12:52,560 --> 00:12:59,680
un episodio muy chulo de incertidumbre que aquí el amigo Mike lo comenta y si queréis

145
00:12:59,680 --> 00:13:04,720
aprender un poco más está muy guay, y bueno, por si yo que sé. Hablando de imágenes

146
00:13:04,720 --> 00:13:12,440
ficticias, este investigador Walter Hugo, que es buenísimo en lo suyo, ha utilizado

147
00:13:12,440 --> 00:13:18,840
stable diffusion para crear un dataset de imágenes ficticias, de imágenes de resonancia magnética

148
00:13:18,840 --> 00:13:26,560
del cerebro, está muy bien, pero yo lo que siempre digo con estos nuevos modelos es que

149
00:13:26,560 --> 00:13:32,760
está genial que la gente los haga pero cuidado cuando lo uséis, yo preferiría usar datos

150
00:13:32,760 --> 00:13:39,760
reales, entonces Cousin use at your own risk, tenía que meter un poco esto porque está

151
00:13:39,760 --> 00:13:45,720
de moda la stable diffusion, y nada, ¿cuál es la realidad de, o sea, quitando toda la

152
00:13:45,720 --> 00:13:50,560
parte ahora científica quiero dar un salto un poco a la realidad de lo que realmente

153
00:13:50,560 --> 00:13:57,400
pasa en el día de aclínico? Y VSTM me deja avisar que dice, tengo cientos de ejemplos

154
00:13:57,400 --> 00:14:03,360
de IAS, de inteligencias artificiales que se usan, y le preguntan, pero se usan de verdad,

155
00:14:03,360 --> 00:14:09,960
no son solo papers, bueno, pues la mayor parte son solo papers. Os voy a contar un poco lo

156
00:14:09,960 --> 00:14:14,040
que nos ha pasado nosotros con una aplicación de seguimiento de una patología que se llama

157
00:14:14,040 --> 00:14:19,360
Astenosis Saórtica, de la cual, bueno, pues hablé en su momento en la charla del 2018,

158
00:14:19,360 --> 00:14:24,840
y de hecho la idea la tuvieron al principio para que empezáramos este proyecto en 2017.

159
00:14:24,840 --> 00:14:28,800
En 2018 ya teníamos los primeros resultados un poco bien, conseguimos una validación

160
00:14:28,800 --> 00:14:34,000
externa con otro hospital que sufrimos un poco para conseguirla, pero demostramos que

161
00:14:34,000 --> 00:14:39,840
funcionaba. Philips decidió que valía intentar hacer una patente, la patente se sale en

162
00:14:39,840 --> 00:14:47,280
2020, en 2021 empezamos una validación clínica posterior, en 2022 sacamos un paper, desde

163
00:14:47,280 --> 00:14:54,440
2017 a 2022 han pasado cinco años. Ningún paciente usa esa aplicación todavía, y

164
00:14:54,440 --> 00:14:59,880
lo más gracioso es que a lo mejor nunca la usen. ¿Por qué? Porque se tienen que juntar

165
00:14:59,880 --> 00:15:05,960
dos cosas, interés comercial y validación clínica. Si estas dos cosas se combinan,

166
00:15:05,960 --> 00:15:12,560
es genial, para adelante, pero en el cajón se quedan muchísimas aplicaciones que no

167
00:15:12,560 --> 00:15:18,520
se van a usar nunca y que se tienen patentes. Por eso se nos llena la boca a veces de la

168
00:15:18,520 --> 00:15:24,120
medicina de precisión, se gasta muchísimo dinero en aplicaciones para ello, pero el

169
00:15:24,120 --> 00:15:30,040
problema es que al final todo es un poco tendo solucionismo, es nosotros, los técnicos,

170
00:15:30,040 --> 00:15:36,160
nos venimos a dar una solución, os la lanzamos ahí en la cara y ahí la tenéis, pero la

171
00:15:36,160 --> 00:15:40,920
realidad del entorno clínico es bien distinta, si vosotros os paseáis tiempo con ellos,

172
00:15:40,920 --> 00:15:47,080
os dais cuenta, y estos son dos cómics que he cogido, que la mayor parte del tiempo y

173
00:15:47,080 --> 00:15:51,400
esto es real, los médicos se pasan delante de un ordenador metiendo datos, haciendo

174
00:15:51,400 --> 00:15:56,560
informes y no estando tiempo con el paciente. Y bueno, el de la derecha es muy gracioso,

175
00:15:56,560 --> 00:16:01,320
porque ella dice, bueno, pues yo me metí en medicina por todo el tipo de regulación,

176
00:16:01,320 --> 00:16:04,720
de papeleo que hay que hacer, del tiempo que tengo que estar en el ordenador, eso es,

177
00:16:04,720 --> 00:16:10,160
por eso me metí en medicina. No debería ser así, la medicina debería de elegirla,

178
00:16:10,160 --> 00:16:15,920
un estudiante, y de hecho estamos aquí en el campus, para ser un doctor y pasar tiempo

179
00:16:15,920 --> 00:16:21,520
con el paciente. Os voy a contar un poco un ejemplo de cómo es la unidad de imagen en

180
00:16:21,520 --> 00:16:28,440
cardiología en Salamanca. Este dibujo lo hizo una chica que hizo un proyecto con nosotros

181
00:16:28,440 --> 00:16:33,760
de Sudáfrica, no es mío, por eso pongo la fuente suya, y era un proyecto de hecho que

182
00:16:33,760 --> 00:16:40,840
intentaba hacer una aplicación con la gente de allí, que tenía ciertas necesidades,

183
00:16:40,840 --> 00:16:46,200
y bueno, aquí vemos un poco cómo sería el flujo de un paciente según entra para hacerse

184
00:16:46,200 --> 00:16:49,920
un ecocardiograma. Un ecocardiograma es una prueba de ultrasonidos que se hace directamente

185
00:16:49,920 --> 00:16:55,440
al corazón, y que es bastante sencilla y que dura entre 15 o 20 minutos, depende de la

186
00:16:55,440 --> 00:17:01,240
dificultad de obtener la imagen. Pero es que no acaba ahí, eso se almacena en una base

187
00:17:01,240 --> 00:17:08,000
de datos, y luego el técnico, la persona experta, tiene que pasarse otros 20 o 30 minutos,

188
00:17:08,000 --> 00:17:16,080
segmentando cosas a mano o bueno, o con ayuda de software, y pasando y dando un informe,

189
00:17:16,080 --> 00:17:21,560
etiquetando, etc. Eso son casi por cada uno de los ecocardiogramas que se hacen unos

190
00:17:21,560 --> 00:17:28,520
20 minutos, y mínimo por la mañana se hacen entre 30 y 50, imaginados el tiempo luego para

191
00:17:28,520 --> 00:17:34,800
informar eso. Entonces, bueno, luego no solo es eso, sino cómo se tienen que enfrentar

192
00:17:34,800 --> 00:17:42,960
con tropecientas aplicaciones que son un galimatía. Yo he puesto un ejemplo de cómo es el commit

193
00:17:42,960 --> 00:17:48,840
de XKCD de los environments en Python, para que sepáis un poco cómo es, pero básicamente

194
00:17:48,840 --> 00:17:54,640
ellos, por ejemplo, hacen una eco, recogen la eco, lo reportan, pero esto tiene que ir

195
00:17:54,640 --> 00:17:58,640
a una base de datos que está dentro del propio departamento, que a su vez está conectada

196
00:17:58,640 --> 00:18:04,080
con otra aplicación del hospital, que a su vez está conectada con otra del centro

197
00:18:04,080 --> 00:18:09,400
regional. Que a su vez, o sea, aquí hay una base de datos, aquí hay otra, esto se duplica

198
00:18:09,400 --> 00:18:17,200
por acá, esto se duplica por allá, y es un caos, es horrible. Y luego ya, pues esto

199
00:18:17,200 --> 00:18:22,160
es un poco bueno, estas dos imágenes usadas con stable diffusion, puse angry doctor y

200
00:18:22,160 --> 00:18:28,640
puse woman y men, esto no tiene que ver con la chat, bueno, sí un poco, pero si veis

201
00:18:28,640 --> 00:18:34,360
el hombre está cabreado normal y la mujer está cabreada a cabreada, ahí lo dejo,

202
00:18:34,360 --> 00:18:40,960
y los sesgos un poco como de qué vais. Y luego, bueno, no quería poner capturas de

203
00:18:40,960 --> 00:18:46,800
pantallas del grupo de WhatsApp, porque me parecía un poco, pues, intimidatorio, pero

204
00:18:46,800 --> 00:18:53,400
estos son mensajes reales que hay casi todos los semanas en el grupo de cardiología sobre

205
00:18:53,400 --> 00:18:58,920
ciertos software que tienen que usar. Es bastante desesperante, el software X no nos

206
00:18:58,920 --> 00:19:02,680
deja confirmar las pestañas y no podemos hacer ingresos desde la guardia, gente que

207
00:19:02,680 --> 00:19:08,840
está a la guardia que no puede trabajar en guardia, porque el software no les va, se

208
00:19:08,840 --> 00:19:13,600
ha vuelto a caer el software, el software va lentísimo, no se puede hacer algo, esos

209
00:19:13,600 --> 00:19:19,440
son los problemas que tienen los doctores. No que vengamos a decirles, hui, va a tener

210
00:19:19,440 --> 00:19:25,280
este señor un infarto dentro de tres días, no, o sea, dejémosle que eso lo hagan ellos.

211
00:19:25,280 --> 00:19:31,640
Entonces, está todo perdido, no se puede hacer nada, no, en realidad es bastante sencillo,

212
00:19:31,640 --> 00:19:36,560
o sea, las soluciones en ver cómo la gente trabaja, escucharles, ver cuáles son sus

213
00:19:36,560 --> 00:19:41,160
necesidades y no solo los doctores, porque muchas veces sacamos fuera de la acuación

214
00:19:41,160 --> 00:19:46,480
a lo que es la gente que trabaja en el hospital, aparte de los doctores y los propios pacientes

215
00:19:46,480 --> 00:19:49,840
que nunca están en la acuación, o sea, el paciente es como, bueno, ahí está, y al

216
00:19:49,840 --> 00:19:54,640
final es el que sufre y que tú le apliques la aplicación de inteligencia artificial.

217
00:19:54,640 --> 00:20:02,440
Entonces, yo recomiendo para todo intentar desarrollarlo con ellos, no sin ellos y diciendo,

218
00:20:02,440 --> 00:20:07,320
aquí lo tenéis y aquí os dejo esto, que es muy bonito, pero que no sabéis ni cómo

219
00:20:07,320 --> 00:20:12,920
usarlo. Pequeños ejemplos, no es por poderme medallas,

220
00:20:12,920 --> 00:20:16,760
podría hablar también de desastres, de hecho, luego si queréis hablamos de desastres que

221
00:20:16,760 --> 00:20:23,960
nos han sucedido, tienen, por ejemplo, una manera de, o sea, lo que estaba hablando antes

222
00:20:23,960 --> 00:20:29,000
de etiquetar ciertas informes que hacen sobre cariografía, que es muy desesperante porque

223
00:20:29,000 --> 00:20:33,760
tienen que ir por códigos, tienen que buscarlo, tal, y nos pidieron si no podíamos hacer

224
00:20:33,760 --> 00:20:38,280
algo muy sencillo con los reportes que ellos sí escriben, porque los gusta escribir, los

225
00:20:38,280 --> 00:20:43,200
escriben, para que se sugirieran etiquetas. Entonces, pues, sin una inteligencia artificial

226
00:20:43,200 --> 00:20:50,200
súper grande, sin un NLP brutal, sino con muchas expulsiones regulares y un poquito

227
00:20:50,200 --> 00:20:56,600
de lógica, hicimos una pequeña aplicación que lo que les va dando es guías para decir,

228
00:20:56,600 --> 00:21:01,880
oye, mira, te gustaría que esta etiqueta salga, elige esta, no, tal, luego obviamente

229
00:21:01,880 --> 00:21:07,640
hicimos una manera de que todo esto sea un aprendizaje activo y tal.

230
00:21:07,640 --> 00:21:11,680
Luego otros pequeños ejemplos, el que ha hablado antes de intentar ayudarles a segmentar ciertas

231
00:21:11,680 --> 00:21:17,440
partes de los ecocardiogramas, que digamos, porque hablo siempre de eco, porque normalmente

232
00:21:17,440 --> 00:21:27,600
el reso, la reso y el tag están más protocolizados y no tienen esa dificultad de tener una segmentación

233
00:21:27,600 --> 00:21:33,760
automática, sin embargo, los ecocardiogramas que vemos aquí a la izquierda y las coronografías

234
00:21:33,760 --> 00:21:38,120
que vemos ahí a la derecha, sí que tienen un poco más dificultad de segmentación o

235
00:21:38,120 --> 00:21:42,040
de poder ver, por ejemplo, en la derecha podemos ver cuando se le hace a alguien, se le va

236
00:21:42,040 --> 00:21:46,560
a hacer un cateter, si está más estrecho o no, pues hacer una segmentación automática

237
00:21:46,560 --> 00:21:51,440
y darles un poco de guía, nunca sustituir al doctor, nunca, esto lo único que tiene

238
00:21:51,440 --> 00:21:56,480
que servir es para ayudarles, para hacerles la vida más fácil y que en el tiempo que

239
00:21:56,480 --> 00:22:00,760
tienen que pasar en el ordenador, porque van a tener que pasarlo, sea el mínimo posible.

240
00:22:00,760 --> 00:22:05,320
Cómo implementarlo, he hablado antes de cómo era el proceso detendioso para que esas aplicaciones

241
00:22:05,320 --> 00:22:09,560
llegaran a que ellos la pueden usar, nosotros lo que hemos hecho en Salamanca es crear

242
00:22:09,560 --> 00:22:14,600
un desarrollo en el cual podemos meter estas aplicaciones, las pueden utilizar y tienen

243
00:22:14,600 --> 00:22:19,360
cierta comunicación con las otras aplicaciones de Galimatías que he hablado anteriormente,

244
00:22:19,360 --> 00:22:23,800
siempre desde una manera de investigación y para que ellos lo puedan usar en momentos

245
00:22:23,800 --> 00:22:27,000
determinados si quieren, nunca obligándoles a hacerlo.

246
00:22:27,000 --> 00:22:34,640
Y bueno, he ido súper rápido al final, dos conclusiones y esto es dos memes, el de la

247
00:22:34,640 --> 00:22:39,400
izquierda dice tenemos que meter algo de inteligencia artificial en el proyecto, si no no lo venderemos

248
00:22:39,400 --> 00:22:43,800
y uno dice transformes, stable diffusion, bueno quizás no hace falta y lo tiran por

249
00:22:43,800 --> 00:22:48,160
la ventana, pues a lo mejor no hace falta meter inteligencia artificial a todo.

250
00:22:48,160 --> 00:22:51,720
Y el otro es, ah mira este sistema funciona, ¿por qué no metemos inteligencia artificial?

251
00:22:51,720 --> 00:22:52,720
¡Pum!

252
00:22:52,720 --> 00:22:54,960
¡Qué podía pasar, se ha agorobado todo!

253
00:22:54,960 --> 00:23:00,120
Pues eso es un poco el mensaje, ya sé, fíjate que me estoy tirando piedras contra mi propio

254
00:23:00,120 --> 00:23:06,680
tejado, porque yo trabajo en esto, pero hagamos las cosas con cabeza, escuchemos a la gente

255
00:23:06,680 --> 00:23:09,360
y no pensemos que nosotros sabemos más que nadie.

256
00:23:09,360 --> 00:23:15,040
El otro día en una charla escuché y esto no es una crítica, esto solo es una crítica

257
00:23:15,040 --> 00:23:21,200
constructiva, que las redes neuronales imitan el cerebro humano, no por favor dejemos eso

258
00:23:21,200 --> 00:23:27,320
ya, es horrible, no podemos imitar algo que no entendemos todavía.

259
00:23:27,320 --> 00:23:32,400
Ni los propios ni los científicos saben exactamente cómo funciona el cerebro, perfectamente,

260
00:23:32,400 --> 00:23:38,160
como nosotros que hacemos código vamos a saber mejor que ellos cómo funciona el cerebro.

261
00:23:38,160 --> 00:23:42,760
Así que nada, bueno eso es todo y pues espero que os haya gustado, gracias.

262
00:23:42,760 --> 00:23:54,600
A mí me ha encantado.

263
00:23:54,600 --> 00:23:55,760
Pasamos al turno de preguntas.

264
00:23:55,760 --> 00:24:11,480
Hay una pregunta, voy con el micro.

265
00:24:11,480 --> 00:24:13,680
Buenas Vítores, muchas gracias por la charla.

266
00:24:13,680 --> 00:24:14,680
A todos los.

267
00:24:14,680 --> 00:24:15,680
Ah vale, que estamos.

268
00:24:15,680 --> 00:24:16,680
Bueno, no pasa nada, yo primero.

269
00:24:16,680 --> 00:24:23,880
Que muchas gracias por la charla tío, es súper interesante porque el tema medicina

270
00:24:23,880 --> 00:24:28,440
es muy bueno, no vamos a entrar a debatir el tema de la protección de la edad, ya has

271
00:24:28,440 --> 00:24:29,760
dicho que se ha hablado mucho de ello.

272
00:24:29,760 --> 00:24:33,720
Te voy a preguntar más, que yo también me he visto en la situación por el tema burocrático

273
00:24:33,720 --> 00:24:41,360
a la hora de encontrar los datos y ya más a nivel España de si cuando estabas en Salaman

274
00:24:41,360 --> 00:24:46,080
que hayas venido a Granada, si has visto que obtener esos datos en un sitio así diferente

275
00:24:46,080 --> 00:24:50,400
al otro y si has podido agregar esos datos de un lado al otro o si has podido obtener

276
00:24:50,400 --> 00:24:55,200
datos de una administración a otra porque es algo que no sé si alguien más aquí le

277
00:24:55,200 --> 00:25:01,880
ha pasado pero es común que lo que haces aquí no es lo mismo que lo que puedes hacer incluso

278
00:25:01,880 --> 00:25:02,880
en otra provincia.

279
00:25:02,880 --> 00:25:03,880
Si, si, si.

280
00:25:03,880 --> 00:25:05,360
Pero si esto también te ha pasado y a qué nivel.

281
00:25:05,360 --> 00:25:11,320
Ah, si, si, yo soy una persona, eso es muy gracioso y bueno, vosotros lo sabéis, podéis

282
00:25:11,320 --> 00:25:18,320
estar por ejemplo si vivís aquí en Andalucía, estáis montando con la moto por ejemplo en

283
00:25:18,320 --> 00:25:24,160
Extremadura, tenéis un accidente, no saben absolutamente nada de vosotros, es como si

284
00:25:24,160 --> 00:25:28,280
no existierais, no tienen ningún dato para saber qué alergias tenéis ni qué problemas

285
00:25:28,280 --> 00:25:31,440
tienen porque todos los datos los tienen aquí, no hay ningún tipo de comunicación entre

286
00:25:31,440 --> 00:25:32,440
comunidades.

287
00:25:32,440 --> 00:25:38,400
Es un poco bueno pues lo de no centralizar los, los, la sanidad que está guay por otras

288
00:25:38,400 --> 00:25:43,000
cosas pero el problema que he dicho un poco con las galematías de las aplicaciones es

289
00:25:43,000 --> 00:25:49,280
muy en concreto para el SACIL que es el servicio de salud de Castile León, a quien Andalucía

290
00:25:49,280 --> 00:25:54,000
va un poco distinto, digamos que en ciertas cosas están mejor en otras no tanto pero

291
00:25:54,000 --> 00:25:59,080
no voy a entrar en esos detalles y si los problemas con los datos son bastante grandes,

292
00:25:59,080 --> 00:26:00,080
es decir nosotros.

293
00:26:00,080 --> 00:26:05,160
Solo a par datos de lo que quieres decir es.

294
00:26:05,160 --> 00:26:10,160
Si, si, para la grabación por lo visto, que si has podido solo a par datos de dos hospitales

295
00:26:10,160 --> 00:26:14,960
distintos de dos comunidades distintas, si se ha intentado siquiera, si ha venido Filipe

296
00:26:14,960 --> 00:26:17,360
y si ha puesto la pasta sobre la mesa para que eso pase.

297
00:26:17,360 --> 00:26:18,360
No, no hay.

298
00:26:18,360 --> 00:26:19,360
No, bueno Filipe no.

299
00:26:19,360 --> 00:26:20,360
Y hay voluntad.

300
00:26:20,360 --> 00:26:27,760
Voluntad sí que hay, hay como bastante voluntades, bueno no sé, estuviste en la ch...

301
00:26:27,760 --> 00:26:33,760
Bueno claro sí, estuviste en la chala de Nerea, lo que estuve hablando de GayaX, sí

302
00:26:33,760 --> 00:26:39,760
que hay voluntad en que eso se cambie pero digamos que va bastante lento, un poco lo

303
00:26:39,760 --> 00:26:43,640
mismo que la idea del 2017 y luego hasta 2022 que se implementa.

304
00:26:43,640 --> 00:26:49,040
Por ejemplo lo que hicimos en esta aplicación que todavía está por acabar de hacerse,

305
00:26:49,040 --> 00:26:53,360
la que he comentado de estenosis aórtica, los datos de validación eran con un hospital

306
00:26:53,360 --> 00:26:54,360
de Madrid.

307
00:26:54,360 --> 00:27:00,120
Sin embargo, para hacerlo lo hemos hecho, no eres un modelo federado porque no lo es

308
00:27:00,120 --> 00:27:04,360
pero ha sido de esa manera, es decir, allí teníamos gente que sabía más o menos cómo

309
00:27:04,360 --> 00:27:08,360
utilizar el modelo que le estábamos dando y ellos lo han aplicado allí directamente.

310
00:27:08,360 --> 00:27:13,960
Nosotros no hemos tocado nunca los datos de Madrid porque no teníamos derecho y bueno

311
00:27:13,960 --> 00:27:18,520
por también porque yo puedo tocar datos de Salamanca porque tenemos un acuerdo de Ocientif,

312
00:27:18,520 --> 00:27:25,120
o sea de firmado entre ambas partes que pues Filipe puede tocar esos datos pero en realidad

313
00:27:25,120 --> 00:27:27,600
es muy complicado, es muy difícil.

314
00:27:27,600 --> 00:27:33,360
Es otro de los temas que hablé en la chala del 2018 y en esta no me he metido tanto

315
00:27:33,360 --> 00:27:34,360
pero sí, sí, sí.

316
00:27:34,360 --> 00:27:36,360
O sea es Jorbo, es Jorbo.

317
00:27:36,360 --> 00:27:39,360
Gracias Andrés.

318
00:27:39,360 --> 00:27:40,360
Tenemos otra pregunta.

319
00:27:40,360 --> 00:27:49,360
Sí es que si se apaga este micrósito se apaga el otro también.

320
00:27:49,360 --> 00:27:55,360
Nada que muchísimas gracias por la charla la verdad que es, vamos, es súper útil porque

321
00:27:55,360 --> 00:28:00,200
al final hay, es muy difícil distinguir lo que es el grano de la paja y al final parece

322
00:28:00,200 --> 00:28:05,680
que hay que se está revolucionando la industria y bueno todavía queda margen.

323
00:28:05,680 --> 00:28:10,400
Entonces una de las cosas que te quería preguntar es, Jorbo, has dicho un montón de cosas que

324
00:28:10,400 --> 00:28:19,000
no hay y me gustaría saber alguna sí que sí que se esté usando algo chulo que miras.

325
00:28:19,000 --> 00:28:24,640
Es sí que hay cosas por ejemplo, a ver, hablo un poco de casa porque es lo que más

326
00:28:24,640 --> 00:28:28,760
conozco no porque quiera hacer publicidad de Philips pero sí que tenemos muchos modelos

327
00:28:28,760 --> 00:28:34,480
que por ejemplo ahora mismo el otro día tengo una reunión con un desarrollador que está

328
00:28:34,480 --> 00:28:43,920
haciendo aplicaciones de hecho con coronavirus frías y si quieres hablo más alto ya está.

329
00:28:43,920 --> 00:28:46,560
No pasa nada, no os preocupéis hablo más alto.

330
00:28:46,560 --> 00:28:51,280
Estaba haciendo como lo que estaba hablando antes de la sementación de coronarias para

331
00:28:51,280 --> 00:28:57,400
que se vea un poco donde está las partes estenosadas y ver un poco también cómo se

332
00:28:57,400 --> 00:28:58,720
inflan los balones de estén.

333
00:28:58,720 --> 00:29:03,360
No sé cuántos formalizados estáis pero al final todos conocemos a alguien que la han

334
00:29:03,360 --> 00:29:07,960
puesto en estén porque es algo muy típico y eso sí que se está desarrollando y se

335
00:29:07,960 --> 00:29:11,760
va a sacar bastante rápido porque hay mucha voluntad.

336
00:29:11,760 --> 00:29:18,280
Es lo que decía antes, cuando hay interés clínico y interés de comercial las cosas

337
00:29:18,280 --> 00:29:20,080
salen mucho más rápido.

338
00:29:20,080 --> 00:29:24,520
En eso se sabe perfectamente que es un poco cómo se mueve el mundo.

339
00:29:24,520 --> 00:29:30,920
Si Siemens que son nuestros competidores han hecho algo parecido rápidamente vamos a intentar

340
00:29:30,920 --> 00:29:36,480
cubrir ese hueco cuanto antes porque sabemos que si no la gente se va con ellos y al revés.

341
00:29:36,480 --> 00:29:39,600
Un poco se mueve así ese tipo de aplicaciones.

342
00:29:39,600 --> 00:29:45,520
Si Siemens saca algo o si General Electric saca algo nosotros tenemos que sacar algo

343
00:29:45,520 --> 00:29:51,400
rápido para cubrir eso o al revés y nosotros sacamos algo ellos también.

344
00:29:51,400 --> 00:29:57,360
En este caso para la gente que se dedica a hemodinámica es bastante bueno porque pueden

345
00:29:57,360 --> 00:30:03,080
tener normalmente lo que les pasa ahora es que estas imágenes no se inventan automáticamente

346
00:30:03,080 --> 00:30:08,600
de las coronarias y lo que tienen que ir es, bueno, parece una nave espacial como está.

347
00:30:08,600 --> 00:30:11,960
No sé si habéis visto como es pero tienen ahí la parte donde meten el cateter están

348
00:30:11,960 --> 00:30:18,720
ahí el paciente tumbado y luego tienen como una especie de pecera donde se sientan y empiezan

349
00:30:18,720 --> 00:30:21,040
ahí un poco a analizar las imágenes.

350
00:30:21,040 --> 00:30:26,360
Y lo que sucede es que muchas veces se tienen que ir a esa pecera o sea están haciendo

351
00:30:26,360 --> 00:30:30,960
el cateterismo se tienen que ir a esa pecera delante del ordenador a calcular a mano cuánto

352
00:30:30,960 --> 00:30:35,720
estás cenosado tal de esta otra manera lo podrían hacer directamente sobre la sala

353
00:30:35,720 --> 00:30:40,200
de quirófano y no tener que tener a alguien o la misma persona que está perando yendo

354
00:30:40,200 --> 00:30:41,200
y viniendo.

355
00:30:41,200 --> 00:30:43,520
Son pequeños, son casos de éxito también.

356
00:30:43,520 --> 00:30:49,160
O sea no, yo me he puesto muy negativo ya lo sé pero es que hay veces que sobre todo

357
00:30:49,160 --> 00:30:55,520
me, bueno ya lo he notido que me cabreo mucho con ciertas cosas pero no todo es horrible.

358
00:30:55,520 --> 00:31:00,800
Buena vista, lo primero es agradecerte las charlas.

359
00:31:00,800 --> 00:31:07,280
Te lo tengo que decir que me da con un poco de miedo soy Gemma, trabajo en Idoven, la

360
00:31:07,280 --> 00:31:13,440
conozcas, estamos aplicando inteligencia artificial a la lectura, electrocardiogramas.

361
00:31:13,440 --> 00:31:16,440
Entonces digo me va a hundir me va a quitar el trabajo.

362
00:31:16,440 --> 00:31:19,420
No he hablado nada de cooker.

363
00:31:19,420 --> 00:31:26,160
Y nada, después de escucharte hemos hablado de problemas con el software que tienen los

364
00:31:26,160 --> 00:31:33,500
médicos, que bueno, haciendo un resumen, según a quién se las signen, típica consultora

365
00:31:33,500 --> 00:31:37,480
cárnica y ya sabemos cómo salen esas cosas. Por otro lado hemos hablado de los problemas

366
00:31:37,480 --> 00:31:43,320
que tenemos para obtener datos de la Junta de Andalucía con la... en fin, eso es problema

367
00:31:43,320 --> 00:31:49,340
de protección de datos. He trabajado en otras empresas que nos encargamos de trabajar con

368
00:31:49,340 --> 00:31:57,460
los datos. Entonces, hemos hablado de problemas pero fuera de la IA. O sea, yo cuando veo

369
00:31:57,460 --> 00:32:01,580
el titular digo, ostias, voy a ir allá a ver qué me cuenta porque tengo un poco de

370
00:32:01,580 --> 00:32:04,740
miedo. Pero todos los problemas que hemos visto es...

371
00:32:04,740 --> 00:32:05,740
De comunicación.

372
00:32:05,740 --> 00:32:08,500
Yo estoy creando... mi equipo está creando un cardiólogo.

373
00:32:08,500 --> 00:32:13,780
Vosotros estamos con gente del CENIC. Es una maravilla lo que estamos haciendo.

374
00:32:13,780 --> 00:32:17,620
Es que estáis con el CENIC, es un centro de investigación top y habéis surgido de

375
00:32:17,620 --> 00:32:23,580
ahí. Tenéis una ventaja que no tienen otras startups que empiezan un poco a ver qué es

376
00:32:23,580 --> 00:32:33,180
lo que hago. O sea, ahí no se ha metido en el mismo saco. Es totalmente distinto. Pero

377
00:32:33,180 --> 00:32:37,980
yo quería un poco tocar cosas que están un poco fuera también de la parte técnica.

378
00:32:37,980 --> 00:32:42,580
Hablé un poco al principio de las partes que se hacen mal sobre todo en publicaciones

379
00:32:42,580 --> 00:32:51,140
y que me salgan un poco de los nervios y también de gente que es un poco vende humos. Y yo

380
00:32:51,140 --> 00:32:56,820
no digo que todos seamos horribles. De hecho, te lo acabo de decir que con esta charla me

381
00:32:56,820 --> 00:33:04,060
estoy tirando un poco piedra sobre el propio tejado porque yo trabajo para esto. Entonces,

382
00:33:04,060 --> 00:33:07,980
también es un poco atacarme a mí mismo. Pero sí que es cierto que hay veces que

383
00:33:07,980 --> 00:33:12,900
hacemos cosas mal. Lo que yo quería hablar y ya no solo con medicina sino con cualquier

384
00:33:12,900 --> 00:33:19,180
disciplina que hay que ser autocríticos. Es decir, no vale solo con meter los datos.

385
00:33:19,180 --> 00:33:24,060
No digo que todos lo hagan. En meter los datos en la máquina y ver lo que sale y ya dárselo.

386
00:33:24,060 --> 00:33:31,460
Sino que hay que intentar ver un poco la parte más que le pasa al paciente. Que le pasa

387
00:33:31,460 --> 00:33:35,980
al médico, que le pasa al técnico que tiene que utilizar esa aplicación.

388
00:33:35,980 --> 00:33:38,460
Sí, comentaba también por el tema de...

389
00:33:38,460 --> 00:33:40,460
Sí, sí, sí.

390
00:33:40,460 --> 00:33:49,460
Y sí, totalmente, la definición que hay en esta parte técnica para las científicas

391
00:33:49,460 --> 00:33:55,460
con la parte médica, espero que es muy difícil trabajar con ellos y hacer caso de ellos.

392
00:33:55,460 --> 00:34:03,460
A ver, es cierto que no he criticado muchos médicos, pero sí también tienen parte de

393
00:34:03,460 --> 00:34:08,300
culpar. No solo somos nosotros. Eso es verdad. Sí, sí. Te doy la razón. Yo he tenido la

394
00:34:08,300 --> 00:34:10,460
suerte de estar en... Sí, perdón.

395
00:34:10,460 --> 00:34:12,460
Perdón. Sí.

396
00:34:12,460 --> 00:34:18,460
Bueno, también quería agradecer de Victor. Muy buena charla. La verdad, muy buenos detalles.

397
00:34:18,460 --> 00:34:19,460
Gracias.

398
00:34:19,460 --> 00:34:26,460
Te quería preguntar desde tu perspectiva. Tal vez esta pregunta es un poco... Tal vez

399
00:34:26,460 --> 00:34:30,460
tu experiencia va un poco más por cómo está el trabajo en empresas grandes, establecidas,

400
00:34:30,460 --> 00:34:35,460
con nombre y mucho capital. Pero en mi caso yo estoy trabajando en un start-up que estamos

401
00:34:35,460 --> 00:34:43,460
buscando sacar al mercado un dispositivo médico acá en la Unión Europea. Y tal vez si tenías

402
00:34:43,460 --> 00:34:52,460
unos consejos o algunos tips que puedas darte o qué cosas es evitar desde tu perspectiva,

403
00:34:52,460 --> 00:34:56,460
¿no? Que puedan servir para ayudarnos a tener éxito en esto.

404
00:34:56,460 --> 00:35:01,460
Uf, pues creo que lo que te recomendé ayer, de hecho, un poco que hablamos un poco de

405
00:35:01,460 --> 00:35:07,460
este tema. La verdad es que yo estoy súper desconectado de IP y de toda la parte legal.

406
00:35:07,460 --> 00:35:12,460
Como dices tú, es una empresa supergrande y tenemos un equipo que se encarga de todo eso.

407
00:35:12,460 --> 00:35:17,460
Y la verdad es que tenemos... Es horrible porque la comunicación que tenemos con ellos es mínima.

408
00:35:17,460 --> 00:35:23,460
Dime lo que hace esta aplicación y ya me encargo yo de toda la parte legal para ver cómo

409
00:35:23,460 --> 00:35:28,460
utilizarla, para conseguir la patente a nivel mundial, europeo, etcétera, etcétera y toda la validación.

410
00:35:28,460 --> 00:35:33,460
Entonces por ahí la verdad es que no me has pillado, pero no te puedo ayudar mucho.

411
00:35:33,460 --> 00:35:40,460
Yo te daría los tips que he dado ahora, pero eso no garantiza que obviamente luego salga las cosas bien.

412
00:35:40,460 --> 00:35:42,460
Gracias.

413
00:35:44,460 --> 00:35:46,460
Tenemos otra pregunta.

414
00:35:48,460 --> 00:35:51,460
Hemos ido pocos, pero muchas preguntas, me gusta.

415
00:35:51,460 --> 00:35:56,460
Hola, Victor. Me ha gustado la charla. Mi pregunta va un poco al hilo de lo que has comentado

416
00:35:56,460 --> 00:35:59,460
cuando hablabas de la instalación de los systems.

417
00:35:59,460 --> 00:36:03,460
Y a mí me ha recordado lo que hace algunos años ley que España es el país de Occidente

418
00:36:03,460 --> 00:36:08,460
donde más systems se colocan frente a hacer bypass gas... Uigástricos no.

419
00:36:08,460 --> 00:36:11,460
Coronarios que son lo que se hace en el resto de Europa.

420
00:36:11,460 --> 00:36:16,460
Entonces en algún momento alguien ha pensado o tú crees que podría resultar útil hacer un modelo

421
00:36:16,460 --> 00:36:23,460
preceptivo o un modelo de inteligencia artificial que te diga por qué eso pasa o qué paciente

422
00:36:23,460 --> 00:36:28,460
sería más apto para un bypass o para un stem, simplemente como ayuda.

423
00:36:28,460 --> 00:36:34,460
Sí, sí, es que además lo que te voy a contar te va a hacer mucha gracia, pero yo estaba en las reuniones

424
00:36:34,460 --> 00:36:41,460
de la mañana porque ellos tienen reuniones clínicas por la mañana y te escuten precisamente eso.

425
00:36:41,460 --> 00:36:45,460
Se llama Hardtine, el equipo de corazón.

426
00:36:45,460 --> 00:36:51,460
Entonces vienen por una parte las personas que realizan bypass y los hemodinamistas,

427
00:36:51,460 --> 00:36:53,460
o sea cirujanos y hemodinismistas.

428
00:36:53,460 --> 00:36:58,460
Se sientan en Salamaca, es muy gracioso porque también se trata, bueno, en otro tono,

429
00:36:58,460 --> 00:37:00,460
se trata gente de Zamora y Ávila.

430
00:37:00,460 --> 00:37:06,460
Entonces van exponiendo casos y vas viendo una cosa muy graciosa, esto es un poco...

431
00:37:06,460 --> 00:37:11,460
Cuando el cirujano ve que el bypass va a ser más fácil,

432
00:37:11,460 --> 00:37:18,460
él sabe que va a obtener éxito con esa intervención y se lo pide.

433
00:37:18,460 --> 00:37:24,460
Yo, para mí, para mí, para mí, porque quiere subir las porcentajes de éxito en cirugía

434
00:37:24,460 --> 00:37:28,460
y por el otro lado está el hemodinimista que dice, jodad, pero no, pero si esto es una tontería,

435
00:37:28,460 --> 00:37:31,460
le meto al cateter y ya está, no, pero ahí empiezan a discutir.

436
00:37:31,460 --> 00:37:34,460
Y al revés, cuando el caso es muy jorobado, dice, no, no, para mí no, lo haces tú con el cateter,

437
00:37:34,460 --> 00:37:37,460
no, pero yo, eso es mejor el bypass, no, no, no.

438
00:37:37,460 --> 00:37:42,460
Entonces, ahí hay un sesgo brutal y sí que se ha intentado hacer porque había muchas,

439
00:37:42,460 --> 00:37:46,460
no con este caso, pero sí con el Tavi, que no sé si sabes lo que es,

440
00:37:46,460 --> 00:37:53,460
que es un reemplazamiento de la válvula aórtica directamente por cateter.

441
00:37:53,460 --> 00:37:59,460
Entonces, se puede hacer cirugía o Tavi, que es, pues, te inyectan una...

442
00:37:59,460 --> 00:38:01,460
Contraste.

443
00:38:01,460 --> 00:38:06,460
No, no contraste, te inyectan directa, contraste también, pero eso antes para verlo,

444
00:38:06,460 --> 00:38:14,460
pero lo que te inyectan es una válvula, digamos, ficticia, que te va a durar cuatro o cinco años,

445
00:38:14,460 --> 00:38:19,460
pero va a hacer que la orta, la válvula, abra bien y cierre.

446
00:38:19,460 --> 00:38:23,460
Y, claro, pues, los hemonomistas están superconvencidos,

447
00:38:23,460 --> 00:38:28,460
que eso es muchísimo mejor a largo de supervivencia, de hecho, hay muchos artículos que lo dicen,

448
00:38:28,460 --> 00:38:32,460
pero los cirujanos siguen diciendo que su manera es mucho más segura,

449
00:38:32,460 --> 00:38:35,460
porque obviamente tienen más datos retrospectivos de éxito que el Tavi,

450
00:38:35,460 --> 00:38:38,460
que ha llegado un poco más recientemente.

451
00:38:38,460 --> 00:38:40,460
Y ahí hay unas peleas.

452
00:38:40,460 --> 00:38:47,460
Entonces, si se consigue un data set, realmente, pocos esgado de decir,

453
00:38:47,460 --> 00:38:55,460
vamos a hacer las cosas bien, vamos a hacer un estudio bien, de cero, y que se demuestre si se podría hacer una valoración así.

454
00:38:55,460 --> 00:39:01,460
Da cosas que se pongan ahí los de acuerdo, que no se estén peleando todo el rato.

455
00:39:01,460 --> 00:39:05,460
Perdona, no te he respondido exactamente, se puede hacer, pero bueno, tengo...

456
00:39:05,460 --> 00:39:10,460
Me sí que la respuesta es que sí se puede hacer, pero si todas las partes están de acuerdo en hacerlo.

457
00:39:10,460 --> 00:39:11,460
Exacto.

458
00:39:11,460 --> 00:39:12,460
Muchas gracias.

459
00:39:12,460 --> 00:39:13,460
Nada a ti.

460
00:39:13,460 --> 00:39:18,460
Hay alguna pregunta más para Víctor?

461
00:39:18,460 --> 00:39:23,460
Qué guay esto, qué guay.

462
00:39:23,460 --> 00:39:26,460
Lo bueno es que, como acababa antes, pues...

463
00:39:26,460 --> 00:39:30,460
Hola, muchas gracias. Yo una muy rápida.

464
00:39:30,460 --> 00:39:36,460
La revista de esta Oyeorno, que aceptaban sin revisar bien los artículos sobre este tema,

465
00:39:36,460 --> 00:39:42,460
solo porque era de ese tema, era una revista si hubo más normalidad o que hubo...

466
00:39:42,460 --> 00:39:44,460
Tenía cierto impacto.

467
00:39:44,460 --> 00:39:49,460
Hay algunas que tenían impacto. El tema es que muchas de ellas, y es una de las problemas que hay,

468
00:39:49,460 --> 00:39:54,460
que tú sabes cómo va esto con lo de la revista, es que hay cierto impacto que sí que es tan...

469
00:39:54,460 --> 00:40:01,460
O sea, tú puedes publicar en una revista clínica y a lo mejor lo que tienen ellos, y eso te lo dicen ellos mismos.

470
00:40:01,460 --> 00:40:06,460
Aquí, por ejemplo, la revista de cardiología ya que nos empezaron a llamar para revisar este tipo de artículos,

471
00:40:06,460 --> 00:40:10,460
porque no conocían a nadie, quisieran nada de Machine Learning en español con cardiología,

472
00:40:10,460 --> 00:40:15,460
pero antes no tenían a nadie. A nosotros nos han revisado una vez un artículo y han dicho,

473
00:40:15,460 --> 00:40:18,460
yo no entiendo nada de esto, pero no me gusta.

474
00:40:18,460 --> 00:40:22,460
Y eso fue la respuesta del revisor y dice, pues vale.

475
00:40:22,460 --> 00:40:28,460
Y el problema es que no encuentran mucha gente y en el caso de estas sí que eran Q1,

476
00:40:28,460 --> 00:40:34,460
pero más en tema de medicina, no de inteligencia artificial en tal, en sí.

477
00:40:34,460 --> 00:40:40,460
Claro, si tú vas con un revisor que sabe y ve que tienes un training, un test sin separar,

478
00:40:40,460 --> 00:40:44,460
pues te va a decir que te vayas a tu casa, no te lo va a publicar.

479
00:40:44,460 --> 00:40:50,460
El problema es encontrar. Es que ya entramos en otro melón de encontrar revisores que hagan ese trabajo

480
00:40:50,460 --> 00:40:53,460
y que se dediquen ese tiempo para, por lo menos...

481
00:40:53,460 --> 00:40:57,460
O sea, he criticado los revisores, pero todos sabemos que no se pagan las revisiones,

482
00:40:57,460 --> 00:41:03,460
que es que todo día ahí que no sé si me van a echarle el trabajo de todas las cosas que estoy diciendo,

483
00:41:03,460 --> 00:41:11,460
pero no sé. Pero sí, la respuesta es que sí, se hagan Q1 y D1.

484
00:41:11,460 --> 00:41:13,460
Hola.

485
00:41:13,460 --> 00:41:15,460
Hola, muchas gracias, Vista.

486
00:41:15,460 --> 00:41:19,460
Yo quería preguntar un poco sobre...

487
00:41:19,460 --> 00:41:24,460
No sé si recuerda, porque como está, ve que está en el tema de cardiología

488
00:41:24,460 --> 00:41:32,460
sobre este artículo que podían predecir la muerte con pocos días o semanas.

489
00:41:32,460 --> 00:41:34,460
No me acuerdo, pero bastante...

490
00:41:34,460 --> 00:41:36,460
Sí, sé cuál dices.

491
00:41:36,460 --> 00:41:42,460
¿Qué tipo de explicación podemos dar? Porque yo eso lo hablo con mi amiga cardióloga y...

492
00:41:42,460 --> 00:41:44,460
¿Qué te dice ella?

493
00:41:44,460 --> 00:41:45,460
Eso no puede ser.

494
00:41:45,460 --> 00:41:47,460
¿Cómo se lo explicamos?

495
00:41:47,460 --> 00:41:55,460
¿Qué tipo de explicación sería válida para el tipo de persona médico con el que trabaja?

496
00:41:55,460 --> 00:42:00,460
Claro, ese es el tema que muchas veces el problema de lo que está hablando...

497
00:42:00,460 --> 00:42:05,460
Bueno, de lo que está hablando Arabia, que hay muchos métodos de explicabilidad,

498
00:42:05,460 --> 00:42:11,460
de interpretabilidad, pero muchas veces cuando se lo hemos hecho nosotros también.

499
00:42:11,460 --> 00:42:15,460
Se lo enseñas al cardiólogo y hay veces que incluso es contraproducente,

500
00:42:15,460 --> 00:42:19,460
porque nosotros tenemos algunos muy agnósticos y otros superpositivos,

501
00:42:19,460 --> 00:42:21,460
es decir, en cuanto les das una explicación,

502
00:42:21,460 --> 00:42:24,460
¡Ah, sí, sí, esto tiene todo el sentido del mundo! Venga, venga, va a publicar.

503
00:42:24,460 --> 00:42:30,460
Pero vamos a espérate un poco, a ver si esto tiene sentido antes de meterte en un jardín.

504
00:42:30,460 --> 00:42:37,460
En ese concreto sí que sé que se han hecho algunas revisiones un poco en profundidad.

505
00:42:37,460 --> 00:42:42,460
No te puedo dar la cita, pero sí que se ha desmontado un poco ese mito

506
00:42:42,460 --> 00:42:46,460
y se ha hablado un poco de ciertos esgos que había en el Dataset.

507
00:42:46,460 --> 00:42:50,460
No lo sé con seguridad, porque estoy hablando de memoria,

508
00:42:50,460 --> 00:42:55,460
de algo que leí hace tiempo y prefiero ir a mirarlo y contestarte mejor.

509
00:42:55,460 --> 00:42:59,460
Pero la explicación muchas veces de...

510
00:42:59,460 --> 00:43:03,460
O sea, es lo que digo, tú puedes tener un modelo que sea perfecto,

511
00:43:03,460 --> 00:43:06,460
que realmente te diga algo que es maravilloso,

512
00:43:06,460 --> 00:43:12,460
pero que no sepas explicar por qué, yo desde luego no me metería en una sala de quirófono

513
00:43:12,460 --> 00:43:16,460
con algo que no sabe decirme porque está tomando una decisión, sí o no.

514
00:43:16,460 --> 00:43:27,460
Entonces claro. Pero si te asalas cierto, 80% y la de un experto es 60%.

515
00:43:27,460 --> 00:43:30,460
Yo sigo pensando que prefiero el experto.

516
00:43:32,460 --> 00:43:38,460
Yo prefiero el experto porque el experto ante cierta duda va a tener un poco más de desarrollo

517
00:43:38,460 --> 00:43:41,460
luego posterior de porque hace algo, ¿no?

518
00:43:41,460 --> 00:43:46,460
Sin embargo, la máquina de decir, pues haz esto, ¿por qué?

519
00:43:46,460 --> 00:43:50,460
No sé, a mí me daría un poco de miedo meterme personalmente.

520
00:43:50,460 --> 00:43:56,460
Sé un poco por lo que va porque Joff Hinton habló un poco de esto,

521
00:43:56,460 --> 00:44:02,460
de que él prefería... personalmente yo no estoy de acuerdo.

522
00:44:02,460 --> 00:44:06,460
Es un crack, científicamente es lo mejor.

523
00:44:06,460 --> 00:44:13,460
Pero por ahí yo no me sentaría en una mesa de quirófono que me vaya a operar

524
00:44:13,460 --> 00:44:19,460
una máquina que no sé que me explique cómo va a hacer las cosas, ¿sabes?

525
00:44:19,460 --> 00:44:22,460
Pero bueno, eso es una discusión bastante chula.

526
00:44:22,460 --> 00:44:24,460
Podemos tenerla.

527
00:44:24,460 --> 00:44:25,460
Muchas gracias.

528
00:44:25,460 --> 00:44:26,460
Nada a ti.

529
00:44:29,460 --> 00:44:31,460
Es una discusión súper chula.

530
00:44:31,460 --> 00:44:39,460
Hoy precisamente en la keynote ha puesto May un ejemplo de un modelo de aprendizaje automático

531
00:44:39,460 --> 00:44:46,460
super eficiente en plan 90% 95% y detectaba la marca de agua que tenía la radiografía,

532
00:44:46,460 --> 00:44:49,460
que ya marcaba la radiografía si tenía cáncer o no,

533
00:44:49,460 --> 00:44:53,460
y en el hombro ni siquiera en el pulmón era un ejemplo que podéis ver en el vídeo

534
00:44:53,460 --> 00:44:57,460
en YouTube de la keynote de la PAICOM 2022.

535
00:44:57,460 --> 00:45:01,460
Pero esto no es de ahora, o sea, perdón, es que me estoy enrollando mucho,

536
00:45:01,460 --> 00:45:06,460
pero hace como cinco, seis años, bueno, más, incluso hubo un concurso de estos

537
00:45:06,460 --> 00:45:10,460
del principio para reconocer solo un caballo en fotos.

538
00:45:10,460 --> 00:45:13,460
Y era la marca de agua, la Lager-Wise Propagation.

539
00:45:13,460 --> 00:45:17,460
Que ponía Fert, que ponía, que es caballo en alemán y se fijaban eso.

540
00:45:17,460 --> 00:45:19,460
Qué bueno que lo mencione.

541
00:45:19,460 --> 00:45:23,460
Lager-Wise relevant propagation.

542
00:45:23,460 --> 00:45:29,460
Ese es el método que descubrió que el modelo de Machin Lenin prefirió,

543
00:45:29,460 --> 00:45:33,460
era más fácil leer la marca de agua que detectase a un caballo, de verdad.

544
00:45:33,460 --> 00:45:35,460
La solución fácil.

545
00:45:35,460 --> 00:45:38,460
¿Alguna pregunta más?

546
00:45:41,460 --> 00:45:46,460
Yo la comparto, sí. No hay problema.

547
00:45:46,460 --> 00:45:50,460
Luego se subirá el vídeo, supongo, también.

548
00:45:50,460 --> 00:45:55,460
Muchísimas gracias, Víctor. Un gran aplauso, genial.

