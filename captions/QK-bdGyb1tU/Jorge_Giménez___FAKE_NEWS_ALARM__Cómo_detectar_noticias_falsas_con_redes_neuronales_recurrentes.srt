1
00:00:00,000 --> 00:00:16,800
INTRO

2
00:00:16,800 --> 00:00:17,200
Hola.

3
00:00:17,200 --> 00:00:18,200
Buenos días a todos.

4
00:00:18,200 --> 00:00:19,920
Ya con lo que han presentado anteriormente,

5
00:00:19,920 --> 00:00:21,080
soy Jorge Jimenez García.

6
00:00:21,080 --> 00:00:24,720
Bueno, soy ingeniero Big Data Engineer en Bluetooth.

7
00:00:24,720 --> 00:00:27,360
Y bueno, les vengo a presentar este proyecto de cómo poder

8
00:00:27,360 --> 00:00:29,600
detectar noticias falsas con redes de neuronas

9
00:00:29,600 --> 00:00:30,880
recurrentes.

10
00:00:30,880 --> 00:00:34,200
Y os quiero presentar, bueno, un pequeño índice, ¿vale?

11
00:00:34,200 --> 00:00:36,840
En el cómo va a hacer toda esta presentación y cómo va a ir

12
00:00:36,840 --> 00:00:37,680
guiada.

13
00:00:37,680 --> 00:00:39,600
Quiero hacer una pequeñita introducción para que me

14
00:00:39,600 --> 00:00:42,000
conozcan un poquito más, no solamente que me vean aquí

15
00:00:42,000 --> 00:00:43,800
y no sepan nada mucho más de mí.

16
00:00:43,800 --> 00:00:45,640
Vamos a hacer una pequeña introducción y vamos a ir

17
00:00:45,640 --> 00:00:49,240
desarrollando cómo sale la idea del proyecto y cómo vamos a

18
00:00:49,240 --> 00:00:52,960
plantearla y crear ese tipo de redes de neuronas y poder

19
00:00:52,960 --> 00:00:54,880
detectar noticias falsas.

20
00:00:54,880 --> 00:00:57,720
Y por último, sacar esas conclusiones con los modelos

21
00:00:57,720 --> 00:00:59,880
que vamos a crear.

22
00:00:59,880 --> 00:01:02,040
Pues como os he mencionado, soy Jorge Jimenez García.

23
00:01:02,040 --> 00:01:05,160
Llevo un año y cuatro meses trabajando para Bluetooth como

24
00:01:05,160 --> 00:01:06,320
ingeniero de datos.

25
00:01:06,320 --> 00:01:10,600
Y soy una persona que tiene mucha curiosidad y mucha

26
00:01:10,600 --> 00:01:12,560
curiosidad, básicamente, por la entesía artificial,

27
00:01:12,560 --> 00:01:14,240
deep learning y las redes de neuronas,

28
00:01:14,240 --> 00:01:17,400
que es un campo nuevo y innovador que todo el mundo al final

29
00:01:17,400 --> 00:01:22,320
quiere un poco más conocer y saber qué lleva y qué esconde.

30
00:01:22,320 --> 00:01:23,560
Entonces, comencemos.

31
00:01:23,560 --> 00:01:25,960
¿Cuándo surge la idea de planternos ese Fegnigo,

32
00:01:25,960 --> 00:01:27,000
ese detector?

33
00:01:27,000 --> 00:01:29,400
Pues este detector surge cuando estamos todos en casa,

34
00:01:29,400 --> 00:01:31,800
confinados en el que no teníamos ni idea de lo que estaba

35
00:01:31,800 --> 00:01:33,440
pasando con el tema de la pandemia.

36
00:01:33,440 --> 00:01:35,520
Entonces, todos recurriamos a lo mismo,

37
00:01:35,520 --> 00:01:37,360
lo que tengamos más cerca de nuestras manos para poder buscar

38
00:01:37,360 --> 00:01:39,760
información, porque no podíamos salir de casa.

39
00:01:39,760 --> 00:01:44,280
Entonces, recurriamos a WhatsApp, Facebook, Twitter,

40
00:01:44,280 --> 00:01:46,880
cualquier tipo de red social, Instagram, TikTok,

41
00:01:46,880 --> 00:01:50,520
para poder recurrir a ella y sacar algo de información.

42
00:01:50,520 --> 00:01:52,640
Pero todo lo que liéllamos o la gran mayoría de las cosas

43
00:01:52,640 --> 00:01:54,960
podían ser reales como falsas.

44
00:01:54,960 --> 00:01:58,120
Es decir, podíamos leer que con una infusión nos podíamos

45
00:01:58,120 --> 00:02:01,120
proteger del COVID, como que Bill Gates nos iba a meter 40

46
00:02:01,120 --> 00:02:04,240
microchips dentro del cuerpo, o que si nos veíamos un básico

47
00:02:04,240 --> 00:02:06,360
de elegía con Donald Trump y vamos a estar más sanos que

48
00:02:06,360 --> 00:02:07,080
nunca.

49
00:02:07,080 --> 00:02:10,920
Entonces, Jackie quise plantear la idea de cómo podemos

50
00:02:10,920 --> 00:02:13,160
solucionar esto, de cuando nosotros entremos a esa red

51
00:02:13,160 --> 00:02:17,160
social, saber con una fiabilidad o con un porcentaje de cómo va

52
00:02:17,160 --> 00:02:19,240
a ser eso real o no va a ser real.

53
00:02:19,240 --> 00:02:21,120
Entonces, con la inteligencia artificial,

54
00:02:21,120 --> 00:02:23,600
el deep learning, que es una de las ramas que tiene la

55
00:02:23,600 --> 00:02:26,360
inteligencia artificial y el análisis de los datos que tenemos

56
00:02:26,360 --> 00:02:29,240
de Twitter, podemos hacer este detector de noticias,

57
00:02:29,240 --> 00:02:30,520
este fake news.

58
00:02:30,520 --> 00:02:32,760
Entonces, tenemos que hacer esa breve introducción,

59
00:02:32,760 --> 00:02:35,040
una pequeña introducción a que se llama la inteligencia

60
00:02:35,040 --> 00:02:35,800
artificial.

61
00:02:35,800 --> 00:02:38,960
Y todos conocemos que el inteligencia artificial es aquello

62
00:02:38,960 --> 00:02:41,920
que todo queremos conseguir con el ordenador a nivel computacional

63
00:02:41,920 --> 00:02:43,320
que nos imite.

64
00:02:43,320 --> 00:02:45,640
Dentro tenemos esos campos de lo que sería el machine learning,

65
00:02:45,640 --> 00:02:48,240
que lo conocemos o la gran mayoría lo conoce más por su

66
00:02:48,240 --> 00:02:51,000
terminología inglesa, que es española,

67
00:02:51,000 --> 00:02:52,640
que es el aprendizaje automático.

68
00:02:52,640 --> 00:02:55,160
El aprendizaje automático nos va a ayudar principalmente a los

69
00:02:55,160 --> 00:02:56,880
conocimientos de datos a mejorarlos,

70
00:02:56,880 --> 00:02:59,840
a mejorar nuestros modelos predictivos y a tomar decisiones.

71
00:02:59,840 --> 00:03:03,400
Y tenemos que hablar del aprendizaje supervisado.

72
00:03:03,400 --> 00:03:05,760
Es decir, vamos a etiquetar nuestros datos que nos van a

73
00:03:05,760 --> 00:03:08,080
venir y poder sacar una conclusión.

74
00:03:08,080 --> 00:03:10,080
Y eso nos va a dar un feedback directo.

75
00:03:10,080 --> 00:03:11,440
También existe el no supervisado.

76
00:03:11,440 --> 00:03:14,760
El no supervisado encuentra patrones, ¿vale?

77
00:03:14,760 --> 00:03:17,320
Partiendo un algoritmo y saca estructuras que visualmente

78
00:03:17,320 --> 00:03:19,880
nosotros no podemos reconocer.

79
00:03:19,880 --> 00:03:22,840
Por último, tenemos el de refuerzo.

80
00:03:22,840 --> 00:03:25,840
Nosotros tendríamos un pequeño agente que se encargaría

81
00:03:25,840 --> 00:03:28,800
dependiendo del entorno en el que se encuentre y las decisiones

82
00:03:28,800 --> 00:03:32,400
que tome, va a ser que tome unas decisiones u otras y sea

83
00:03:32,400 --> 00:03:34,080
recompensado por ello.

84
00:03:34,080 --> 00:03:37,080
Si nosotros juntamos estos 3 aprendizajes automáticos y lo

85
00:03:37,080 --> 00:03:39,520
intentamos optimizar dándole muchos más datos de los que

86
00:03:39,520 --> 00:03:42,920
reciben aprendizaje automático, conseguimos lo denominado

87
00:03:42,920 --> 00:03:44,040
deep learning.

88
00:03:44,040 --> 00:03:45,440
Aquí como podemos ver en la imagen,

89
00:03:45,440 --> 00:03:48,440
es más fácil de comprender en qué punto se encuentra el

90
00:03:48,440 --> 00:03:49,000
deep learning.

91
00:03:49,000 --> 00:03:50,760
Se encuentra en lo más profundo de la visibilidad de

92
00:03:50,760 --> 00:03:54,880
entidad artificial, ya que es lo más novedor y revolucionario.

93
00:03:54,880 --> 00:03:57,720
También tiene un sistema muy parecido al nuestro nervioso.

94
00:03:57,720 --> 00:04:00,760
Intenta poder transmitir información entre neuronas.

95
00:04:00,760 --> 00:04:04,760
De ahí el nombre del proyecto, Treads de Neurona.

96
00:04:04,760 --> 00:04:07,560
Y además tiene un alto rendimiento cuando mayor

97
00:04:07,560 --> 00:04:10,400
número de datos le vayamos dando a nuestro modelo o a que

98
00:04:10,400 --> 00:04:14,280
provenga del deep learning va a ir mejorando.

99
00:04:14,280 --> 00:04:16,440
Aquí es un simple ejemplo de cómo sería la diferencia entre

100
00:04:16,440 --> 00:04:17,880
machine learning y el deep learning.

101
00:04:17,880 --> 00:04:19,680
El machine learning, nosotros tenemos que etiquetar,

102
00:04:19,680 --> 00:04:21,960
tenemos que hacer esa extracción para después se cree esa

103
00:04:21,960 --> 00:04:24,440
clasificación y nosotros saquemos un resultado.

104
00:04:24,440 --> 00:04:27,120
El deep learning se encargaría perfectamente de hacer esa

105
00:04:27,120 --> 00:04:30,680
extracción y esa clasificación y sacar el dato que nosotros

106
00:04:30,680 --> 00:04:31,400
queremos.

107
00:04:31,400 --> 00:04:33,600
Como podemos ver en esta imagen, podemos intuir de si es un

108
00:04:33,600 --> 00:04:36,240
coche, podría ser un camión o podría ser una moto.

109
00:04:36,240 --> 00:04:37,280
Pero este no es nuestro caso.

110
00:04:37,280 --> 00:04:39,640
Nosotros lo que queremos saber es si nosotros,

111
00:04:39,640 --> 00:04:42,920
un texto que estamos leyendo, es real o no es real.

112
00:04:42,920 --> 00:04:45,280
Y tenemos que hablar de una rama en específico del deep learning

113
00:04:45,280 --> 00:04:47,000
que se llama NLP.

114
00:04:47,000 --> 00:04:48,640
Natural Language Processing.

115
00:04:48,640 --> 00:04:50,160
Muchos de ustedes lo habrán escuchado a la hora de estas

116
00:04:50,160 --> 00:04:52,680
charlas y es una rama fundamental.

117
00:04:52,680 --> 00:04:55,240
Con esta imagen podemos comprender mucho más rápido cómo

118
00:04:55,240 --> 00:04:57,320
funciona el NLP.

119
00:04:57,320 --> 00:05:00,440
Vamos, con un texto, un vídeo o un audio,

120
00:05:00,440 --> 00:05:03,880
el NLP podría ser capaz de identificar estructuras,

121
00:05:03,880 --> 00:05:07,400
podría recuperar entidades, análisis de sentimientos,

122
00:05:07,400 --> 00:05:10,680
es decir, saber si es falso, si es real, si estás contento,

123
00:05:10,680 --> 00:05:13,880
puedes estar triste.

124
00:05:13,880 --> 00:05:16,520
Entonces, también el NLP tiene un gran problema en el que si

125
00:05:16,520 --> 00:05:18,240
nosotros ponemos este caso en el que podemos decir,

126
00:05:18,240 --> 00:05:22,000
a este plato le falta sal o sal allí ahora mismo que se está

127
00:05:22,000 --> 00:05:23,160
quemando la casa.

128
00:05:23,160 --> 00:05:24,600
Por ejemplo, tenemos una repetición,

129
00:05:24,600 --> 00:05:26,360
tenemos esa palabra de, por ejemplo, sal.

130
00:05:26,360 --> 00:05:28,880
Nosotros obviamente no tenemos esa confusión,

131
00:05:28,880 --> 00:05:30,600
entendemos esa diferencia.

132
00:05:30,600 --> 00:05:33,560
Pero un ordenador va a recibir esa oración y no va a entender

133
00:05:33,560 --> 00:05:34,960
cuál es la diferencia.

134
00:05:34,960 --> 00:05:37,680
Y aquí es cuando entra el factor de cuando estábamos en el

135
00:05:37,680 --> 00:05:40,960
instituto de saber el análisis sintáptico.

136
00:05:40,960 --> 00:05:45,000
Y esa es una importancia vital porque hay que evitar que el

137
00:05:45,000 --> 00:05:48,960
ordenador pueda confundirse, tener esa ambigüedad.

138
00:05:48,960 --> 00:05:51,800
Y eso es unos valores importantes que tiene el NLP y también hay

139
00:05:51,800 --> 00:05:53,640
que relacionarle el contexto.

140
00:05:53,640 --> 00:05:56,880
Tiene que entender que uno es para la comida y el otro es para

141
00:05:56,880 --> 00:05:58,840
una emergencia, por ejemplo.

142
00:05:58,840 --> 00:06:00,880
Eso lo tiene que comprender.

143
00:06:00,880 --> 00:06:03,960
Cómo también podemos mejorar para que el NLP funcione para la

144
00:06:03,960 --> 00:06:04,640
perfección.

145
00:06:04,640 --> 00:06:07,480
Tenemos que toquenizar todo el texto o toda la información que

146
00:06:07,480 --> 00:06:08,200
nos venga.

147
00:06:08,200 --> 00:06:10,200
¿Qué significa toquenización?

148
00:06:10,200 --> 00:06:13,800
Es fragmentarlo en partes más pequeñas y lo vamos a denominar

149
00:06:13,800 --> 00:06:14,720
tokens.

150
00:06:14,720 --> 00:06:15,880
Esto tenemos que etiquetarlo.

151
00:06:15,880 --> 00:06:18,800
Es decir, tenemos que saber si son adjetivos, verbos,

152
00:06:18,800 --> 00:06:19,400
adverbios.

153
00:06:19,400 --> 00:06:22,760
Hay que etiquetarlos de alguna manera para que él comprenda y

154
00:06:22,760 --> 00:06:24,920
pueda ir mejorando con respecto al tiempo.

155
00:06:24,920 --> 00:06:28,080
También tenemos que quedar solamente con la raíz de las

156
00:06:28,080 --> 00:06:30,680
palabras, porque va a ser mucho más eficiente a que si tenemos

157
00:06:30,680 --> 00:06:33,480
las derivadas de las palabras o con los gentilicios de las

158
00:06:33,480 --> 00:06:34,680
palabras.

159
00:06:34,680 --> 00:06:36,200
Y por último, tenemos que eliminar,

160
00:06:36,200 --> 00:06:37,680
que se llama en inglés las stop words,

161
00:06:37,680 --> 00:06:40,040
que son palabras que no nos aportan información para las

162
00:06:40,040 --> 00:06:41,280
oraciones.

163
00:06:41,280 --> 00:06:45,080
Y con todo esto, podemos ya empezar a plantearnos cómo va

164
00:06:45,080 --> 00:06:46,360
a funcionar esa red neuronal.

165
00:06:46,360 --> 00:06:48,680
¿Y qué es una red neuronal?

166
00:06:48,680 --> 00:06:51,240
Pues básicamente es la manera en que nosotros podemos conectarnos

167
00:06:51,240 --> 00:06:53,760
entre nodos y transmitir esa información.

168
00:06:53,760 --> 00:06:57,440
Estos nodos los vamos a denominar neuronas.

169
00:06:57,440 --> 00:07:00,360
A nivel computacional nos quedaría de esta manera.

170
00:07:00,360 --> 00:07:02,200
Nosotros podemos tener una entrada de capas,

171
00:07:02,200 --> 00:07:04,720
podemos poner el número N que nosotros queramos de entrada y

172
00:07:04,720 --> 00:07:06,520
un número también N de salida.

173
00:07:06,520 --> 00:07:09,000
En nuestro caso, lo más parecido va a ser a este tipo de red

174
00:07:09,000 --> 00:07:09,760
neuronal.

175
00:07:09,760 --> 00:07:10,080
¿Por qué?

176
00:07:10,080 --> 00:07:11,720
Porque solamente queremos una única salida.

177
00:07:11,720 --> 00:07:14,400
Queremos saber si la noticia que está recibiendo o que estamos

178
00:07:14,400 --> 00:07:17,360
leyendo es real o es falsa.

179
00:07:17,360 --> 00:07:19,120
¿Cómo se conectan estas neuronas?

180
00:07:19,120 --> 00:07:21,400
¿Cómo vamos de una a la otra?

181
00:07:21,400 --> 00:07:23,200
Esas conexiones, estas líneas que vemos,

182
00:07:23,200 --> 00:07:26,080
existen unos pesos y unos umbrales que,

183
00:07:26,080 --> 00:07:28,440
si se cumple las condiciones que nosotros hemos puesto,

184
00:07:28,440 --> 00:07:30,920
se transmitirá esa información de una neurona a otra.

185
00:07:30,920 --> 00:07:33,520
Con eso llegando a que saquemos la conclusión que nosotros

186
00:07:33,520 --> 00:07:37,160
queremos y el modelo puede entrenar.

187
00:07:37,160 --> 00:07:39,240
Claro, ahora tenemos que plantearnos la idea de ¿cómo

188
00:07:39,240 --> 00:07:40,880
vamos a hacer esto con palabras?

189
00:07:40,880 --> 00:07:42,800
Un ejemplo muy sencillo para poder entenderlo,

190
00:07:42,800 --> 00:07:44,120
vamos a hablar de los autoencoder.

191
00:07:44,120 --> 00:07:46,800
El autoencoder nos va a ayudar a poder crear esas palabras

192
00:07:46,800 --> 00:07:49,000
y vectorizarlas, porque las palabras tenemos que convertirlas

193
00:07:49,000 --> 00:07:51,000
de alguna manera en números para que el ordenador lo

194
00:07:51,000 --> 00:07:52,280
pueda entender.

195
00:07:52,280 --> 00:07:54,800
A nivel visual, quiero que sepan a nivel de autoencoder,

196
00:07:54,800 --> 00:07:58,400
puede hacer esto, es decir, poder recibir una imagen pixelada y

197
00:07:58,400 --> 00:08:00,880
va bajando en ese espacio latente que vemos aquí,

198
00:08:00,880 --> 00:08:03,600
baja las dimensionalidades a tal punto que podemos hablar de

199
00:08:03,600 --> 00:08:06,000
que sería ir casi pixel a pixel.

200
00:08:06,000 --> 00:08:09,360
Y no solamente es que tenga la misma imagen,

201
00:08:09,360 --> 00:08:11,000
va a crear algo completamente nuevo.

202
00:08:11,000 --> 00:08:13,920
En este caso vemos que está muy pixelada, la monaliza,

203
00:08:13,920 --> 00:08:15,360
y sale perfecta esa imagen.

204
00:08:15,360 --> 00:08:17,880
Sale como debería estar.

205
00:08:17,880 --> 00:08:20,720
Entonces, esto lo nosotros tenemos que hacerlo a nivel de

206
00:08:20,720 --> 00:08:22,000
texto, ¿cómo?

207
00:08:22,000 --> 00:08:23,480
Con el one-hot encoding.

208
00:08:23,480 --> 00:08:25,560
Esto lo que va a hacer es poder ir asignándole,

209
00:08:25,560 --> 00:08:26,680
como vemos aquí con las palabras,

210
00:08:26,680 --> 00:08:28,840
de las abreviaciones, zoológicos, zoom,

211
00:08:28,840 --> 00:08:31,880
asignarle un vector a su espacio vectorial.

212
00:08:31,880 --> 00:08:34,480
Y ese espacio vectorial sería ese contexto del que nosotros

213
00:08:34,480 --> 00:08:37,360
hemos mencionado anteriormente para que no se confunda nuestra

214
00:08:37,360 --> 00:08:40,280
máquina y comprenda de lo que estamos hablando o del

215
00:08:40,280 --> 00:08:43,880
contexto en el que nos encontramos.

216
00:08:43,880 --> 00:08:45,880
Tenemos que hablar de las redes neuronales recurrentes,

217
00:08:45,880 --> 00:08:47,240
porque de esto va el proyecto.

218
00:08:47,240 --> 00:08:50,000
Porque eso es las recurrentes y no es una red neuronal simple,

219
00:08:50,000 --> 00:08:52,000
porque existe la memoria interna.

220
00:08:52,000 --> 00:08:54,960
La memoria interna es ese proceso en el que esta red neuronal es

221
00:08:54,960 --> 00:08:56,960
capaz de acordarse de lo que hizo anteriormente.

222
00:08:56,960 --> 00:08:59,600
Por eso es una de las más importantes y por esas tenemos

223
00:08:59,600 --> 00:09:00,600
que utilizar.

224
00:09:00,600 --> 00:09:03,680
Son muy robustas y al tener ese momento de poder tener ese

225
00:09:03,680 --> 00:09:06,840
recuerdo, la decisión que van a tomar va a ser mucho más

226
00:09:06,840 --> 00:09:08,000
precisa.

227
00:09:08,000 --> 00:09:10,840
Pero no vamos a hablar solamente de una red neuronal específica.

228
00:09:10,840 --> 00:09:15,560
Vamos a hablar de las LSTM que vienen de un término anglosajón

229
00:09:15,560 --> 00:09:19,600
que se llama long short term memory.

230
00:09:19,600 --> 00:09:22,760
Este proceso son muy diferentes a las recurrentes.

231
00:09:22,760 --> 00:09:24,920
Como podemos ver en esta imagen, se ve mucho más sencillo.

232
00:09:24,920 --> 00:09:27,400
La primera imagen es la que os enseñé anteriormente con todas

233
00:09:27,400 --> 00:09:31,400
esas capas de entrada, con una capa oculta y la capa de salida.

234
00:09:31,400 --> 00:09:33,960
Las recurrentes tenemos eso que está en el medio, que sea esa

235
00:09:33,960 --> 00:09:36,080
manera de poder tener la memoria.

236
00:09:36,080 --> 00:09:38,640
Y el último que tenemos es esa manera de,

237
00:09:38,640 --> 00:09:40,960
existe que la diferencia con las recurrentes,

238
00:09:40,960 --> 00:09:43,080
es la puerta de olvido.

239
00:09:43,080 --> 00:09:48,000
Es capaz de saber si el movimiento o la decisión que tomó es más

240
00:09:48,000 --> 00:09:49,480
óptima que la anterior.

241
00:09:49,480 --> 00:09:52,760
Y si esa decisión es mejor, se olvida y continúa.

242
00:09:52,760 --> 00:09:56,080
Y sigue estudiando y sigue memorizando y sigue mejorando lo

243
00:09:56,080 --> 00:09:57,520
que sea esa red neuronal.

244
00:09:57,520 --> 00:10:01,920
Por eso es tan importante utilizar esas LSTM.

245
00:10:01,920 --> 00:10:03,800
Entonces, ya partidas aquí es cuando tenemos que plantearnos

246
00:10:03,800 --> 00:10:04,400
ese problema.

247
00:10:04,400 --> 00:10:06,800
¿Qué podemos hacer para detectar esas noticias falsas y

248
00:10:06,800 --> 00:10:08,160
poder pararlas?

249
00:10:08,160 --> 00:10:11,600
Pues lo primero, vamos a utilizar una red neuronal LSTM y

250
00:10:11,600 --> 00:10:12,600
vamos a entrenarla.

251
00:10:12,600 --> 00:10:13,480
¿Cómo es para entrenarla?

252
00:10:13,480 --> 00:10:14,360
¿Qué necesitamos?

253
00:10:14,360 --> 00:10:15,120
Muchos datos.

254
00:10:15,120 --> 00:10:15,520
Datos.

255
00:10:15,520 --> 00:10:16,640
¿De dónde los vamos a sacar?

256
00:10:16,640 --> 00:10:17,520
De Twitter.

257
00:10:17,520 --> 00:10:19,800
Porque es la que queremos intentar analizar y poder sacar esa

258
00:10:19,800 --> 00:10:20,800
fiabilidad.

259
00:10:20,800 --> 00:10:22,880
Y, por lo tanto, también tenemos que analizar esos datos que

260
00:10:22,880 --> 00:10:25,920
estamos sacando de Twitter para saber si van a ser útiles o no

261
00:10:25,920 --> 00:10:28,440
que nos van a ser útiles.

262
00:10:28,440 --> 00:10:31,760
Entonces, antes de comenzar y ya meternos en el fregado,

263
00:10:31,760 --> 00:10:33,280
vamos a hablar de las herramientas utilizadas.

264
00:10:33,280 --> 00:10:34,600
Obviamente, vamos a utilizar Python.

265
00:10:34,600 --> 00:10:36,280
No creo que les sorprenda mucho.

266
00:10:36,280 --> 00:10:38,200
Vamos a utilizar la API de Twitter.

267
00:10:38,200 --> 00:10:40,240
Y lo que mencionó anteriormente es el lenguaje de poder

268
00:10:40,240 --> 00:10:44,360
traducirlo del texto a lo que sería al lenguaje de máquina.

269
00:10:44,360 --> 00:10:45,880
Tenemos que utilizar una librería muy importante,

270
00:10:45,880 --> 00:10:49,280
la NLTK, Natural Language Toolkit.

271
00:10:49,280 --> 00:10:51,360
Utilizamos también MatBloodlib y Sieborn,

272
00:10:51,360 --> 00:10:53,440
que nos va a ayudar para poder representar gráficamente

273
00:10:53,440 --> 00:10:56,200
los datos y poder analizarlos mucho mejor.

274
00:10:56,200 --> 00:10:58,440
Utilizaremos Google Colab porque nos va a ayudar a poder

275
00:10:58,440 --> 00:11:00,440
tener procesadores gráficos en la nube.

276
00:11:00,440 --> 00:11:02,240
Vamos a poder tener todos nuestros repositorios y

277
00:11:02,240 --> 00:11:03,240
información en la nube.

278
00:11:03,240 --> 00:11:04,840
Y no tenemos que gastar recursos propios,

279
00:11:04,840 --> 00:11:06,760
que al final es una de las mejores herramientas para no

280
00:11:06,760 --> 00:11:09,640
tener que estar sobrecargando nuestro propio ordenador.

281
00:11:09,640 --> 00:11:12,080
Y, obviamente, tenemos que utilizar TensorFlow Ikeras para

282
00:11:12,080 --> 00:11:15,680
entrenar estos modelos y utilizarlo.

283
00:11:15,680 --> 00:11:17,680
¿Cómo vamos a crear o vamos a sacar esos datos?

284
00:11:17,680 --> 00:11:21,800
Porque, obviamente, no hay una fiabilidad cierta o plena para

285
00:11:21,800 --> 00:11:24,320
poder sacar estos datos, no hay un repositorio.

286
00:11:24,320 --> 00:11:27,600
Entonces, consideramos sacar determinadas cuentas de Twitter,

287
00:11:27,600 --> 00:11:31,840
tanto reales, falsas, en las que se le van a amasar,

288
00:11:31,840 --> 00:11:35,680
principalmente, las reales de la falsa, por su veracidad.

289
00:11:35,680 --> 00:11:38,240
Es decir, no vamos a amasar, por ejemplo, en Bebes New,

290
00:11:38,240 --> 00:11:42,880
o puede ser, por ejemplo, en el país o cualquier tipo de

291
00:11:42,880 --> 00:11:45,280
televiario que nos pueda dar una información más relevante.

292
00:11:45,280 --> 00:11:46,800
Y nos basaremos en cuentas satílicas,

293
00:11:46,800 --> 00:11:49,080
como puede ser el mundo today, que nosotros sabemos que

294
00:11:49,080 --> 00:11:51,240
cualquier tipo de Twitter que vayan a publicar lo más probable

295
00:11:51,240 --> 00:11:54,440
que sea satílico, que también eso provoca que se genere esa

296
00:11:54,440 --> 00:11:55,600
fake new.

297
00:11:55,600 --> 00:11:57,880
Cuando recogemos nuestros datos, tenemos que tener en cuenta esa

298
00:11:57,880 --> 00:12:00,440
columna de veracidad, asignándole un 0 a las que sean

299
00:12:00,440 --> 00:12:03,680
falsas y un 1 a las que sean reales.

300
00:12:03,680 --> 00:12:05,760
Aquí tenemos estas cuentas reales y falsas.

301
00:12:05,760 --> 00:12:07,720
Y no vamos a utilizar ninguna cuenta española,

302
00:12:07,720 --> 00:12:09,600
no vamos a utilizar nada del castellano.

303
00:12:09,600 --> 00:12:10,320
¿Por qué?

304
00:12:10,320 --> 00:12:12,960
Porque si ya nos puede entendernos entre la gente de

305
00:12:12,960 --> 00:12:15,280
nuestras comunidades, por ejemplo, yo que soy canario o una

306
00:12:15,280 --> 00:12:16,840
persona que sea Andalusa o Olga Llego,

307
00:12:16,840 --> 00:12:19,760
por nuestra forma de escribir que es completamente distinta,

308
00:12:19,760 --> 00:12:21,800
pongamos toda la comunidad castellana,

309
00:12:21,800 --> 00:12:23,680
toda la comunidad tanto latinoamericana.

310
00:12:23,680 --> 00:12:25,640
Tenemos una complejidad que aumenta mucho más nuestro

311
00:12:25,640 --> 00:12:26,040
problema.

312
00:12:26,040 --> 00:12:29,000
A nivel anglosajón es mucho más sencillo de poder analizar

313
00:12:29,000 --> 00:12:31,040
ese tipo de tweets y poder sacar un mejor análisis de

314
00:12:31,040 --> 00:12:32,000
sentimiento.

315
00:12:32,000 --> 00:12:35,880
Por eso las cuentas que sean inglesas.

316
00:12:35,880 --> 00:12:37,240
Afinal, cuando recogemos todos estos datos,

317
00:12:37,240 --> 00:12:40,080
estamos recogiendo 1,000 tweets de cada cuenta.

318
00:12:40,080 --> 00:12:41,840
Hay muchas cuentas que no llegan a esos 1,000 tweets.

319
00:12:41,840 --> 00:12:44,480
Pero intentar conseguir 1,000 tweets de cada cuenta y los

320
00:12:44,480 --> 00:12:47,200
quedaría guardados así de esta manera.

321
00:12:47,200 --> 00:12:49,000
Partiendo de esto, vamos a analizar esos datos y vamos a

322
00:12:49,000 --> 00:12:50,960
saber con qué nos podemos quedar y con qué no nos vamos a

323
00:12:50,960 --> 00:12:51,760
quedar.

324
00:12:51,760 --> 00:12:53,920
Obviamente, nos vamos a quedar con ese texto para poder

325
00:12:53,920 --> 00:12:57,280
analizarlo, la veracidad, el usuario, su identificador.

326
00:12:57,280 --> 00:12:59,320
Y las variables que hemos borrado son principalmente

327
00:12:59,320 --> 00:13:01,880
buleanos que nos aporta Twitter, que no nos aporta ningún tipo

328
00:13:01,880 --> 00:13:04,640
de información.

329
00:13:04,640 --> 00:13:07,280
Como mencionó anteriormente, en LEP tiene que llegarle un

330
00:13:07,280 --> 00:13:10,800
texto muy limpio, muy perfecto para poder saber diferenciar

331
00:13:10,800 --> 00:13:11,880
todas las palabras que les llegan.

332
00:13:11,880 --> 00:13:14,720
Por lo tanto, tenemos que importar esas top words que

333
00:13:14,720 --> 00:13:16,000
son anglosajonas.

334
00:13:16,000 --> 00:13:18,520
Primero, también, es quitar todos los emojis, emoticonos,

335
00:13:18,520 --> 00:13:21,000
símbolos, pintogramas, información que no nos aporta.

336
00:13:21,000 --> 00:13:24,160
Sin nos apuntuación, anexador de HTML, URL,

337
00:13:24,160 --> 00:13:27,400
y es utilizar últimamente ya ahí al final,

338
00:13:27,400 --> 00:13:29,920
quitar los top words.

339
00:13:29,920 --> 00:13:32,000
A nivel de Python nos quedaría de esta manera.

340
00:13:32,000 --> 00:13:35,360
A nivel de código que utilicemos y que le haríamos a nivel

341
00:13:35,360 --> 00:13:37,800
secuencia, como mencionado anteriormente, esos pasos para

342
00:13:37,800 --> 00:13:41,240
poder crear ese texto completamente limpio.

343
00:13:41,240 --> 00:13:43,440
A partir de que utilizaremos el algoritmo de Snowball Steamer

344
00:13:43,440 --> 00:13:45,440
para poder quedarnos, como ha mencionado anteriormente,

345
00:13:45,440 --> 00:13:47,560
con la raíz de la palabra, que es lo que nos importa.

346
00:13:47,560 --> 00:13:51,480
No tenemos que quedarnos con ningún tipo de declinación.

347
00:13:51,480 --> 00:13:53,880
Y quería mirar si existía algún tipo de ruido de que hubiese

348
00:13:53,880 --> 00:13:55,560
mucha diferencia, mucha disparidad entre las noticias

349
00:13:55,560 --> 00:13:56,880
reales y las falsas.

350
00:13:56,880 --> 00:13:58,960
Viendolo con esta gráfica, la única diferencia que podemos

351
00:13:58,960 --> 00:14:02,160
ver, aproximadamente, son unos 1.500 tweets de diferencia,

352
00:14:02,160 --> 00:14:02,880
1.600.

353
00:14:02,880 --> 00:14:04,520
Pero estamos teniendo aproximadamente unos 15.000

354
00:14:04,520 --> 00:14:07,120
tweets de cuentas tanto reales como falsas,

355
00:14:07,120 --> 00:14:09,800
teniendo ya 16 cuentas.

356
00:14:09,800 --> 00:14:12,000
Quién se hace una correlación para poder intentar identificar si

357
00:14:12,000 --> 00:14:15,160
existía alguna relación entre las variables que teníamos.

358
00:14:15,160 --> 00:14:18,560
Y obviamente, la mayor relación que tenemos con respecto a lo

359
00:14:18,560 --> 00:14:20,880
que se llama la veracidad de la noticia es su identificador,

360
00:14:20,880 --> 00:14:21,680
es su idea.

361
00:14:21,680 --> 00:14:22,000
¿Por qué?

362
00:14:22,000 --> 00:14:24,280
Porque nosotros hemos dicho, si está real, le vamos a poner un

363
00:14:24,280 --> 00:14:24,520
1.

364
00:14:24,520 --> 00:14:25,960
Si es falsa, vamos a poner un 0.

365
00:14:25,960 --> 00:14:27,480
Va a existir esa correlación.

366
00:14:27,480 --> 00:14:31,960
Y entonces, es la que se puede observar en esta gráfica.

367
00:14:31,960 --> 00:14:33,920
Y quería enseñarlos con un Work Cloud, que es una librería

368
00:14:33,920 --> 00:14:36,920
muy utilizada en Python, para poder enseñarlos a nivel visual

369
00:14:36,920 --> 00:14:39,840
la diferencia de palabras más frecuentes entre las reales y

370
00:14:39,840 --> 00:14:40,720
las falsas.

371
00:14:40,720 --> 00:14:42,160
Porque lo que mencionado anteriormente es que hay que saber el

372
00:14:42,160 --> 00:14:44,320
contexto en el que están esas palabras vectorizadas y poder

373
00:14:44,320 --> 00:14:46,560
relacionarlas para poder entrenar este modelo.

374
00:14:46,560 --> 00:14:48,720
Tenemos tanto las reales como esa falsa.

375
00:14:48,720 --> 00:14:51,960
Es decir, podemos ver que se repiten News, Say, Tweet, One,

376
00:14:51,960 --> 00:14:53,280
también se ha de toda una Trump.

377
00:14:53,280 --> 00:14:55,320
Entonces, tenemos que tener en cuenta todos esos factores para

378
00:14:55,320 --> 00:14:57,600
poder crear nuestro modelo.

379
00:14:57,600 --> 00:14:59,480
A partir de aquí, vamos a empezar ya con el miojo de crear

380
00:14:59,480 --> 00:15:00,720
nuestros modelos LSTM.

381
00:15:00,720 --> 00:15:02,800
Vamos a partir que vamos a hacer cuatro modelos.

382
00:15:02,800 --> 00:15:05,080
Estos cuatro modelos se van a poder comparar porque vamos a

383
00:15:05,080 --> 00:15:07,320
crear nosotros, nuestros propios embedding.

384
00:15:07,320 --> 00:15:10,440
Vamos a crear nosotros con estos 16,000 tweets que tenemos.

385
00:15:10,440 --> 00:15:13,200
Vamos a crear nuestro propio campo vectorial y, además,

386
00:15:13,200 --> 00:15:14,200
nuestros vectores.

387
00:15:14,200 --> 00:15:16,960
Y, aún así, vamos a y se denominan embeddings.

388
00:15:16,960 --> 00:15:18,440
También queremos nuestros embedding.

389
00:15:18,440 --> 00:15:21,560
Vamos a poner una importación de la librería Glove que lo

390
00:15:21,560 --> 00:15:24,840
utilizan en Stanford, en la que es un aprendizaje no

391
00:15:24,840 --> 00:15:28,000
supervisado en el que encuentra patrones, principalmente por

392
00:15:28,000 --> 00:15:30,280
una gran cantidad de cuentas que ellos tienen de Twitter,

393
00:15:30,280 --> 00:15:32,480
pero no está relacionado con nada de las fake news.

394
00:15:32,480 --> 00:15:35,000
Solamente es relación con las cuentas de Twitter y está

395
00:15:35,000 --> 00:15:37,880
mucho mejor vectorizado y optimizado para el caso que

396
00:15:37,880 --> 00:15:39,240
nosotros tenemos.

397
00:15:39,240 --> 00:15:41,960
Tenemos que tener en cuenta la cantidad de neuronos que

398
00:15:41,960 --> 00:15:43,200
nosotros queremos entrenar.

399
00:15:43,200 --> 00:15:46,760
Que de ahí la variable Max SequenceLens, que va a ser 280.

400
00:15:46,760 --> 00:15:49,000
Nosotros tenemos que tener en cuenta la dimensionalidad que

401
00:15:49,000 --> 00:15:51,080
he mencionado anteriormente del tema de los autóncodes,

402
00:15:51,080 --> 00:15:53,440
de saber esas dimensiones, de poder hacerla de una imagen

403
00:15:53,440 --> 00:15:56,440
pixelada a una más pequeña y volver a hacerla grande,

404
00:15:56,440 --> 00:15:59,120
tenemos que poner un tamaño de dimensiones.

405
00:15:59,120 --> 00:16:01,960
Tenemos que tener nuestro campo, tanto de entrenamiento,

406
00:16:01,960 --> 00:16:04,040
que los vamos a llamar XTrain y Xtest.

407
00:16:04,040 --> 00:16:06,760
Y van a haber unos valores que van a hacer los yTrain y los

408
00:16:06,760 --> 00:16:12,360
yTest, que los van a servir para poder validar nuestros datos.

409
00:16:12,360 --> 00:16:14,320
A partir de aquí nuestros datos quedarían de esta manera.

410
00:16:14,320 --> 00:16:17,000
Y como mencionado anteriormente, hay que toquenizar estos

411
00:16:17,000 --> 00:16:20,040
textos que ya tenemos limpios y perfectos y ya listos para

412
00:16:20,040 --> 00:16:22,600
poderlos en nuestro modelo.

413
00:16:22,600 --> 00:16:25,080
A partir de aquí vamos a crear este modelo,

414
00:16:25,080 --> 00:16:26,600
¿cómo se creería?

415
00:16:26,600 --> 00:16:29,480
Este número 128 que vemos aquí arriba, bueno, primero.

416
00:16:29,480 --> 00:16:31,760
La parte de arriba que tenemos de size en bedding dimension,

417
00:16:31,760 --> 00:16:32,840
ya sea nuestros en bedding.

418
00:16:32,840 --> 00:16:35,280
Vamos a ponerlos y crearlos de nuestra manera.

419
00:16:35,280 --> 00:16:39,320
A partir de ahí tenemos esa número de entras que son 128,

420
00:16:39,320 --> 00:16:42,080
que era esa imagen que teníamos antes de los nodos amarillos.

421
00:16:42,080 --> 00:16:44,280
Vamos a tener 128 entradas.

422
00:16:44,280 --> 00:16:47,760
También vamos a tener una capa de salida, que es la de dense

423
00:16:47,760 --> 00:16:48,160
1.

424
00:16:48,160 --> 00:16:49,960
Y utilizamos una activación que pueden ver ahí que pone

425
00:16:49,960 --> 00:16:50,600
sigmoide.

426
00:16:50,600 --> 00:16:51,880
¿Por qué se llama sigmoide?

427
00:16:51,880 --> 00:16:54,400
Es una función matemática que los resultados que nos va a

428
00:16:54,400 --> 00:16:57,760
dar a esta red neuronal van a variar entre el número 0 y el 1,

429
00:16:57,760 --> 00:17:00,360
que es lo que nosotros queremos, queremos que sea real o que

430
00:17:00,360 --> 00:17:02,840
sea falsa el valor que estamos metiendo.

431
00:17:02,840 --> 00:17:05,440
Y por último, vamos a entrenarla y esta función que pone al

432
00:17:05,440 --> 00:17:08,080
final del todo, que pone Colback, que es el sr stopping,

433
00:17:08,080 --> 00:17:10,960
si nosotros vemos que nuestro modelo no es capaz de poder

434
00:17:10,960 --> 00:17:14,320
entrenar o que está haciendo que exista un sobreentramiento,

435
00:17:14,320 --> 00:17:16,800
hubiese algún tipo de problema, para.

436
00:17:16,800 --> 00:17:19,680
Para y no deja continuar que el modelo entrene.

437
00:17:19,680 --> 00:17:22,360
Entonces, nosotros ya después de haber entrenado este modelo,

438
00:17:22,360 --> 00:17:24,920
vamos a hacer unas métricas, las de pérdida y precisión,

439
00:17:24,920 --> 00:17:26,280
que son los más importantes.

440
00:17:26,280 --> 00:17:28,760
La línea azul va a ser la de entrenamiento y la línea

441
00:17:28,760 --> 00:17:30,340
naranja es la de testeo.

442
00:17:30,340 --> 00:17:32,280
Podemos comprobar en la precisión que hay una

443
00:17:32,280 --> 00:17:33,440
disparidad brutal.

444
00:17:33,440 --> 00:17:35,440
Hay una disparidad que estamos viendo que en el nivel de

445
00:17:35,440 --> 00:17:38,440
entrenamiento y testeo, hay una diferencia muy grande.

446
00:17:38,440 --> 00:17:42,200
Entonces, consideré que lo mejor sería probar a,

447
00:17:42,200 --> 00:17:44,040
vamos a conseguir 3,000 tweets de cuentas,

448
00:17:44,040 --> 00:17:46,080
completamente distintas a las anteriores,

449
00:17:46,080 --> 00:17:49,320
y vamos a probar con una matriz de confusión el resultado que

450
00:17:49,320 --> 00:17:50,080
nos va a dar.

451
00:17:50,080 --> 00:17:52,960
Si va a ser algo que solamente en el entrenamiento está

452
00:17:52,960 --> 00:17:55,960
fallando, pero en el momento de la verdad puede mejorar.

453
00:17:55,960 --> 00:17:57,600
Cuando hacemos esta matriz de confusión,

454
00:17:57,600 --> 00:18:00,480
podemos comprobar que tenemos un 50-50.

455
00:18:00,480 --> 00:18:02,440
Está haciendo una pérdida de discusión completamente aleatoria.

456
00:18:02,440 --> 00:18:03,800
No sabe diferenciar bien, tal vez,

457
00:18:03,800 --> 00:18:06,200
las noticias reales de la falsa.

458
00:18:06,200 --> 00:18:08,680
Entonces, vamos a probar, tal vez, a mejorarlo,

459
00:18:08,680 --> 00:18:09,880
cuando no con los envedi.

460
00:18:09,880 --> 00:18:11,400
Vamos a utilizar los de globes.

461
00:18:11,400 --> 00:18:13,680
Vamos a ver si esto mejora con el entrenamiento.

462
00:18:13,680 --> 00:18:14,440
El modelo es el mismo.

463
00:18:14,440 --> 00:18:17,640
El mismo número de entrada, mismo número de salida.

464
00:18:17,640 --> 00:18:20,240
A partir de aquí vemos esa precisión y el tema de pérdida.

465
00:18:20,240 --> 00:18:21,800
La presión es mucho más cercana.

466
00:18:21,800 --> 00:18:24,520
Se parece mucho más entre entrenamiento y el testeo.

467
00:18:24,520 --> 00:18:26,600
Esto es lo que nos sirve, porque ahí vemos que, oye,

468
00:18:26,600 --> 00:18:29,280
pues tiene mejor pinta que utilizar nuestros envedi.

469
00:18:29,280 --> 00:18:32,320
Vamos a probar con esa matriz de confusión.

470
00:18:32,320 --> 00:18:35,560
Estamos viendo que, no, otra vez, o ese 50-50.

471
00:18:35,560 --> 00:18:38,360
Hay una predicción aleatoria que no está cumpliéndose.

472
00:18:38,360 --> 00:18:40,600
A principios pensábamos que puede hacer por nuestros envedi.

473
00:18:40,600 --> 00:18:42,760
Pero tal vez el problema puede ser el número de entrada.

474
00:18:42,760 --> 00:18:45,280
Habrá que meter, tal vez, más entrada para que pueda

475
00:18:45,280 --> 00:18:47,400
mejorar nuestro rendimiento.

476
00:18:47,400 --> 00:18:48,960
Por eso vamos a crear este segundo modelo,

477
00:18:48,960 --> 00:18:50,840
en el que vamos a meter mucho más número de entrada.

478
00:18:50,840 --> 00:18:52,640
Vamos a cambiar nuestra funcionalidad de que,

479
00:18:52,640 --> 00:18:55,160
cuando estemos entrenando nuestro modelo, no se va a parar.

480
00:18:55,160 --> 00:18:57,760
Vamos a dejar que entren, que llega hasta el final.

481
00:18:57,760 --> 00:18:59,560
Y vamos a utilizar una función que se llama Reducing Learning

482
00:18:59,560 --> 00:19:00,280
o Plato.

483
00:19:00,280 --> 00:19:02,400
Lo que él va haciendo durante ese entrenamiento es poder

484
00:19:02,400 --> 00:19:05,520
optimizar y va a ir mejorando esos rendimientos dentro

485
00:19:05,520 --> 00:19:08,040
del modelo, ya sea esas entradas y esas salidas,

486
00:19:08,040 --> 00:19:10,760
dentro del terreno neuronal, para que el resultado varía.

487
00:19:10,760 --> 00:19:13,200
Y pueda entrenar y pueda mejorar.

488
00:19:13,200 --> 00:19:15,760
Y por último, vamos a coger cantidades más pequeñas.

489
00:19:15,760 --> 00:19:18,000
Vamos a intentar coger cantidades, tal vez, más grandes,

490
00:19:18,000 --> 00:19:22,200
que nos pueda ayudar para nuestro cuerpo y mejorar.

491
00:19:22,200 --> 00:19:25,360
Cuando estamos aquí, ya vemos que ha pasado 15 épocas.

492
00:19:25,360 --> 00:19:28,320
Y estamos viendo que el entrenamiento está teniendo un 100%.

493
00:19:28,320 --> 00:19:29,240
O sea, no falla.

494
00:19:29,240 --> 00:19:30,120
Es algo imposible.

495
00:19:30,120 --> 00:19:32,280
Esto no puede ser que en el entrenamiento tengamos un 100%.

496
00:19:32,280 --> 00:19:35,480
Y cuando llegamos, a la verdad, al testear, estemos en un 90%.

497
00:19:35,480 --> 00:19:36,720
Aquí hay algún problema.

498
00:19:36,720 --> 00:19:40,800
Entonces, vamos a hacer esa matriz de confusión y vemos que no.

499
00:19:40,800 --> 00:19:43,080
Sigue siendo una predicción completamente aleatoria.

500
00:19:43,080 --> 00:19:45,280
Pues lo más probable es que puedan hacer las entradas.

501
00:19:45,280 --> 00:19:47,360
Yo creo el problema está en nuestro embed y en nuestro

502
00:19:47,360 --> 00:19:48,040
corpus.

503
00:19:48,040 --> 00:19:49,440
No es capaz nuestro espacio vectorial,

504
00:19:49,440 --> 00:19:52,400
o tiene que ser muy pequeño, y estar recibiendo valores que no

505
00:19:52,400 --> 00:19:55,160
las ha visto nunca antes y provoca que sean falsas,

506
00:19:55,160 --> 00:19:57,600
que él detecte que como no lo ha visto, no lo ha entrenado,

507
00:19:57,600 --> 00:19:58,480
pues es falso.

508
00:19:58,480 --> 00:19:59,880
No me lo creo.

509
00:19:59,880 --> 00:20:01,200
Pero vamos a probar con Glove.

510
00:20:01,200 --> 00:20:02,240
Va a tener muchos más valores.

511
00:20:02,240 --> 00:20:03,680
Va a tener muchas más probabilidades de que pueda

512
00:20:03,680 --> 00:20:05,360
encontrar una relación.

513
00:20:05,360 --> 00:20:08,360
Con el mismo modelo es decir, vamos a tener esa entrada de 128,

514
00:20:08,360 --> 00:20:13,360
64, 32 y una final de 1 para poder saber qué salida queremos.

515
00:20:13,360 --> 00:20:15,480
¿Y va a ser real o va a ser falso?

516
00:20:15,480 --> 00:20:17,400
Vemos en nuestra precisión y en nuestra pérdida que ha

517
00:20:17,400 --> 00:20:18,240
mejorado mucho más.

518
00:20:18,240 --> 00:20:21,240
Es decir, hay determinados picos en determinadas zonas.

519
00:20:21,240 --> 00:20:23,280
Pero en la última época estamos viendo que la disparidad

520
00:20:23,280 --> 00:20:26,080
que hay no es tan grande, que es aceptable.

521
00:20:26,080 --> 00:20:27,680
Vamos a probar con esa matriz de confusión.

522
00:20:27,680 --> 00:20:30,200
Tal vez ya ha mejorado mucho más nuestro modelo y puede ser

523
00:20:30,200 --> 00:20:32,160
mucho más óptimo.

524
00:20:32,160 --> 00:20:34,320
Y no, la matriz de confusión nos vuelva a dar ese error.

525
00:20:34,320 --> 00:20:36,720
Pues lo más probable es que no sea en los temas de embedding.

526
00:20:36,720 --> 00:20:38,600
Pueden ser número de puertas.

527
00:20:38,600 --> 00:20:40,760
Vamos a necesitar entonces nuestro tercer modelo.

528
00:20:40,760 --> 00:20:41,560
Vamos a ir al primero.

529
00:20:41,560 --> 00:20:44,480
Vamos a dejarlo con las 128 entradas que tenemos y esas 280

530
00:20:44,480 --> 00:20:45,160
neuronas.

531
00:20:45,160 --> 00:20:46,240
Vamos a dejarlo igual.

532
00:20:46,240 --> 00:20:47,920
Pero vamos a utilizar un nuevo término que se llama

533
00:20:47,920 --> 00:20:50,040
los regulaciones de kernels.

534
00:20:50,040 --> 00:20:51,400
O los regulaciones de núcleo.

535
00:20:51,400 --> 00:20:54,720
Estos regulaciones de núcleo, lo que va a utilizar es que va

536
00:20:54,720 --> 00:20:57,440
a ir variando lo que menciona anteriormente esas conexiones

537
00:20:57,440 --> 00:20:58,640
entre nodos.

538
00:20:58,640 --> 00:21:02,080
Va a ir variando la en la necesidad, los pesos y los

539
00:21:02,080 --> 00:21:05,200
umbrales para que nos pueda dar otra aportación y pueda

540
00:21:05,200 --> 00:21:08,640
recorrer otros nodos que no ha querido recorrer antes

541
00:21:08,640 --> 00:21:10,720
durante su entrenamiento.

542
00:21:10,720 --> 00:21:13,240
Vamos a utilizar, obviamente, primero nuestros embedding para

543
00:21:13,240 --> 00:21:13,960
ver los resultados.

544
00:21:13,960 --> 00:21:15,800
Porque ya que estamos utilizando los nuestros,

545
00:21:15,800 --> 00:21:17,480
vamos a llegar hasta el final.

546
00:21:17,480 --> 00:21:20,480
Pues podemos ver que la culpa es mucha.

547
00:21:20,480 --> 00:21:21,760
O sea, la diferencia es mucho mayor.

548
00:21:21,760 --> 00:21:23,320
Está haciendo mucho más dispar.

549
00:21:23,320 --> 00:21:25,280
Pues si lo ponemos otra vez con la matriz de confusión,

550
00:21:25,280 --> 00:21:27,520
podemos comprobar que estos resultados, otra vez,

551
00:21:27,520 --> 00:21:32,600
está dando ese 50-50 que no podemos detectar cuál es real y

552
00:21:32,600 --> 00:21:33,240
cuál es falsa.

553
00:21:33,240 --> 00:21:36,480
Porque está pasando este problema.

554
00:21:36,480 --> 00:21:38,240
Y entonces vamos a probarlo otra vez con Glove.

555
00:21:38,240 --> 00:21:39,320
Vamos a probar el mismo modelo.

556
00:21:39,320 --> 00:21:40,480
Vamos a probar con estas métricas,

557
00:21:40,480 --> 00:21:42,560
a ver los resultados que nos pueda aportar.

558
00:21:42,560 --> 00:21:46,360
Y nos estamos dando cuenta que sigue diciendo esta disparidad

559
00:21:46,360 --> 00:21:49,320
y vamos a utilizar esa matriz de confusión para sacar estos

560
00:21:49,320 --> 00:21:51,480
resultados.

561
00:21:51,480 --> 00:21:53,040
¿Qué es lo que podemos plantear?

562
00:21:53,040 --> 00:21:55,000
Creemos que el corpus no es el suficientemente,

563
00:21:55,000 --> 00:21:58,480
ese espacio vectorial no es suficiente para poder representar

564
00:21:58,480 --> 00:21:59,600
todo lo que necesitamos.

565
00:21:59,600 --> 00:22:01,800
Entonces vamos a introducir 16,000 nuevos datos.

566
00:22:01,800 --> 00:22:03,360
Vamos a tener 32,000 datos.

567
00:22:03,360 --> 00:22:05,400
Vamos a mejorar nuestro corpus.

568
00:22:05,400 --> 00:22:07,440
Vamos a hacer una mezcla de todos los modelos que hemos visto

569
00:22:07,440 --> 00:22:08,040
anteriormente.

570
00:22:08,040 --> 00:22:10,080
Vamos a meterle todas las cantidades de entrada que

571
00:22:10,080 --> 00:22:13,920
teníamos, 128, 64, 32 y una capa de salida.

572
00:22:13,920 --> 00:22:16,760
Y además, lo que vamos a hacer es esos regularizadores de núcleo.

573
00:22:16,760 --> 00:22:19,880
Vamos a ver si puede ir mejorando con eso.

574
00:22:19,880 --> 00:22:22,280
Y vamos a mejorar nuestro corpus porque vamos a tener muchos

575
00:22:22,280 --> 00:22:24,440
más datos que hemos limpiado, hemos procesado y hemos hecho

576
00:22:24,440 --> 00:22:26,360
todos los pasos anteriores.

577
00:22:26,360 --> 00:22:29,680
Cuando le hemos entrenado, oye, os ha ido mucho mejor.

578
00:22:29,680 --> 00:22:31,720
Es decir, tal no tiene que ser tan sencillo nuestro modelo.

579
00:22:31,720 --> 00:22:33,240
Tal vez tiene que tener esas complejidades,

580
00:22:33,240 --> 00:22:36,280
esos números de puertas y poder ir mejorando y sus rendimientos

581
00:22:36,280 --> 00:22:38,320
a lo largo de los entrenamientos.

582
00:22:38,320 --> 00:22:40,280
Cuando hacemos esa matriz de confusión,

583
00:22:40,280 --> 00:22:42,280
nos vuelvo a dar otros 50-50.

584
00:22:42,280 --> 00:22:45,520
Estamos viendo que aquí hay algo que con estos tweets que

585
00:22:45,520 --> 00:22:48,200
está recibiendo nuevos, no es tan tal vez nuestro corpus y por

586
00:22:48,200 --> 00:22:51,800
eso hagas así 50-50 o puede ser número de entrada.

587
00:22:51,800 --> 00:22:52,720
Ahí no lo sabemos.

588
00:22:52,720 --> 00:22:55,320
Ahí hay que seguir entrenando ese modelo con más datos,

589
00:22:55,320 --> 00:22:57,320
que es uno de los problemas que tenemos ahora mismo.

590
00:22:57,320 --> 00:22:59,800
Volvemos a hacerlo con el embedding de Glove y nos damos

591
00:22:59,800 --> 00:23:03,080
cuenta de que la precisión y la pérdida es muy parecida a

592
00:23:03,080 --> 00:23:04,320
nuestro corpus.

593
00:23:04,320 --> 00:23:05,600
¿Por qué ocurre esto?

594
00:23:05,600 --> 00:23:08,120
Porque si lo vamos a hacer en la mesa matriz de confusión,

595
00:23:08,120 --> 00:23:10,160
vemos que está dando los mismos resultados que el anterior,

596
00:23:10,160 --> 00:23:11,320
con nuestros embeddings.

597
00:23:11,320 --> 00:23:12,800
Un poco raro, ¿no?

598
00:23:12,800 --> 00:23:17,440
Esto ocurre principalmente porque nosotros no tenemos la

599
00:23:17,440 --> 00:23:18,520
cantidad de datos suficiente.

600
00:23:18,520 --> 00:23:21,640
Son 32,000 tweets que son bastantes.

601
00:23:21,640 --> 00:23:25,280
Pero claro, son pocos porque no todos escribimos igual y no

602
00:23:25,280 --> 00:23:27,000
todas las cuentas escriben igual.

603
00:23:27,000 --> 00:23:29,320
Una cuenta real puede escribir la misma noticia que otra

604
00:23:29,320 --> 00:23:31,920
cuenta real, pero la forma de escribirse y la forma de

605
00:23:31,920 --> 00:23:33,800
desplayarse, tener esa sintaxis,

606
00:23:33,800 --> 00:23:37,680
provoque que la complejidad sea mucho mayor y que provoque que

607
00:23:37,680 --> 00:23:40,120
no podamos relacionarse con una noticia real o falsa.

608
00:23:40,120 --> 00:23:42,000
Para poder sacar esto, tenemos que aumentar nuestra

609
00:23:42,000 --> 00:23:44,600
complejidad tanto temporal como computacional.

610
00:23:44,600 --> 00:23:46,560
Es decir, tendríamos que estar 24 horas,

611
00:23:46,560 --> 00:23:48,520
7 días de la semana sacando tweets,

612
00:23:48,520 --> 00:23:51,880
podiéndolos clasificar entre reales y falsos y poder

613
00:23:51,880 --> 00:23:55,440
entender las sintaxis y esos estándares de cómo escribir

614
00:23:55,440 --> 00:23:57,360
o cómo escribe cada persona.

615
00:23:57,360 --> 00:24:00,400
Algo muy complejo porque sabemos que Twitter es algo heterogéneo,

616
00:24:00,400 --> 00:24:02,880
es algo que es muy inmenso y es tan constante cambio.

617
00:24:02,880 --> 00:24:08,000
No es algo que pueda quedarse estable durante un cierto tiempo.

618
00:24:08,000 --> 00:24:10,240
Entonces, ¿por qué ocurre esta predicción tan baja?

619
00:24:10,240 --> 00:24:12,160
Principalmente por lo que mencionaba anteriormente,

620
00:24:12,160 --> 00:24:14,800
el corpus, esos espacios vectorales que hemos quedado

621
00:24:14,800 --> 00:24:17,680
en nosotros, es muy escaso, es muy pequeño.

622
00:24:17,680 --> 00:24:21,000
El de Globe nos estaba ayudando, estaba dando mejores resultados.

623
00:24:21,000 --> 00:24:22,840
Pero lo más probable es que nuestro modelo,

624
00:24:22,840 --> 00:24:26,560
al recibir nuevos tweets o tweets completamente distintos,

625
00:24:26,560 --> 00:24:30,080
que tal vez el modelo de Globe no esté entrenado,

626
00:24:30,080 --> 00:24:34,160
perdón, modelo no, es algoritmo de Globe con esos victores,

627
00:24:34,160 --> 00:24:37,280
no esté preparado, no esté preparado con tweets que son

628
00:24:37,280 --> 00:24:40,480
recientes, que son de hace poco tiempo y hay que volver a entrenar

629
00:24:40,480 --> 00:24:42,960
ese algoritmo y sacar esas relaciones.

630
00:24:42,960 --> 00:24:45,160
Y a futuro, ¿cómo está el proyecto?

631
00:24:45,160 --> 00:24:47,480
¿Cómo está esa relación de entidades que hemos dicho de

632
00:24:47,480 --> 00:24:51,000
que este tweet es de esta persona y es real o es falsa?

633
00:24:51,000 --> 00:24:53,240
Podríamos analizarlo, que podríamos sacar mucho más

634
00:24:53,240 --> 00:24:55,720
tweets, podríamos sacar mucha más información y podríamos

635
00:24:55,720 --> 00:25:00,400
ser capaces de poder adivinar el próximo tweet de esa persona.

636
00:25:00,400 --> 00:25:03,680
Y además, identificar si podría ser real o podría ser falso.

637
00:25:03,680 --> 00:25:07,280
Pero bueno, esto es una harina de otro costal y esto se puede

638
00:25:07,280 --> 00:25:10,520
quedar para otra charla de la Python para otro año.

639
00:25:10,520 --> 00:25:13,400
Y nada, muchísimas gracias por la atención y por estar aquí

640
00:25:13,400 --> 00:25:14,400
en la charla hoy.

641
00:25:14,400 --> 00:25:25,680
Muchas gracias y muchas gracias a la conversación de

642
00:25:25,680 --> 00:25:26,800
higherntime interesante.

643
00:25:26,800 --> 00:25:35,720
Ahora tenemos cinco minutillos para la pregunta.

644
00:25:35,720 --> 00:25:45,240
Gracias. ¿Qué le hizo una pregunta? Porque es más

645
00:25:45,240 --> 00:25:51,560
extrañado cuando se ha hablado de la limpieza de datos preliminar, que se limpiara en Moji,

646
00:25:51,560 --> 00:25:55,960
emoticono y otras cosas más gráficas, porque entiendo que la comunicación electrónica

647
00:25:55,960 --> 00:26:04,320
a veces lo que complementa el texto para darle lo que falta en la parte textual sin entonación,

648
00:26:04,320 --> 00:26:09,760
entonces lo mejor es como se detesta que una ironía o el sentimiento que tiene con respecto

649
00:26:09,760 --> 00:26:16,520
al texto. Entonces se ha hecho simplemente por hacerlo más fácil en primer proceso,

650
00:26:16,520 --> 00:26:21,760
o es que se ha considerado que no era tan relevante en el contexto de noticias falsas

651
00:26:21,760 --> 00:26:23,800
o en ese género discursivo electrónico de gamos.

652
00:26:23,800 --> 00:26:28,840
Claro, el tema es que eso sí está en consideración desde el principio, pero cuenta con el caso

653
00:26:28,840 --> 00:26:34,680
de que ¿qué valor le asignas tú a una Emoji? Va a tener un relacional, va a tener un número

654
00:26:34,680 --> 00:26:39,880
así, va a tener ese número en el que le vas a asignar, que un Emoji contento es una

655
00:26:39,880 --> 00:26:43,440
noticia falsa, es una noticia real, es que puede prestar mucho esa confusión. Por eso

656
00:26:43,440 --> 00:26:47,920
considero que todo lo que sea pictograma, simbología, Emoji se tengan que eliminar,

657
00:26:47,920 --> 00:26:51,600
porque no aportan esa forma, o sea, para nosotros obviamente sí, porque nosotros comprendemos

658
00:26:51,600 --> 00:26:56,080
esa ironía, comprendemos que existe esa sátira, pero el ordenador va a recibir ese carácter

659
00:26:56,080 --> 00:26:59,800
así y lo va a asignar a un valor o lo va a asignar a otro valor. Entonces esa limpieza

660
00:26:59,800 --> 00:27:03,760
hay que hacerla de tal manera que nos quede solamente a lo que he mencionado antes, palabras

661
00:27:03,760 --> 00:27:07,320
que sean muy sencillas y muy fáciles para que el ordenador pueda entenderlo y además

662
00:27:07,320 --> 00:27:11,400
entienda ese contexto. Si es verdad que si vamos entrenándolo poco a poco y entrenamos

663
00:27:11,400 --> 00:27:15,080
solamente lo que sería la sátira y entrenar a tal momento que pilla lo que sería ese

664
00:27:15,080 --> 00:27:19,560
contexto de lo que sería la broma, podríamos tal vez incluir esos Emoji. Pero claro, sería

665
00:27:19,560 --> 00:27:23,040
ir entrenándolo con muchísimos datos hasta tal punto de que sepa que eso es un mensaje

666
00:27:23,040 --> 00:27:27,240
de ironía. Sería cambiar el contexto de ser falso o real a si es sátira o no sátira.

667
00:27:27,240 --> 00:27:32,560
No sé si lo he podido responder. ¿Alguien más?

668
00:27:32,560 --> 00:27:33,560
¿Me pregunté?

669
00:27:33,560 --> 00:27:38,440
Arriba, creo. Ah bueno, también. Si hay otro, arriba creo.

670
00:27:38,440 --> 00:27:40,240
Hola, gracias por la charla.

671
00:27:40,240 --> 00:27:41,240
Hola, gracias.

672
00:27:41,240 --> 00:27:48,480
Veo que... Bueno, hay una charla, creo que soy, mañana sobre explicabilidad que usa los

673
00:27:48,480 --> 00:27:53,480
sap values, quizá te pueda ayudar. Veo que no has aplicado explicabilidad ni cuantificación

674
00:27:53,480 --> 00:27:58,800
de la incertidumbre. A menudo no es la solución aumentar el conjunto de datos para saber qué

675
00:27:58,800 --> 00:27:59,800
está pasando. Claro.

676
00:27:59,800 --> 00:28:05,160
Y mi pregunta también, o sea, la pregunta que tengo es, porque solamente estamos mirando

677
00:28:05,160 --> 00:28:08,880
medidas de métricas como a curasi para entender la calidad del modelo.

678
00:28:08,880 --> 00:28:12,560
Vale, teníamos que mirar principalmente la curasi o lo que habría debajo de la curva

679
00:28:12,560 --> 00:28:16,120
de rock y me parecían valores estadísticos tal vez que para gente que tal vez no tiene

680
00:28:16,120 --> 00:28:19,800
mucho conocimiento estadística no quería meterlo porque es muy pesada, pero esas métricas

681
00:28:19,800 --> 00:28:24,280
nos pueden servir para básicamente saber cómo está evolucionando este modelo y saber

682
00:28:24,280 --> 00:28:26,920
por dónde puede estar fallando y poder estar cogiendo esos fallos.

683
00:28:26,920 --> 00:28:29,600
Solamente, por ejemplo, mirando la precisión es uno de los valores que tenemos diciendo

684
00:28:29,600 --> 00:28:35,240
de que, oye, pues está acertando más en entrenamiento o ya puede ser eso, en la pérdida o en el

685
00:28:35,240 --> 00:28:39,800
fallo. Y nosotros, por ejemplo, he puesto eso de la curasi o de la AUK, básicamente,

686
00:28:39,800 --> 00:28:43,760
para ponerlo debajo de la curva de rock. Ahora mismo no tengo aquí porque no es mi portátil,

687
00:28:43,760 --> 00:28:47,280
pero la curva de rock, cuando la analizamos después de esa AUK, lo que estamos viendo

688
00:28:47,280 --> 00:28:50,640
es que mucho de las curvas, pero lo que estamos viendo en esa representación es que se está

689
00:28:50,640 --> 00:28:54,680
haciendo muy plano, esos datos no nos están ayudando. No sé si te estoy ayudando mucho

690
00:28:54,680 --> 00:28:55,680
a la respuesta.

691
00:28:55,680 --> 00:28:59,840
¿De la práctilidad o de los datos?

692
00:28:59,840 --> 00:29:04,000
Vale, es que no, no, es que no aprenden ni mucho, es que si se acoplaven no te escuchen

693
00:29:04,000 --> 00:29:08,080
mucho, lo siento. No sé si es eso, vale. Perdón. Quiero que había otra persona arriba

694
00:29:08,080 --> 00:29:09,080
también, pero no.

695
00:29:09,080 --> 00:29:14,080
Pero no sé si va a llegar el micrófono.

696
00:29:14,080 --> 00:29:15,080
Sí, pues, sí.

697
00:29:15,080 --> 00:29:16,080
Ah, vale.

698
00:29:16,080 --> 00:29:27,080
Bueno, muchas gracias por la presentación.

699
00:29:27,080 --> 00:29:28,080
Gracias.

700
00:29:28,080 --> 00:29:29,080
Esa fue genial.

701
00:29:29,080 --> 00:29:30,080
Gracias.

702
00:29:30,080 --> 00:29:33,120
Mi pregunta es, parecía que teníamos problemas de overfitting, a medida que iba a aumentar

703
00:29:33,120 --> 00:29:34,120
la complejidad del modelo.

704
00:29:34,120 --> 00:29:35,120
Sí.

705
00:29:35,120 --> 00:29:42,080
¿No puede ser que debamos reducirla un poco?

706
00:29:42,080 --> 00:29:45,200
Reducir la complejidad del modelo, cuando estaba hablando de los regularizadores de

707
00:29:45,200 --> 00:29:46,880
Kernel y demás, estaba mencionando.

708
00:29:46,880 --> 00:29:50,800
No, al final, cuanto mayor complejidad le metamos en ese sentido de poder ir optimizándolo

709
00:29:50,800 --> 00:29:54,680
poco a poco, va a ser que nuestro modelo rinda mucho más. Si, por ejemplo, no hemos metido

710
00:29:54,680 --> 00:29:58,600
esos regularizadores de Kernel, no hemos metido esa función de Colbad, de ese revivir

711
00:29:58,600 --> 00:30:02,680
sin relleno plató, nuestro modelo cuando está entrenando, lo más probable es que no

712
00:30:02,680 --> 00:30:05,240
mejor, sino que vaya, tal vez, empeorando.

713
00:30:05,240 --> 00:30:10,160
Entonces, intentándome, entiendole esos medios apuntes y vamos a metiéndoles esas diferencias,

714
00:30:10,160 --> 00:30:12,680
va a ser que nuestro modelo vaya mejorando mucho más.

715
00:30:12,680 --> 00:30:16,240
Porque cuando más que mayor sea complejo ese modelo, los resultados van a ser mucho

716
00:30:16,240 --> 00:30:18,200
más eficientes, van a ser mucho más precisos.

717
00:30:18,200 --> 00:30:22,080
Casi lo dejamos con los modelos sencillos y simples.

718
00:30:22,080 --> 00:30:27,040
Y eso se va demostrando, cambiando esos modelos y probando nuevos resultados y analizando,

719
00:30:27,040 --> 00:30:31,280
básicamente, esos datos que nos deben principalmente en el entrenamiento y en su testé.

720
00:30:31,280 --> 00:30:34,960
No sé si… Sí, vaya el entrenamiento.

721
00:30:34,960 --> 00:30:35,960
Gracias.

722
00:30:35,960 --> 00:30:36,960
Y delante, ¿no?

723
00:30:36,960 --> 00:30:37,960
¿Y otra?

724
00:30:37,960 --> 00:30:38,960
¿Y otra más?

725
00:30:38,960 --> 00:30:39,960
Y allí ya la quiero.

726
00:30:39,960 --> 00:30:40,960
Vale, después de eso ya te voy a dar la multitud.

727
00:30:40,960 --> 00:30:41,960
Vale.

728
00:30:41,960 --> 00:30:42,960
Me levanto.

729
00:30:42,960 --> 00:30:43,960
Bueno, sí, como quieras.

730
00:30:43,960 --> 00:30:50,960
Pues, Jorge, enhorabuena por tu trabajo, me parece muy interesante.

731
00:30:50,960 --> 00:30:57,680
Y sí te quería preguntar, ¿por qué planteaste un problema de clasificación en vez de un

732
00:30:57,680 --> 00:30:58,680
modelo probabilístico?

733
00:30:58,680 --> 00:31:04,480
Es decir, ¿qué probabilidad hay de que un Twitter se abra de lo falso?

734
00:31:04,480 --> 00:31:09,640
Y ¿cuál es el que quería… y que extendieras un poquito más, cuál sería los siguientes

735
00:31:09,640 --> 00:31:10,640
pasos?

736
00:31:10,640 --> 00:31:16,600
Por ejemplo, yo considero a veces que los seguidores del tweet pueden afectar a esa

737
00:31:16,600 --> 00:31:19,880
decisión del modelo, de saber si es real o no, etcétera.

738
00:31:19,880 --> 00:31:22,240
Hablan un poquito de eso, por favor.

739
00:31:22,240 --> 00:31:23,240
Vale.

740
00:31:23,240 --> 00:31:26,360
Por parte, la primera parte del tema era… Se me ha olvidado.

741
00:31:26,360 --> 00:31:27,360
Se me ha olvidado.

742
00:31:27,360 --> 00:31:28,920
Claro, el probabilístico, vale.

743
00:31:28,920 --> 00:31:33,960
El tema es que era mucho más fácil de poder hablar si era real o es falsa, decir, ese

744
00:31:33,960 --> 00:31:38,080
0 o 1 básicamente, cada un porcentaje de fiabilidad.

745
00:31:38,080 --> 00:31:41,480
Porque ese porcentaje de fiabilidad también tiene que basarlo en qué.

746
00:31:41,480 --> 00:31:45,640
Es decir, tenemos que tener esos datos de poder decir sí, tenemos toda esta probabilística,

747
00:31:45,640 --> 00:31:49,320
sabemos que existen todos estos valores y pongamos que tenemos 3.000, 4.000, 5.000 datos

748
00:31:49,320 --> 00:31:50,320
que sean reales.

749
00:31:50,320 --> 00:31:52,880
Sabemos que esos 100 van a ser falsos.

750
00:31:52,880 --> 00:31:58,200
A nivel probabilístico estás diciendo que bueno sí, sería un 90 o 0,000, 5 posibilidades

751
00:31:58,200 --> 00:31:59,200
de que sea real.

752
00:31:59,200 --> 00:32:01,200
El problema en eso está a nivel temporal.

753
00:32:01,200 --> 00:32:05,720
Es decir, yo puedo publicar un tweet ahora y decir, oye, pues me he roto el tobillo,

754
00:32:05,720 --> 00:32:08,520
se ha caído, oye, que se cualquiera, una piedra, lo que sea.

755
00:32:08,520 --> 00:32:12,920
Y hasta que no pase un cierto tiempo, sean 5 minutos, 10, el tiempo que sea relevante,

756
00:32:12,920 --> 00:32:15,080
no podemos verificar si esa noticia es falsa o no es falsa.

757
00:32:15,080 --> 00:32:18,720
Entonces, principalmente una noticia que te tiene que decir es que va a ser falsa.

758
00:32:18,720 --> 00:32:20,920
Lo que tienda es decirte que va a ser falsa.

759
00:32:20,920 --> 00:32:23,920
Entonces, a nivel probabilístico, sí, en vez de poner un 0,01 te podría haber dicho

760
00:32:23,920 --> 00:32:28,520
sí, está un 0,000, 0,000, 0,000, 5 por ciento fiable de que es real.

761
00:32:28,520 --> 00:32:33,640
Pero creo que si te pongo que es un 0,000, 0,5 por ciento de que es real, no te va a

762
00:32:33,640 --> 00:32:35,760
ser tanta gracia, es real o es falso.

763
00:32:35,760 --> 00:32:39,440
Entonces, si nosotros mejor hacemos nuestro modelo, llegás a un punto determinado en el

764
00:32:39,440 --> 00:32:43,080
que con esos servidores que podíamos tener esa complejidad de poder guardar esos datos,

765
00:32:43,080 --> 00:32:47,520
seguir entrenándolo, seguir mejorando y analizar esos datos y esos resultados, pues podríamos

766
00:32:47,520 --> 00:32:51,040
tener ese modelo perfecto y poder decir, vale, es real.

767
00:32:51,040 --> 00:32:54,480
A partir de aquí, con qué fiabilidad podemos darse al usuario.

768
00:32:54,480 --> 00:32:57,920
Pero en ese momento en que nosotros podamos decir, sí, yo le he metido 2,000 cuentas

769
00:32:57,920 --> 00:33:02,720
de Twitter que sabemos que son reales y me está diciendo que las 2,000 son reales o

770
00:33:02,720 --> 00:33:05,880
falla en un x por ciento, el que sea.

771
00:33:05,880 --> 00:33:08,520
En ese fallo hay cuando podíamos dar ese porcentaje de fiabilidad.

772
00:33:08,520 --> 00:33:10,720
Entonces, ahí sería la idea.

773
00:33:10,720 --> 00:33:11,720
Gracias.

774
00:33:11,720 --> 00:33:14,720
Y bueno, creo que había, no sé si...

775
00:33:14,720 --> 00:33:21,120
Bueno, aquí están justamente los dos, no sé, da igual.

776
00:33:21,120 --> 00:33:22,120
Me hayan diferente.

777
00:33:22,120 --> 00:33:24,120
Si quieres tú que está más cerca ya es cuando estemos...

778
00:33:24,120 --> 00:33:25,120
A mí es muy rápido.

779
00:33:25,120 --> 00:33:26,120
Vale.

780
00:33:26,120 --> 00:33:27,120
Es el set, sobre todo.

781
00:33:27,120 --> 00:33:28,120
¿Cómo hiciste para bajarte tanto?

782
00:33:28,120 --> 00:33:29,120
O sea, con Twitter, ¿no?

783
00:33:29,120 --> 00:33:30,120
Sí.

784
00:33:30,120 --> 00:33:31,120
Sí, Twitter.

785
00:33:31,120 --> 00:33:33,120
¿Tú hiciste para bajarte tantos tweets?

786
00:33:33,120 --> 00:33:36,320
O sea, porque yo creo que como que tiene límites, o sea, hiciste algo...

787
00:33:36,320 --> 00:33:37,320
No, o sea, cuando...

788
00:33:37,320 --> 00:33:41,120
O pagando a Twitter para que te dejara bajarte más.

789
00:33:41,120 --> 00:33:44,680
No, o sea, cuando tenemos esta parte del código, nosotros cuando...

790
00:33:44,680 --> 00:33:47,480
Cuando estamos inicializando, ahí, perdón, aquí, ¿vale?

791
00:33:47,480 --> 00:33:50,080
Nosotros podemos hacer ese número de count, ese número de...

792
00:33:50,080 --> 00:33:51,880
La cantidad que nosotros queremos.

793
00:33:51,880 --> 00:33:56,680
Entonces, yo podíais haber puesto si querías 30 mil, 40 mil, 50 mil, 60 mil de números

794
00:33:56,680 --> 00:33:58,680
de tweets que saques de esa cuenta.

795
00:33:58,680 --> 00:33:59,880
De la cuenta que tú quieras.

796
00:33:59,880 --> 00:34:04,280
Claro, porque la gracia que tiene todo esto es que tenemos que elegir las cuentas, porque

797
00:34:04,280 --> 00:34:08,360
imaginémonos que tú publicas una noticia, yo publico otra, otra persona publica otra

798
00:34:08,360 --> 00:34:09,360
cuenta.

799
00:34:09,360 --> 00:34:12,920
Al final, esto es tan recursivo que no podemos saber cuántas cuentas son reales y cuántas

800
00:34:12,920 --> 00:34:13,920
son falsas.

801
00:34:13,920 --> 00:34:15,920
Pero, al final, no podemos saber cómo es el cambio, cómo la cotamos.

802
00:34:15,920 --> 00:34:21,280
He preferido coger la sátira, algo que sea más cómico y más fácil de detectar.

803
00:34:21,280 --> 00:34:24,960
Porque al final, el que tú lo escribas, yo lo escribas, se tiene que basar en que nuestro

804
00:34:24,960 --> 00:34:28,600
modelo identifique y sepa cómo escribes tú y cómo escribo yo.

805
00:34:28,600 --> 00:34:30,600
Y poder saber si es real o es falso.

806
00:34:30,600 --> 00:34:32,600
Vale, muchas gracias.

807
00:34:32,600 --> 00:34:36,600
Ya está, creo que es el último y...

808
00:34:36,600 --> 00:34:38,600
El último ya es el último.

809
00:34:38,600 --> 00:34:39,600
Sí.

810
00:34:39,600 --> 00:34:44,920
Hola, lo primero de la gracia es por la charla, que es una dinámica súper bien.

811
00:34:44,920 --> 00:34:51,080
Y mi pregunta más por cómo está formular el problema, que por cómo lo resuelves, que

812
00:34:51,080 --> 00:34:53,080
está muy bien detallado todo.

813
00:34:53,080 --> 00:34:57,560
A mí, por lo menos, me cuesta reconocer muchas veces si una noticia es falsa, si es verdadera,

814
00:34:57,560 --> 00:35:00,520
si no tengo el contexto adecuado de lo que habla.

815
00:35:00,520 --> 00:35:01,520
Sí.

816
00:35:01,520 --> 00:35:02,520
Solo primero.

817
00:35:02,520 --> 00:35:06,120
De hecho, incluso casi que me cuesta veces distinguir un clickbait de lo que no un clickbait.

818
00:35:06,120 --> 00:35:07,120
Claro.

819
00:35:07,120 --> 00:35:08,120
Que más rápido, más leyendo, tal.

820
00:35:08,120 --> 00:35:09,120
Sí.

821
00:35:09,120 --> 00:35:14,920
Si no entendí el problema, tú intentas descubrir una sintase, una forma de escribir

822
00:35:14,920 --> 00:35:20,400
inherente a esa fake news y que se distingue perfectamente de la que no es fake.

823
00:35:20,400 --> 00:35:21,400
Sí.

824
00:35:21,400 --> 00:35:26,880
Entonces, yo realmente me pregunto hasta qué punto, a qué límite puedes tener eso realmente

825
00:35:26,880 --> 00:35:28,800
más allá de lo que estás consiguiendo aquí.

826
00:35:28,800 --> 00:35:33,720
Porque quizás faltaría información de sentiment, como lo que planteaban en la primera pregunta

827
00:35:33,720 --> 00:35:39,080
de los emoticonos, del tipo de interacciones que tiene ese tweet, de qué otras fuentes

828
00:35:39,080 --> 00:35:43,280
le dan retweet, que otras fuentes le dan fab, quiénes le contestan, quiénes no.

829
00:35:43,280 --> 00:35:44,280
Sí.

830
00:35:44,280 --> 00:35:47,880
O sea, creo que es simplemente comentarlo, ¿no?

831
00:35:47,880 --> 00:35:49,280
Que cuáles son tus opiniones en eso.

832
00:35:49,280 --> 00:35:50,280
A ver.

833
00:35:50,280 --> 00:35:51,280
Es que...

834
00:35:51,280 --> 00:35:52,280
La información que podría faltar.

835
00:35:52,280 --> 00:35:53,280
Sí.

836
00:35:53,280 --> 00:35:54,280
Gracias.

837
00:35:54,280 --> 00:35:57,000
Muchas gracias por la pregunta.

838
00:35:57,000 --> 00:36:01,320
El tema sería plantearlo de que consideré como había que acotarlo, porque es verdad

839
00:36:01,320 --> 00:36:05,480
que le dices que muchas veces cuesta diferenciar un clickbait del que no sea uno real.

840
00:36:05,480 --> 00:36:09,200
Si es verdad que, por ejemplo, el clickbait intenta hacer que con tres palabras que tú

841
00:36:09,200 --> 00:36:10,960
leas ya entre.

842
00:36:10,960 --> 00:36:14,000
Una real también te va a conseguir con tres letras o tres palabras.

843
00:36:14,000 --> 00:36:15,000
Conseguí que tú entre.

844
00:36:15,000 --> 00:36:20,400
Pero es verdad que tal vez la real se explaia un poco más, tiene una mejor forma de expresarse.

845
00:36:20,400 --> 00:36:21,400
Esturno te...

846
00:36:21,400 --> 00:36:24,920
En una falsa o una real podrías diferenciarlo, porque, por ejemplo, hay faltas de ortografía,

847
00:36:24,920 --> 00:36:27,200
habría falta de sintaxis, no se ha escrito bien.

848
00:36:27,200 --> 00:36:30,600
Ahí puedes sacar esas conclusiones, podrías sacar una relación de decir, oye, pues las

849
00:36:30,600 --> 00:36:35,460
personas que están publicando principalmente fake news, oye, pues no escriben bien determinadas

850
00:36:35,460 --> 00:36:37,080
oraciones, no lo saben hacer bien.

851
00:36:37,080 --> 00:36:40,040
Entonces, ahí puedes sacar algún tipo de patrón para poder identificar las que son

852
00:36:40,040 --> 00:36:42,280
reales de esa falsa.

853
00:36:42,280 --> 00:36:45,320
El planteamiento es otra vez el mismo, es dependiendo del objetivo que queramos y

854
00:36:45,320 --> 00:36:49,120
tienes que acotar ese campo, porque Twitter es muy grande, es lo que he comentado anteriormente,

855
00:36:49,120 --> 00:36:50,120
es muy heterogéneo.

856
00:36:50,120 --> 00:36:54,440
Por lo que encima lo hemos apagado a que será en inglés, que es porque la estructura es

857
00:36:54,440 --> 00:36:58,120
mucho más sencilla, pues imagínate nosotros que somos más graciosos que nadie, que tenemos

858
00:36:58,120 --> 00:37:00,320
que tener formas de hablar y escribir.

859
00:37:00,320 --> 00:37:03,240
Intenta analizarlo y sacar una predicción de eso.

860
00:37:03,240 --> 00:37:05,960
Tenemos esos recursos computacionales y temporales.

861
00:37:05,960 --> 00:37:09,420
Cuanto mayor tengamos, o mayor tiempo tengamos para poder analizar y entrenar ese modelo

862
00:37:09,420 --> 00:37:14,280
y sacar un buen cuerpo y ese espacio vectorial y relacionarlo, se podría sacar.

863
00:37:14,280 --> 00:37:16,560
Pero faltaría ese tiempo y a nivel computacional para poder sacarlo.

864
00:37:16,560 --> 00:37:19,560
Así que nada, gracias.

865
00:37:19,560 --> 00:37:20,560
Muchas gracias a todos.

866
00:37:20,560 --> 00:37:21,560
Gracias.

867
00:37:21,560 --> 00:37:22,560
Gracias.

868
00:37:22,560 --> 00:37:23,560
Gracias.

869
00:37:23,560 --> 00:37:24,560
Gracias.

870
00:37:24,560 --> 00:37:33,560
Gracias.

